{
    "docs": [
        {
            "location": "/", 
            "text": "IT Conferences notes\n\n\nNotes taken during various IT Tech conferences I have attended.", 
            "title": "IT Conferences notes"
        }, 
        {
            "location": "/_index/", 
            "text": "IT Conferences notes\n\n\nNotes taken during various IT Tech conferences I have attended.", 
            "title": "IT Conferences notes"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/", 
            "text": "20160509_CoreOSFest-1\n\n\n\n\n\n\n\n1 - Intro\n\n\n1.1 - Mission\n\n\n1.2 - Speakers and spotlight\n\n\n1.3 Timeline\n\n\n1.4 - News\n\n\ngRPC\n\n\nInmages discovery + transport\n\n\nOCI\n\n\n\n\n\n\n\n\n\n\n\n1 - Intro\n\n\n1.1 - Mission\n\n\nSecure the Internet\n\n\nPush / force updates the fastest and the safest possible\n\n\n1.2 - Speakers and spotlight\n\n\n\n\nRkt\n\n\nDEX\n\n\nClair / HyperClair CLI\n\n\n\n\n1.3 Timeline\n\n\n\n\n2.5 Years ago: Applications containers\n\n\nDocker inspired\n\n\n1.5 Years ago: Standards + security\n\n\nAppCI\n\n\nOCI\n\n\n1 Year ago: Kubertenes 1.0\n\n\nApplication orchestration\n\n\n\n\n1.4 - News\n\n\ngRPC\n\n\nMuch much more efficient\n\n-\n Update etcd\n\n-\n Update Fleet\n\n\nInmages discovery + transport\n\n\nQuay.io\n\n- Bittorrent\n\n- Squashed\n\n\nJwtproxy\n\n\nOCI\n\n\n\n\nMaintainer from across the industry\n\n\nBest of AppCI + Docker Inage\n\n-\n OCI Image Spec\n\n-\n Support comming (AWS containers registry, DockerHub, Google containers registry, Quai.io, \n)\n\n\n\n\nhttps://github.com/opencontainers/runtime-spec", 
            "title": "20160509 CoreOSFest 1"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#11-mission", 
            "text": "Secure the Internet  Push / force updates the fastest and the safest possible", 
            "title": "1.1 - Mission"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#12-speakers-and-spotlight", 
            "text": "Rkt  DEX  Clair / HyperClair CLI", 
            "title": "1.2 - Speakers and spotlight"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#13-timeline", 
            "text": "2.5 Years ago: Applications containers  Docker inspired  1.5 Years ago: Standards + security  AppCI  OCI  1 Year ago: Kubertenes 1.0  Application orchestration", 
            "title": "1.3 Timeline"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#14-news", 
            "text": "", 
            "title": "1.4 - News"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#grpc", 
            "text": "Much much more efficient \n-  Update etcd \n-  Update Fleet", 
            "title": "gRPC"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#inmages-discovery-transport", 
            "text": "Quay.io \n- Bittorrent \n- Squashed  Jwtproxy", 
            "title": "Inmages discovery + transport"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-1/#oci", 
            "text": "Maintainer from across the industry  Best of AppCI + Docker Inage \n-  OCI Image Spec \n-  Support comming (AWS containers registry, DockerHub, Google containers registry, Quai.io,  )   https://github.com/opencontainers/runtime-spec", 
            "title": "OCI"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/", 
            "text": "20160509_CoreOSFest-2\n\n\n\n\n\n\n\n1 - Intro\n\n\nWeave\n\n\nWeave topology\n\n\nRaft\n\n\n2 - Add ETCD for discovery\n\n\nETCD\n\n\nGossip / Raft implementation\n\n\n\n\n\n\n\nEtcD over gossip protocol\n\n\n1 - Intro\n\n\nWeave\n\n\nWeave networks for containers\n\n- Docker initialy\n\n- CNI plugin for Kube / DCOS\n\n\nWeave topology\n\n\n\n\nRouter\n\n\nMeshed network\n\n\nGossip Layer (between routers)\n\n\nReceiver\n\n\nsender\n\n\nYour application\n\n\nTalks to the Gossip Communication protocol\n\n\n\n\nRaft\n\n\nRaft is a \nConsensus\n protocol\n\n\n2 - Add ETCD for discovery\n\n\nETCD\n\n\nExpected Stack:\n\n1. API\n\n2. \nMechanical Bits\n\n3. Storage\n\n4. \nPropose Layer\n\n5. Raft\n\n6. Transport\n\n\nReality:\n\n- Not clearly decoupled\n\n- Not easy programable API\n\n-\n Will have to reimplment a Raft server node \n\n\nGossip / Raft implementation\n\n\n\n\nIn ETCD:\n\n* Raft \nnodes\n are supposed persistant\n\n* Raft \nnodes\n are identified\n\n\nWants \nEphemeral\n behavior\n\n-\n Implement \nnew NodeName ID\n as UUID \nat start-up", 
            "title": "20160509 CoreOSFest 2"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#weave", 
            "text": "Weave networks for containers \n- Docker initialy \n- CNI plugin for Kube / DCOS", 
            "title": "Weave"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#weave-topology", 
            "text": "Router  Meshed network  Gossip Layer (between routers)  Receiver  sender  Your application  Talks to the Gossip Communication protocol", 
            "title": "Weave topology"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#raft", 
            "text": "Raft is a  Consensus  protocol", 
            "title": "Raft"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#2-add-etcd-for-discovery", 
            "text": "", 
            "title": "2 - Add ETCD for discovery"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#etcd", 
            "text": "Expected Stack: \n1. API \n2.  Mechanical Bits \n3. Storage \n4.  Propose Layer \n5. Raft \n6. Transport  Reality: \n- Not clearly decoupled \n- Not easy programable API \n-  Will have to reimplment a Raft server node", 
            "title": "ETCD"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-2/#gossip-raft-implementation", 
            "text": "In ETCD: \n* Raft  nodes  are supposed persistant \n* Raft  nodes  are identified  Wants  Ephemeral  behavior \n-  Implement  new NodeName ID  as UUID  at start-up", 
            "title": "Gossip / Raft implementation"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/", 
            "text": "20160509_CoreOSFest-3\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - Docker \n Rkt\n\n\nDocker daemon\n\n\nRkt for Platform builder\n\n\n3 - Rkt in practice\n\n\n\n\n\n\n\nRkt for Platform builders\n\n\n1 - Intro\n\n\n\n\nPODs\n\n\nAppC\n\n\nOCI\n\n\n\n\n\n\n2 - Docker \n Rkt\n\n\n\n\nRkt does NOT have a daemon\n\n-\n leave this to the Host INIT system mgnt (SystemD mainly)\n\n\n\n\nDocker daemon\n\n\n\n\nDaemon is a great \nuser\n experience\n\n\nBUT\n\n\nlack of composability\n\n\nHost integration for process mgnt, Unikernel, networking\n\n\nOK if you use the whole ecosystem (Compose, swarm, \n)\n\n\n\n\n\n\nProcess / Container MGNT especially (Restart Docker daemon -\n Restart all containers)\n\n\n\n\nRkt for Platform builder\n\n\n\n\nUnix philosophiy (Composable)\n\n\nDo not need ROOT permission\n\n\n\n\n3 - Rkt in practice\n\n\n\n\n\n\nRkt is a Container runtime, NOT imnage \nbuilder\n -\n Use \nACbuild\n tools\n\n\n\n\n\n\nRun\n container:\n\n\n\n\nRKT implement several \nStages1\n\n\n\n\nDefault is \nsystemd-nspawn\n\n  -\n Logs access is done via \nmachinectl\n / \njournalctl\n\n\n\n\n\n\nRkt can run container from \nDocker\n:\n\n\n\n\ndocker2aci\n tool\n\n\n\n\nDefault \nSquash\n layers\n\n\n\n\n\n\nImages\n discovery + Distribution:\n\n\n\n\nAPPC specs (HTTP + HTML Meta Tags)\n\n\n\n\n$ curl -sL https://quay.io/coreos/etcd \n|\n grep meta \n|\n grep discovery\n\n  \nmeta \nname\n=\nac-discovery\n \ncontent\n=\nquay.io https://quay.io/c1/aci/{name}/{version}/{ext}/{os}/{arch}/\n\n  \nmeta \nname\n=\nac-discovery-pubkeys\n \ncontent\n=\nquay.io https://quay.io/aci-signing-key\n\n\n\n\n\n\n\nPODs\n\n\nPopularized by k8s\n\n\nCollection of multiple Images sharing SOME namespaces\n\n\nDefault (Network, FS, IPC)\n\n\n\n\n$ sudo rkt list\n\nUUID App Image Name State CREATED STARTED NETWORKS\n...\n\n\n\n\n\n\nComments\n\n\nPOD\n\n- 1 UUID\n\n- Multiple App listed", 
            "title": "20160509 CoreOSFest 3"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#1-intro", 
            "text": "PODs  AppC  OCI", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#2-docker-rkt", 
            "text": "Rkt does NOT have a daemon \n-  leave this to the Host INIT system mgnt (SystemD mainly)", 
            "title": "2 - Docker &amp; Rkt"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#docker-daemon", 
            "text": "Daemon is a great  user  experience  BUT  lack of composability  Host integration for process mgnt, Unikernel, networking  OK if you use the whole ecosystem (Compose, swarm,  )    Process / Container MGNT especially (Restart Docker daemon -  Restart all containers)", 
            "title": "Docker daemon"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#rkt-for-platform-builder", 
            "text": "Unix philosophiy (Composable)  Do not need ROOT permission", 
            "title": "Rkt for Platform builder"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#3-rkt-in-practice", 
            "text": "Rkt is a Container runtime, NOT imnage  builder  -  Use  ACbuild  tools    Run  container:   RKT implement several  Stages1   Default is  systemd-nspawn \n  -  Logs access is done via  machinectl  /  journalctl    Rkt can run container from  Docker :   docker2aci  tool   Default  Squash  layers    Images  discovery + Distribution:   APPC specs (HTTP + HTML Meta Tags)   $ curl -sL https://quay.io/coreos/etcd  |  grep meta  |  grep discovery\n\n   meta  name = ac-discovery   content = quay.io https://quay.io/c1/aci/{name}/{version}/{ext}/{os}/{arch}/ \n   meta  name = ac-discovery-pubkeys   content = quay.io https://quay.io/aci-signing-key    PODs  Popularized by k8s  Collection of multiple Images sharing SOME namespaces  Default (Network, FS, IPC)   $ sudo rkt list\n\nUUID App Image Name State CREATED STARTED NETWORKS\n...", 
            "title": "3 - Rkt in practice"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-3/#comments", 
            "text": "POD \n- 1 UUID \n- Multiple App listed", 
            "title": "Comments"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/", 
            "text": "20160509_CoreOSFest-4\n\n\n\n\n\n\n\n1 - Intro\n\n\nRkt approach\n\n\nInside\n\n\nComposability\n\n\n2 - Standards\n\n\nOCI\n\n\nCNI\n\n\n3 - K8s\n\n\nOverview\n\n\nInsides\n\n\n4 - Rktnetes\n\n\nPb\n\n\nDocker 1.11\n\n\nRktnetes\n\n\n5 - Next\n\n\n\n\n\n\n\nRktnetes: Support updates\n\n\n1 - Intro\n\n\nRkt approach\n\n\nSecurity at core\n\n    - UX verify images signature\n\n    - UX verify images integrity\n\n\nUnix philosophy\n\n    - Well defined operations\n\n    - No central privilieges\n\n\nInside\n\n\nLinux tech inside\n\n- User namespace (euid Mapping)\n\n- SELinux\n\n\nSoon:\n\n- Capabilities\n\n- Seccomps\n\n- Cgroups2\n\n- Unprivileged containers\n\n\nComposability\n\n\nExternal\n composability for integration\n\n-\n Simple command progra\n\n\nInternal\n composability\n\n- Stage0: Rkt API\n\n- Stage1: POD abstraction\n\n- Stage2: Container engine\n\n\nStage1s\n\n- Cgroups + Linux namespaces\n\n- LKVM\n\n- Chroot (\nfly\n)\n\n- QEMU (soon)\n\n\n\n\nComments\n\n\nFly\n is especially usefull if you want to \nuse RKT as a pkg manager\n (for CoreOS by example and just run a process) at host level\n\n\n\n\n\n\n2 - Standards\n\n\nRKT emerged from need of standars, not depending of 1 Technology\n\n\n\n\nAppc well defined SPEC for images\n\n\nOCI\n\n\nCNF\n\n\nCNI\n\n\n\n\nOCI\n\n\nRkt is going to support OCI soon\n\n\nCNI\n\n\nHas been widely adopted\n\n\n3 - K8s\n\n\nOverview\n\n\nCluster-level container orchestration\n\n\n\n\nToday = Docker containers\n\n\n\n\n\n\nComments\n\n\nK8s expose the PODs API\n\n-\n No reason to be limited to docker\n\n\n\n\n\n\nInsides\n\n\n\n\nKublet is a daeons on every worker node\n\n\nRespond to deployment requests\n\n\nGive everything to the container engine\n\n\n\n\nSyncPod()\nGetPod()\nRunPod()\n...\n\n\n\n\n4 - Rktnetes\n\n\n\n\nPre: Kublet talks to Docker daemon API\n\n\n\n\nPb\n\n\n\n\nDocker does not understand PODs natively\n\n\nWorkaround with \nInfra container\n to hold namespace for POD\n\n\nDockerD is a SPOF\n\n\nDaemon stop, every container stops\n\n\nDocker does not integrat with SystemD (Cgroup mgnt)\n\n\n\n\nDocker 1.11\n\n\nDockerD +\u00a0ContainerD\n\n* ContainerD runs and Manage the containers\n\n* DockerD is still a daemon managing Images and API interface\n\n\n\n\nComments\n\n\nGood start, but \ncotnainerd\n is still a SPOF at the time\n\n\n\n\n\n\nRktnetes\n\n\nKublet + Rkt:\n\n* POD Native concept\n\n* Integration with SystemD of Host\n\n* POD process model (No SPOF)\n\n\nStack (Current):\n\n* Kublet: Main entry point\n\n* SystemD: For launching containers\n\n* Rkt (API service) for quering managing running containers\n\n\nNext ?\n\n* Kubelet + Rkt without SystemD direct interaction\n\n\nShould be released in k8s 1.2 late June\n\n\n5 - Next\n\n\n\n\nK8s want to be able to update / modify a runing pod (Add / Remove container from POD)\n\n\nLeveraging POD concept to systemD\n\n\nOCI\n\n\noctool", 
            "title": "20160509 CoreOSFest 4"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#rkt-approach", 
            "text": "Security at core \n    - UX verify images signature \n    - UX verify images integrity  Unix philosophy \n    - Well defined operations \n    - No central privilieges", 
            "title": "Rkt approach"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#inside", 
            "text": "Linux tech inside \n- User namespace (euid Mapping) \n- SELinux  Soon: \n- Capabilities \n- Seccomps \n- Cgroups2 \n- Unprivileged containers", 
            "title": "Inside"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#composability", 
            "text": "External  composability for integration \n-  Simple command progra  Internal  composability \n- Stage0: Rkt API \n- Stage1: POD abstraction \n- Stage2: Container engine  Stage1s \n- Cgroups + Linux namespaces \n- LKVM \n- Chroot ( fly ) \n- QEMU (soon)", 
            "title": "Composability"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#comments", 
            "text": "Fly  is especially usefull if you want to  use RKT as a pkg manager  (for CoreOS by example and just run a process) at host level", 
            "title": "Comments"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#2-standards", 
            "text": "RKT emerged from need of standars, not depending of 1 Technology   Appc well defined SPEC for images  OCI  CNF  CNI", 
            "title": "2 - Standards"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#oci", 
            "text": "Rkt is going to support OCI soon", 
            "title": "OCI"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#cni", 
            "text": "Has been widely adopted", 
            "title": "CNI"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#3-k8s", 
            "text": "", 
            "title": "3 - K8s"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#overview", 
            "text": "Cluster-level container orchestration   Today = Docker containers", 
            "title": "Overview"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#comments_1", 
            "text": "K8s expose the PODs API \n-  No reason to be limited to docker", 
            "title": "Comments"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#insides", 
            "text": "Kublet is a daeons on every worker node  Respond to deployment requests  Give everything to the container engine   SyncPod()\nGetPod()\nRunPod()\n...", 
            "title": "Insides"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#4-rktnetes", 
            "text": "Pre: Kublet talks to Docker daemon API", 
            "title": "4 - Rktnetes"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#pb", 
            "text": "Docker does not understand PODs natively  Workaround with  Infra container  to hold namespace for POD  DockerD is a SPOF  Daemon stop, every container stops  Docker does not integrat with SystemD (Cgroup mgnt)", 
            "title": "Pb"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#docker-111", 
            "text": "DockerD +\u00a0ContainerD \n* ContainerD runs and Manage the containers \n* DockerD is still a daemon managing Images and API interface", 
            "title": "Docker 1.11"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#comments_2", 
            "text": "Good start, but  cotnainerd  is still a SPOF at the time", 
            "title": "Comments"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#rktnetes", 
            "text": "Kublet + Rkt: \n* POD Native concept \n* Integration with SystemD of Host \n* POD process model (No SPOF)  Stack (Current): \n* Kublet: Main entry point \n* SystemD: For launching containers \n* Rkt (API service) for quering managing running containers  Next ? \n* Kubelet + Rkt without SystemD direct interaction  Should be released in k8s 1.2 late June", 
            "title": "Rktnetes"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-4/#5-next", 
            "text": "K8s want to be able to update / modify a runing pod (Add / Remove container from POD)  Leveraging POD concept to systemD  OCI  octool", 
            "title": "5 - Next"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/", 
            "text": "20160509_CoreOSFest-5\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - Containers\n\n\n3 - DGR\n\n\nConcepts\n\n\nProject\n\n\nCLI\n\n\n4 - Next\n\n\n\n\n\n\n\nDGR: An Appc image builder tool\n\n\n1 - Intro\n\n\n\n\nBlablacar\n\n\nExponential grow (+20M user in 2 years)\n\n\nOwn DC + Bare Metal\n\n\n\n\n2 - Containers\n\n\n\n\nCoreOS had pretty attractive \nconcepts\n\n\nAt the time \nRkt\n was new kid of the Block\n\n-\n Bleeding edges\n\n\n\n\nTries to build images:\n\n* Chef (We used chef at the time)\n\n* Packer\n\n* Acbuild\n\n-\n Build our own wrapper around \nacbuil\n to map Blabla workflow\n\n\n3 - DGR\n\n\nhttps://github.com/blablacar/dgr\n\n\nConcepts\n\n\n\n\nYAML format\n\n\nDependencies concepts (FRON Images)\n\n\nConfig Templating\n\n\nTest included\n\n\n\n\nProject\n\n\nPROJECT\n|\n\u251c\u2500\u2500 aci-manifest.yml\n|\n\u251c\u2500\u2500 attribures/\n\u251c\u2500\u2500 templates/          // Go templating\n|\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 runlevels/\n    \u2514\u2500\u2500 build/\n        \u2514\u2500\u2500 install.sh\n\n\n\n\ninstall.sh\n-----------\n\n#!bin/xxxx/busyx-box ...\n\nSHELL SCRIPTING\n\n\n\n\nCLI\n\n\ndgr init      // Bootstrap Project structure\ndgr try       // Template rendering\ndbr build\ndrg test\ndgr install   // In local RKT image store\ndgr push      // Only support for NEXUS TARG.GZ repo\n\n\n\n\n4 - Next\n\n\n\n\nSign ACI\n\n\nPush to other repos\n\n\nOrchestrate", 
            "title": "20160509 CoreOSFest 5"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#1-intro", 
            "text": "Blablacar  Exponential grow (+20M user in 2 years)  Own DC + Bare Metal", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#2-containers", 
            "text": "CoreOS had pretty attractive  concepts  At the time  Rkt  was new kid of the Block \n-  Bleeding edges   Tries to build images: \n* Chef (We used chef at the time) \n* Packer \n* Acbuild \n-  Build our own wrapper around  acbuil  to map Blabla workflow", 
            "title": "2 - Containers"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#3-dgr", 
            "text": "https://github.com/blablacar/dgr", 
            "title": "3 - DGR"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#concepts", 
            "text": "YAML format  Dependencies concepts (FRON Images)  Config Templating  Test included", 
            "title": "Concepts"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#project", 
            "text": "PROJECT\n|\n\u251c\u2500\u2500 aci-manifest.yml\n|\n\u251c\u2500\u2500 attribures/\n\u251c\u2500\u2500 templates/          // Go templating\n|\n\u251c\u2500\u2500 tests/\n\u2514\u2500\u2500 runlevels/\n    \u2514\u2500\u2500 build/\n        \u2514\u2500\u2500 install.sh  install.sh\n-----------\n\n#!bin/xxxx/busyx-box ...\n\nSHELL SCRIPTING", 
            "title": "Project"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#cli", 
            "text": "dgr init      // Bootstrap Project structure\ndgr try       // Template rendering\ndbr build\ndrg test\ndgr install   // In local RKT image store\ndgr push      // Only support for NEXUS TARG.GZ repo", 
            "title": "CLI"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-5/#4-next", 
            "text": "Sign ACI  Push to other repos  Orchestrate", 
            "title": "4 - Next"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/", 
            "text": "20160509_CoreOSFest-6\n\n\n\n\n\n\n\n1 - Pb\n\n\n2 - Squashed images\n\n\n4 - BitTorrent distributions\n\n\nWhy bittorrent?\n\n\nQuay.io implementation\n\n\nSecurity?\n\n\n5 - Perf benefits ?\n\n\n6 - Side projects\n\n\n7 - FAQ\n\n\n\n\n\n\n\nImage distributions\n\n\n1 - Pb\n\n\nHow images are pulled ?\n\n* Pull from a platform called \nRegistry\n\n* API over HTTP\n\n* Images have format (Docker images v1/v2 / ACI)\n\n\nDocker images\n\n* Layers\n\n* .tar.gz layers\n\n\nHow many request for images ?\n\n* List of Tags\n\n* Metadata\n\n* 1x for each parent (meh)\n\n* BLOB of each layer\n\n=\n 10 rqst for a 3 layers image\n\n\n2 - Squashed images\n\n\nDocker layers:\n\n* Each layer is .tar.gz\n\n* \ndocker load\n take .tar.gz\n\n-\n Real time squashing\n\n\nSquashing process\n\n* Parse each layers to not keep the deleted / overwritten files\n\n* Tar on the fly\n\n* Respond to client + Store squashed image (for later requests)\n\n\nSquashed downsides\n\n* No more benefits of shared layers\n\n* Realtime squashing is slow\n\n* LOAD cmd requieres more memory\n\n\n4 - BitTorrent distributions\n\n\nWhy bittorrent?\n\n\nPulling does not scale\n\n* Each Container node makes HTTP requests to the registry\n\n* But they are all requesting the same data\n\n=\n Share the data among the cluster\n\n\nTorrent today\n\n* Used for many content distribution\n\n* Scale at \ncore\n concept\n\n\nQuay.io implementation\n\n\nquayctl docker  torrent pull  xxx\nquayctl rkt     torrent fetch xxx\n\nquayctl rkt     torrent seed xxx --duration=YY\nquayctl rkt     torrent seed xxx --squashed\n\n\n\n\nSecurity?\n\n\n\n\nPrivate repo supports\n\n\nCredentials support\n\n\nTorrent swarm node trust support\n\n\n\n\n5 - Perf benefits ?\n\n\nTest: 30% Traffic reduction\n\n\n6 - Side projects\n\n\nBlog post: \nhttps://blog.quay.io/torrent/\n\nACI registry: \nhttps://github.com/containerops/dockyard\n\n\n7 - FAQ\n\n\n\n\nk8s integration?\n\n\n\n\nWIP\n\n\n\n\n\n\nOWn on premise BT tracker?\n\n\n\n\nQuay.io provide tracker service + Authentication\n\n\n\n\nProvide some OSS pieces if you want to run your own\n\n\n\n\n\n\nIPFS for distribution?\n\n\n\n\nWe have quikcly looked at it.\n\n\nStill young\n\n\nSome privacy issue at the time\n\n\nBut keep an eye on it", 
            "title": "20160509 CoreOSFest 6"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#1-pb", 
            "text": "How images are pulled ? \n* Pull from a platform called  Registry \n* API over HTTP \n* Images have format (Docker images v1/v2 / ACI)  Docker images \n* Layers \n* .tar.gz layers  How many request for images ? \n* List of Tags \n* Metadata \n* 1x for each parent (meh) \n* BLOB of each layer \n=  10 rqst for a 3 layers image", 
            "title": "1 - Pb"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#2-squashed-images", 
            "text": "Docker layers: \n* Each layer is .tar.gz \n*  docker load  take .tar.gz \n-  Real time squashing  Squashing process \n* Parse each layers to not keep the deleted / overwritten files \n* Tar on the fly \n* Respond to client + Store squashed image (for later requests)  Squashed downsides \n* No more benefits of shared layers \n* Realtime squashing is slow \n* LOAD cmd requieres more memory", 
            "title": "2 - Squashed images"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#4-bittorrent-distributions", 
            "text": "", 
            "title": "4 - BitTorrent distributions"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#why-bittorrent", 
            "text": "Pulling does not scale \n* Each Container node makes HTTP requests to the registry \n* But they are all requesting the same data \n=  Share the data among the cluster  Torrent today \n* Used for many content distribution \n* Scale at  core  concept", 
            "title": "Why bittorrent?"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#quayio-implementation", 
            "text": "quayctl docker  torrent pull  xxx\nquayctl rkt     torrent fetch xxx\n\nquayctl rkt     torrent seed xxx --duration=YY\nquayctl rkt     torrent seed xxx --squashed", 
            "title": "Quay.io implementation"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#security", 
            "text": "Private repo supports  Credentials support  Torrent swarm node trust support", 
            "title": "Security?"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#5-perf-benefits", 
            "text": "Test: 30% Traffic reduction", 
            "title": "5 - Perf benefits ?"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#6-side-projects", 
            "text": "Blog post:  https://blog.quay.io/torrent/ \nACI registry:  https://github.com/containerops/dockyard", 
            "title": "6 - Side projects"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-6/#7-faq", 
            "text": "k8s integration?   WIP    OWn on premise BT tracker?   Quay.io provide tracker service + Authentication   Provide some OSS pieces if you want to run your own    IPFS for distribution?   We have quikcly looked at it.  Still young  Some privacy issue at the time  But keep an eye on it", 
            "title": "7 - FAQ"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/", 
            "text": "20160509_CoreOSFest-7\n\n\n\n\n\n\n\n1 - CoreOS Company\n\n\n2 -  Intel\ns word\n\n\n3 - k8s updates\n\n\n4 - Network\n\n\n 5 - DTC aka \nDistributed Trust Computing\n\n\n\n\n\n\n\nKeynotes\n\n\n1 - CoreOS Company\n\n\n\n\nNew office in Berlin\n\n\nSeriesB rising $28 Millions\n\n\nWorking with Intel + Google\n\n\n\n\n2 -  Intel\ns word\n\n\nGIFEE\n\n* Google Infrastructure For EveryonE\n\n\nEven if Google seems big, enormous and scary\n\n* You can apply the same principles\n\n* Run from Laptop\n\n* To \nBig\n clusters\n\n\nGIFEE\n\n- Google offers Compute Engine over \nBorg\n\n- CoreOS offers OpenStack over k8s\n\n\n\n\nOpenStack components are applications like any others\n\n\n\n\n\n\nMonitoring\n\n- Work with Graphana Team\n\n\n3 - k8s updates\n\n\n\n\nNetwork policy\n\n\nSecurityGroup for the container\n\n\nScheduled for 1.3 late June (with RKT support)\n\n\n\n\n4 - Network\n\n\n\n\nCalico:   Network policy\n\n\nFlannel:  Overlay network\n\n=\n Announcing new project \nCanal\n\n\n\n\nFlannel plugable backends:\n\n- BGPG\n\n- IP\n\n- VXLAN\n\n\n5 - DTC aka \nDistributed Trust Computing\n\n\nFirmware and TPM\n\n- TPM: \nsignature\n of HW\n\n\nhttps://stackpoint.io", 
            "title": "20160509 CoreOSFest 7"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/#1-coreos-company", 
            "text": "New office in Berlin  SeriesB rising $28 Millions  Working with Intel + Google", 
            "title": "1 - CoreOS Company"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/#2-intels-word", 
            "text": "GIFEE \n* Google Infrastructure For EveryonE  Even if Google seems big, enormous and scary \n* You can apply the same principles \n* Run from Laptop \n* To  Big  clusters  GIFEE \n- Google offers Compute Engine over  Borg \n- CoreOS offers OpenStack over k8s   OpenStack components are applications like any others    Monitoring \n- Work with Graphana Team", 
            "title": "2 -  Intel's word"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/#3-k8s-updates", 
            "text": "Network policy  SecurityGroup for the container  Scheduled for 1.3 late June (with RKT support)", 
            "title": "3 - k8s updates"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/#4-network", 
            "text": "Calico:   Network policy  Flannel:  Overlay network \n=  Announcing new project  Canal   Flannel plugable backends: \n- BGPG \n- IP \n- VXLAN", 
            "title": "4 - Network"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-7/#5-dtc-aka-distributed-trust-computing", 
            "text": "Firmware and TPM \n- TPM:  signature  of HW  https://stackpoint.io", 
            "title": "5 - DTC aka \"Distributed Trust Computing\""
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-8/", 
            "text": "20160509_CoreOSFest-8\n\n\n\n\n\n\n\n1 - Numbers\n\n\n2 - How to stay sane\n\n\n3 - Security\n\n\n\n\n\n\n\nThe Linux kernel\n\n\nhttps://github.com/gregkh/kernel-development\n\n\n1 - Numbers\n\n\nRealse 4.5.0:\n\n- 53.000 files\n\n- 21 Millions lines of code\n\n\nBranch 4.x\n\n- 4000 contributors\n\n- 440 Companies\n\n\nPace\n\n- 10.000 Lines added\n\n- 5.000 lines removed\n\n- 2.000 lines modified\n\n\nper day !\n\n\n2 - How to stay sane\n\n\n\n\nTime based releases\n\n\nIncremental changes\n\n=\n We waste engineers times (because we have lot of them :p)\n\n\n\n\nGuarenties\n\n* 2 2/1 months release\n\n* Really accurate\n\n* No API broken, no feare to delay merge\n\n\nKernels flavor\n\n* LTS\n\n  - 2 years support\n\n  - Various versions (Fron Linux foundation, Distro communities)\n\n* Stable\n\n  - Hotfix every weeks\n\n  - Release every 2 1/2 years with new features\n\n* RCs\n\n  - Features every 1/2 weeks\n\n\n3 - Security\n\n\n\n\nEvery bug can be a \nsecurity\n issue\n\n\n\n\n\n\n=\n Fix ASAP\n\n=\n 10 Fix per day\n\n\nYou have to update software !\n\n\nhttps://slides.com/mricon/giant-bags-of-water\n\n\nhttp://outflux.net/slides/2015/ks/security.pdf\n\n\n\n\nChromeBox / CoreOS have a nice way to push update out !\n\n- Very secure\n\n- Very safe for users", 
            "title": "20160509 CoreOSFest 8"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-8/#1-numbers", 
            "text": "Realse 4.5.0: \n- 53.000 files \n- 21 Millions lines of code  Branch 4.x \n- 4000 contributors \n- 440 Companies  Pace \n- 10.000 Lines added \n- 5.000 lines removed \n- 2.000 lines modified  per day !", 
            "title": "1 - Numbers"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-8/#2-how-to-stay-sane", 
            "text": "Time based releases  Incremental changes \n=  We waste engineers times (because we have lot of them :p)   Guarenties \n* 2 2/1 months release \n* Really accurate \n* No API broken, no feare to delay merge  Kernels flavor \n* LTS \n  - 2 years support \n  - Various versions (Fron Linux foundation, Distro communities) \n* Stable \n  - Hotfix every weeks \n  - Release every 2 1/2 years with new features \n* RCs \n  - Features every 1/2 weeks", 
            "title": "2 - How to stay sane"
        }, 
        {
            "location": "/2016/coreosfest/day1/20160509_CoreOSFest-8/#3-security", 
            "text": "Every bug can be a  security  issue    =  Fix ASAP \n=  10 Fix per day  You have to update software !  https://slides.com/mricon/giant-bags-of-water  http://outflux.net/slides/2015/ks/security.pdf   ChromeBox / CoreOS have a nice way to push update out ! \n- Very secure \n- Very safe for users", 
            "title": "3 - Security"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/", 
            "text": "20160510_CoreOSFest-1\n\n\n\n\n\n\n\n1 - Intro\n\n\nDistributed systems are Hard\n\n\nRaft - Background\n\n\nFixing Bugs\n\n\nDesign Phase\n\n\n2 - Runway\n\n\nOverview\n\n\nDemo\n\n\nBuilding a Model\n\n\nSpec\n\n\n\n\n\n\n3 - Summary\n\n\n\n\n\n\n\nKeynote: Runway\n\n\n1 - Intro\n\n\nDistributed systems are Hard\n\n\n\n\nConcurrency + Delays\n\n\nFailures\n\n\nLots of events (expected or not)\n\n\n\n\n=\n Lack of visibility and debugging\n\n\nRaft - Background\n\n\n\n\nLeader election\n\n\nLogs replicated to followers\n\n\n\n\n=\n Complex setup and situations\n\n\nFixing Bugs\n\n\n\n\nMany techniques for Technical bugs\n\n\nNothing for DESIGN bug\n\n\n\n\nDesign Phase\n\n\n2 - Runway\n\n\nOverview\n\n\n\n\nSubmit a Model (according to Specs)\n\n\nCreates\n\n  - A simuator\n\n  - Model checker\n\n\nExecute the simulation\n\n\nCreates\n\n  - Visualization\n\n  - Data\n\n\n\n\nDemo\n\n\nBuilding a Model\n\n\nModel:\n\n* Spec (New kind of language)\n\n* View (JS, SVG, ..)\n\n\nSpec\n\n\n\n\n\n\nSpec IS code\n\n\n\n\n\n\nStarting State\n\n\n\n\ntransitional rules\n\n\ninvariants\n\n\n\n\n3 - Summary\n\n\n\n\nModeling help to focus on concepts\n\n\nRunway: 1 Tool to help design distributed systems", 
            "title": "20160510 CoreOSFest 1"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#distributed-systems-are-hard", 
            "text": "Concurrency + Delays  Failures  Lots of events (expected or not)   =  Lack of visibility and debugging", 
            "title": "Distributed systems are Hard"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#raft-background", 
            "text": "Leader election  Logs replicated to followers   =  Complex setup and situations", 
            "title": "Raft - Background"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#fixing-bugs", 
            "text": "Many techniques for Technical bugs  Nothing for DESIGN bug", 
            "title": "Fixing Bugs"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#design-phase", 
            "text": "", 
            "title": "Design Phase"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#2-runway", 
            "text": "", 
            "title": "2 - Runway"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#overview", 
            "text": "Submit a Model (according to Specs)  Creates \n  - A simuator \n  - Model checker  Execute the simulation  Creates \n  - Visualization \n  - Data", 
            "title": "Overview"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#demo", 
            "text": "", 
            "title": "Demo"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#building-a-model", 
            "text": "Model: \n* Spec (New kind of language) \n* View (JS, SVG, ..)", 
            "title": "Building a Model"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#spec", 
            "text": "Spec IS code    Starting State   transitional rules  invariants", 
            "title": "Spec"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-1/#3-summary", 
            "text": "Modeling help to focus on concepts  Runway: 1 Tool to help design distributed systems", 
            "title": "3 - Summary"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/", 
            "text": "20160510_CoreOSFest-2\n\n\n\n\n\n\n\nJWTProxy intro\n\n\nv1\n\n\nAuthentication mechanism\n\n\nIssues Feedbacks\n\n\nv2\n\n\nJWT\n\n\nDemo\n\n\nRSA notes\n\n\nFuture\n\n\n\n\n\n\n\nSecure auth between services\n\n\nJWTProxy intro\n\n\nMotivations:\n\n* Many services respect HTTP_PROXY env variables\n\n* Proxy allow to be language agnostics / app agnostic\n\n* Inspired by Oauth_proxy service\n\n\nDownside\n\n- 1 Extra network Hop\n\n\nv1\n\n\nAuthentication mechanism\n\n\nTLS Clients Certs (we considered)\n\n* Already used between Quay / clair\n\n* BUT PKI integration in enterprise in complicated\n\n* BUT When LB / other teams dependency\n\n=\n DROPPED\n\n\nHMAC\n\n* Hash of Message + Secret_key\n\n* Virtues\n\n  - No secrets needed in the requests\n\n  - Fast\n\n  - Request is self-signed\n\n\nIssues Feedbacks\n\n\n\n\nRequest signature\n\n-\n Headers are also signed !\n\n-\n BUT Extra headers from intern LB\n\n\n\n\nv2\n\n\nJWT\n\n\nVirtues\n\n* Secret is NERVER transmit\n\n* Signed\n\n* Resistant to replay attacks (Annonce mechanism)\n\n* Can be transmited in the clear\n\n* Request optionaly signed\n\n\n\n\nSSL Everywhere\n\n\n\n\nDemo\n\n\n\n\nUse \nrequest-bin\n that is opensource and can be self hosted to debug HTTP request\n\n\nStart JWTProxy service\n\n\nGenerate a RSA key pair locally\n\n\nSubmit to a PKI for approval + Wait\n\n\n\n\n\n\nRSA notes\n\n\n\n\nKey rotations easy\n\n\n\n\nFuture\n\n\n\n\nPerformance enhancement for RSA keypair generation\n\n\nIntegration for Auto-validation of keys\n\n\nCaching keys if PKI server is unavailable\n\n\nNonce\n protection", 
            "title": "20160510 CoreOSFest 2"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#jwtproxy-intro", 
            "text": "Motivations: \n* Many services respect HTTP_PROXY env variables \n* Proxy allow to be language agnostics / app agnostic \n* Inspired by Oauth_proxy service  Downside \n- 1 Extra network Hop", 
            "title": "JWTProxy intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#v1", 
            "text": "", 
            "title": "v1"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#authentication-mechanism", 
            "text": "TLS Clients Certs (we considered) \n* Already used between Quay / clair \n* BUT PKI integration in enterprise in complicated \n* BUT When LB / other teams dependency \n=  DROPPED  HMAC \n* Hash of Message + Secret_key \n* Virtues \n  - No secrets needed in the requests \n  - Fast \n  - Request is self-signed", 
            "title": "Authentication mechanism"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#issues-feedbacks", 
            "text": "Request signature \n-  Headers are also signed ! \n-  BUT Extra headers from intern LB", 
            "title": "Issues Feedbacks"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#v2", 
            "text": "", 
            "title": "v2"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#jwt", 
            "text": "Virtues \n* Secret is NERVER transmit \n* Signed \n* Resistant to replay attacks (Annonce mechanism) \n* Can be transmited in the clear \n* Request optionaly signed   SSL Everywhere", 
            "title": "JWT"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#demo", 
            "text": "Use  request-bin  that is opensource and can be self hosted to debug HTTP request  Start JWTProxy service  Generate a RSA key pair locally  Submit to a PKI for approval + Wait", 
            "title": "Demo"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#rsa-notes", 
            "text": "Key rotations easy", 
            "title": "RSA notes"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-2/#future", 
            "text": "Performance enhancement for RSA keypair generation  Integration for Auto-validation of keys  Caching keys if PKI server is unavailable  Nonce  protection", 
            "title": "Future"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/", 
            "text": "20160510_CoreOSFest-3\n\n\n\n\n\n\n\n1 - Intro\n\n\nFleet\n\n\nGiant-swarm\n\n\nLimitations of Fleet\n\n\n2 - Why still fleet?\n\n\n3 - What is next?\n\n\nIdeas\n\n\ngRPC\n\n\nNew Architecture\n\n\nRolling updates\n\n\n4 - Benefits\n\n\n5 - Validation\n\n\n6 - Future\n\n\n\n\n\n\n\nFleet improvement with gRPC\n\n\n1 - Intro\n\n\nFleet\n\n\nFleet: \nDistributed Init System\n\n- Leverage SystemD units\n\n- Use ETCD as distributed system\n\n\nFleet:\n\n- \nfleetd\n: Daemon runningon every nodes, watching etcd\n\n- \nfleetctl\n: Client cli to submit units\n\n\nGiant-swarm\n\n\n\n\n1 Services.json main ail\n\n\nEach servies used several Units\n\n\n\n\nLimitations of Fleet\n\n\n\n\nPerformances (due to ETCD)\n\n\nFleet \nout of sync\n\n\nSome agents are unresponsive\n\n\nSome agents are incoherents\n\n\n\n\n=\n Limit of Scalability\n\n\nLimit of FAIL\n\n- If \netcd\n goes down, eveything falls appart\n\n\n2 - Why still fleet?\n\n\n\n\nSimple\n\n\nIntuitive usage of \nsystemd\n\n\n\n\n=\n Fleet helps to control basic blocs of infrastructure\n\n  - Kubernetes / Mesos components\n\n  - Others stuff that do not have complex workflows, lifecycle\n\n\n3 - What is next?\n\n\nIdeas\n\n\n\n\nReduce etcd dependencies\n\n\nBetter perf\n\n\nImprove failures recovery\n\n\nMitigate scalability issues\n\n\n\n\ngRPC\n\n\n\n\ngRPC is now used as communication layer between \nfleetd\n\n\netcd\n is used only for storage\n\n=\n Perf and dependencies\n\n\n\n\nNew Architecture\n\n\nFleetd Engine\n\n- \nServers\n mode\n\n- Only \nmaster\n talks to etcd\n\n\nFleetd agents\n\n- talks to Engines cluster via gRPC\n\n\nRolling updates\n\n\n\n\nEngine/Agents detec if there are \netcd\n nodes in the cluster\n\n\nImplement \nLock\n mechanism on etcd\n\n\n\n\n4 - Benefits\n\n\n5 - Validation\n\n\nConfig\n\n* 3 nodes Engines cluster\n\n* 4 nodes Agents cluster\n\n\nScenario\n\n1. 3k units comparison \nfleet-etcd\n / \nfleet-grpc\n\n2. Fleet vs systemd (interaction)\n\n3.\n\n\n\n\nNotes\n\n\nSystemd \nunits linking\n is very EXPENSIVE\n\n\n\n\n\n\n\n\nPerformance improvements\n\n\n\n\n6 - Future\n\n\n\n\netcd v3\n\n\nTLS\n\n\nImprove \nstate transition\n of units\n\n\nChange \nReconciliation loop\n mechanism", 
            "title": "20160510 CoreOSFest 3"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#fleet", 
            "text": "Fleet:  Distributed Init System \n- Leverage SystemD units \n- Use ETCD as distributed system  Fleet: \n-  fleetd : Daemon runningon every nodes, watching etcd \n-  fleetctl : Client cli to submit units", 
            "title": "Fleet"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#giant-swarm", 
            "text": "1 Services.json main ail  Each servies used several Units", 
            "title": "Giant-swarm"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#limitations-of-fleet", 
            "text": "Performances (due to ETCD)  Fleet  out of sync  Some agents are unresponsive  Some agents are incoherents   =  Limit of Scalability  Limit of FAIL \n- If  etcd  goes down, eveything falls appart", 
            "title": "Limitations of Fleet"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#2-why-still-fleet", 
            "text": "Simple  Intuitive usage of  systemd   =  Fleet helps to control basic blocs of infrastructure \n  - Kubernetes / Mesos components \n  - Others stuff that do not have complex workflows, lifecycle", 
            "title": "2 - Why still fleet?"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#3-what-is-next", 
            "text": "", 
            "title": "3 - What is next?"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#ideas", 
            "text": "Reduce etcd dependencies  Better perf  Improve failures recovery  Mitigate scalability issues", 
            "title": "Ideas"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#grpc", 
            "text": "gRPC is now used as communication layer between  fleetd  etcd  is used only for storage \n=  Perf and dependencies", 
            "title": "gRPC"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#new-architecture", 
            "text": "Fleetd Engine \n-  Servers  mode \n- Only  master  talks to etcd  Fleetd agents \n- talks to Engines cluster via gRPC", 
            "title": "New Architecture"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#rolling-updates", 
            "text": "Engine/Agents detec if there are  etcd  nodes in the cluster  Implement  Lock  mechanism on etcd", 
            "title": "Rolling updates"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#4-benefits", 
            "text": "", 
            "title": "4 - Benefits"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#5-validation", 
            "text": "Config \n* 3 nodes Engines cluster \n* 4 nodes Agents cluster  Scenario \n1. 3k units comparison  fleet-etcd  /  fleet-grpc \n2. Fleet vs systemd (interaction) \n3.", 
            "title": "5 - Validation"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#notes", 
            "text": "Systemd  units linking  is very EXPENSIVE     Performance improvements", 
            "title": "Notes"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-3/#6-future", 
            "text": "etcd v3  TLS  Improve  state transition  of units  Change  Reconciliation loop  mechanism", 
            "title": "6 - Future"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/", 
            "text": "20160510_CoreOSFest-4\n\n\n\n\n\n\n\n1 - Intro\n\n\nPhase 0\n\n\nPhase 1\n\n\nPhase 2\n\n\nPhase 3\n\n\n2 - Dockyard\n\n\n\n\n\n\n\nDockyard - Container Registry And Volume Management For rkt\n\n\n1 - Intro\n\n\nPhase 0\n\n\nDocker\n\n* Leveraged LXC\n\n* Avantages:\n\n  - REST API\n\n  - \nRegistry\n\n  - AUFS\n\n\n=\n Great UX\n\n=\n Greate User adoption\n\n\nDocker Registry\n\n* v1\n\n  - Index + resgistry\n\n\nPhase 1\n\n\nRkt + ACI\n\n* Advanced concepts of container and decoupling, composability\n\n* Advanced concepts of containers images formats\n\n\nPhase 2\n\n\nDocker Registry v2\n\n* Index + Image stockage\n\n* + \nAuthorization service\n\n\nPhase 3\n\n\nOCI initiative\n\n\n2 - Dockyard\n\n\nhttps://github.com/containersops/dockyard\n\n\nGeneric registry for both world\n\n* Docker / Rkt ACI\n\n* HTTP(s) / BitTorrent distributions support\n\n\n+ Built-in Object Storage service", 
            "title": "20160510 CoreOSFest 4"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#phase-0", 
            "text": "Docker \n* Leveraged LXC \n* Avantages: \n  - REST API \n  -  Registry \n  - AUFS  =  Great UX \n=  Greate User adoption  Docker Registry \n* v1 \n  - Index + resgistry", 
            "title": "Phase 0"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#phase-1", 
            "text": "Rkt + ACI \n* Advanced concepts of container and decoupling, composability \n* Advanced concepts of containers images formats", 
            "title": "Phase 1"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#phase-2", 
            "text": "Docker Registry v2 \n* Index + Image stockage \n* +  Authorization service", 
            "title": "Phase 2"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#phase-3", 
            "text": "OCI initiative", 
            "title": "Phase 3"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-4/#2-dockyard", 
            "text": "https://github.com/containersops/dockyard  Generic registry for both world \n* Docker / Rkt ACI \n* HTTP(s) / BitTorrent distributions support  + Built-in Object Storage service", 
            "title": "2 - Dockyard"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/", 
            "text": "20160510_CoreOSFest-5\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - \nMgnt\n tool\n\n\nDesign\n\n\n3 - Notes\n\n\nEvents\n\n\nDistributed\n\n\nAuto-dependencies\n\n\n4 - Demo\n\n\nAuto-clustering\n\n\nReal-time updates\n\n\n5 - Future\n\n\n\n\n\n\n\nNextGen Config Mgnt\n\n\n1 - Intro\n\n\nhttps://github.com/purpleidea\n\n\nHacked a lot around Puppet\n\n* Recursion\n\n* Timers\n\n* State machine\n\n\nBut HACKY\n\n\n2 - \nMgnt\n tool\n\n\nDesign\n\n\n\n\nParallel\n\n\nEvent driven\n\n\nDistributed\n\n\n\n\n3 - Notes\n\n\nEvents\n\n\n\n\nHooks with iNotify\n\n\nHooks with SystemD\n\n\n\n\nDistributed\n\n\n\n\nBased on ETCD for shared / exported resources\n\n\n\n\nAuto-dependencies\n\n\n\n\nAvailable to leverage OS packages Mgnt to find dependencies\n\n\n\n\n4 - Demo\n\n\nAuto-clustering\n\n\n\n\nTrue\n elastic cluter mgnt\n\n - Can start additionnal member that won\nt be part of the cluster IF max_peers already reached\n\n - If 1 peer leave, the next candidate will join\n\n\n\n\n\n\nReal-time updates\n\n\nThanks to Events\n\n\n5 - Future\n\n\n\n\nDSL\n\n\nResources", 
            "title": "20160510 CoreOSFest 5"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#1-intro", 
            "text": "https://github.com/purpleidea  Hacked a lot around Puppet \n* Recursion \n* Timers \n* State machine  But HACKY", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#2-mgnt-tool", 
            "text": "", 
            "title": "2 - 'Mgnt' tool"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#design", 
            "text": "Parallel  Event driven  Distributed", 
            "title": "Design"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#3-notes", 
            "text": "", 
            "title": "3 - Notes"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#events", 
            "text": "Hooks with iNotify  Hooks with SystemD", 
            "title": "Events"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#distributed", 
            "text": "Based on ETCD for shared / exported resources", 
            "title": "Distributed"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#auto-dependencies", 
            "text": "Available to leverage OS packages Mgnt to find dependencies", 
            "title": "Auto-dependencies"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#4-demo", 
            "text": "", 
            "title": "4 - Demo"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#auto-clustering", 
            "text": "True  elastic cluter mgnt \n - Can start additionnal member that won t be part of the cluster IF max_peers already reached \n - If 1 peer leave, the next candidate will join", 
            "title": "Auto-clustering"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#real-time-updates", 
            "text": "Thanks to Events", 
            "title": "Real-time updates"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-5/#5-future", 
            "text": "DSL  Resources", 
            "title": "5 - Future"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/", 
            "text": "20160510_CoreOSFest-6\n\n\n\n\n\n\n\n1 - Intro\n\n\nk8s\n\n\nAPI server\n\n\n2 - Auth Flow\n\n\nAuthN (Authentication)\n\n\nExisting plugins\n\n\nDex\n\n\n\n\n\n\nAuthZ (permissions)\n\n\nExisting plugins\n\n\nNew\n\n\n\n\n\n\n\n\n\n\n\nKubernes + Dex\n\n\n1 - Intro\n\n\nk8s\n\n\nWorker\n\n* Kubelet (worker)\n\n* Proxy (Iptables manager on Host)\n\n* Container engine (Docker / Rkt)\n\n\nK8s Control Plane\n\n* \nAPI server\n\n* Controller\n\n* Scheduler\n\n\nAdmin CLI\n\n* REST API\n\n* \nkubectl\n\n\n\n\nRelies on \netcd\n for state\n\n\n\n\nAPI server\n\n\n\n\nMain block (Everything talks to the API server)\n\n\nAuth PLUGIN mechanism\n\n\n\n\n2 - Auth Flow\n\n\nAuthN (Authentication)\n\n\nExisting plugins\n\n\nAuthN plugins\n\n* x509\n\n* Passwords / Tokens files\n\n* OpenStack Keystone\n\n* Built-in \nService Accounts\n\n* Token webhooks (outside source)\n\n* OpenID\n\n* \n\n\nDex\n\n\nFederation:\n\n* K8s -\n Dex -\n OpenLDAP, Github, Google, ....\n\n\nAuthZ (permissions)\n\n\nExisting plugins\n\n\n\n\nABAC\n\n\nJSON static policy file\n\n\nWebhook\n\n\nOutside source\n\n\n\n\n\n\nWhat happens when remote service dies ?\n\n\n\n\nUser: use remote service\n\n\nWorkers nodes: use ABAC style\n\n\n\n\n\n\n\n\nNew\n\n\n\n\nRBAC\n\n\nYAML format\n\n\n\n\nin k8s 1.3\n\n\n\n\n\n\nKind of permissions\n\n\n\n\nAdminControl\n\n\nResourcesQuotas\n\n\nLimites (Pods point of view)", 
            "title": "20160510 CoreOSFest 6"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#k8s", 
            "text": "Worker \n* Kubelet (worker) \n* Proxy (Iptables manager on Host) \n* Container engine (Docker / Rkt)  K8s Control Plane \n*  API server \n* Controller \n* Scheduler  Admin CLI \n* REST API \n*  kubectl   Relies on  etcd  for state", 
            "title": "k8s"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#api-server", 
            "text": "Main block (Everything talks to the API server)  Auth PLUGIN mechanism", 
            "title": "API server"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#2-auth-flow", 
            "text": "", 
            "title": "2 - Auth Flow"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#authn-authentication", 
            "text": "", 
            "title": "AuthN (Authentication)"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#existing-plugins", 
            "text": "AuthN plugins \n* x509 \n* Passwords / Tokens files \n* OpenStack Keystone \n* Built-in  Service Accounts \n* Token webhooks (outside source) \n* OpenID \n*", 
            "title": "Existing plugins"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#dex", 
            "text": "Federation: \n* K8s -  Dex -  OpenLDAP, Github, Google, ....", 
            "title": "Dex"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#authz-permissions", 
            "text": "", 
            "title": "AuthZ (permissions)"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#existing-plugins_1", 
            "text": "ABAC  JSON static policy file  Webhook  Outside source", 
            "title": "Existing plugins"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#what-happens-when-remote-service-dies", 
            "text": "User: use remote service  Workers nodes: use ABAC style", 
            "title": "What happens when remote service dies ?"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-6/#new", 
            "text": "RBAC  YAML format   in k8s 1.3    Kind of permissions   AdminControl  ResourcesQuotas  Limites (Pods point of view)", 
            "title": "New"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-7/", 
            "text": "20160510_CoreOSFest-7\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - Sec features\n\n\n\n\n\n\n\nKeynote: Security in systemd\n\n\n1 - Intro\n\n\nSandbox everything\n\n* Not only containers, system services too\n\n\nWhat diff \nSystem service\n vs \nContainer\n ?\n\n\nSystemd-nspawn\n\n* User namespace that work\n\n\n2 - Sec features\n\n\nPrivateTmp=yes|no\n\n* Private instance of \n/tmp\n and \n/var/tmp\n\n* Lifecycle is bound to service runtimes\n\n\nJoinsNamespaceOf=\n\n* Allow IPC between services that use some UNIX sockets in \n/tmp\n\n\nCapabilityBoundingSet=\n\n* FlagBits to give very specific permissions (Example, NTP priviliges to set Host clock)\n\n\nAmbiantCapabilities=\n\n* Heritage of capabilities\n\n\nPrivateDevices=yes|no\n\n* Shared devices are \n/dev/null\n and so on that are part of Unix API\n\n\nProtectHome=yes|no|read-only\n\n* If service is exploited, won\nt be able to access admin home and access keys or some private stuff\n\n\nMountFlags=false\n\n* Capacity to edit Mount table file\n\n\nReadWriteDirectories=\nReadOnlyDirectories=\nInaccessilblesDirectories=", 
            "title": "20160510 CoreOSFest 7"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-7/#1-intro", 
            "text": "Sandbox everything \n* Not only containers, system services too  What diff  System service  vs  Container  ?  Systemd-nspawn \n* User namespace that work", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/coreosfest/day2/20160510_CoreOSFest-7/#2-sec-features", 
            "text": "PrivateTmp=yes|no \n* Private instance of  /tmp  and  /var/tmp \n* Lifecycle is bound to service runtimes  JoinsNamespaceOf= \n* Allow IPC between services that use some UNIX sockets in  /tmp  CapabilityBoundingSet= \n* FlagBits to give very specific permissions (Example, NTP priviliges to set Host clock)  AmbiantCapabilities= \n* Heritage of capabilities  PrivateDevices=yes|no \n* Shared devices are  /dev/null  and so on that are part of Unix API  ProtectHome=yes|no|read-only \n* If service is exploited, won t be able to access admin home and access keys or some private stuff  MountFlags=false \n* Capacity to edit Mount table file  ReadWriteDirectories=\nReadOnlyDirectories=\nInaccessilblesDirectories=", 
            "title": "2 - Sec features"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/", 
            "text": "20160510_Coreosfest_notes-1\n\n\n\n\n\n\n\n1 - coreos-baremetal\n\n\nOverview\n\n\ncoreos-baremetal/bootcfg\n\n\nNetwork Environment setup\n\n\n2 - Tests\n\n\nInfra setup\n\n\nBootcfg API\n\n\n\n\n\n\n\nCoreOS Bootstraping\n\n\n1 - coreos-baremetal\n\n\nOverview\n\n\nhttps://github.com/coreos/coreos-baremetal\n\n\nCoreos-baremetal:\n\n* Guides\n\n* 1 service (\nbootcfg\n)\n\n\nbootcfg\n:\n\n* Network boot config\n\n* Provisioning CoreOS clusters\n\n* Metadata\n\n\ncoreos-baremetal/bootcfg\n\n\nhttps://github.com/coreos/coreos-baremetal/blob/master/Documentation/bootcfg.md\n\n\nHTTP server\n\n* iPXE scripts (Kernel + Initrd)\n\n* Ignitions / cloud-config config files (Bootstraping)\n\n* Metadata / Logic\n\n\n/var/lib/bootcfg/\n |\n \u251c\u2500\u2500 assets/\n |   \u2514\u2500\u2500 coreos\n |        \u2514\u2500\u2500 VERSION\n |            \u251c\u2500\u2500 coreos_production_pxe.vmlinuz\n |            \u2514\u2500\u2500 coreos_production_pxe_image.cpio.gz\n |\n |\n |   // Network boot config\n \u251c\u2500\u2500 groups/\n \u2502   \u2514\u2500\u2500 default.json\n \u2502   \u2514\u2500\u2500 node1.json\n \u2502   \u2514\u2500\u2500 us-central1-a.json\n |\n \u2514\u2500\u2500 profiles/\n \u2502   \u2514\u2500\u2500 etcd.json\n \u2502   \u2514\u2500\u2500 worker.json\n |\n |\n |   // CoreOS Config specific\n \u251c\u2500\u2500 cloud\n \u2502   \u251c\u2500\u2500 cloud.yaml\n \u2502   \u2514\u2500\u2500 worker.sh\n \u2514\u2500\u2500 ignition\n     \u2514\u2500\u2500 hello.json\n     \u2514\u2500\u2500 etcd.yaml\n     \u2514\u2500\u2500 simple_networking.yaml\n\n\n\n\n\n\nGroups\n\n\n1 Profile\n\n\n1 \nSelector\n (HW / Metadata logics)\n\n\n\n\nMetadata associated for Profile templating\n\n\n\n\n\n\nProfil\n\n\n\n\nIgnition / cloud-config\n\n\nNetwork BOOT args (iPXE)\n\n\n\n\nNetwork Environment setup\n\n\n\n\nNO DHCP\n\n\nNO TFTP\n\n\n\n\n=\n Use DNSMasq\n\n\n2 - Tests\n\n\nInfra setup\n\n\n\n\n1 CoreOS VM\n\n\nBootCFG HTTP servers\n\n\n\n\nDnsmasq DHCP/TFTP server\n\n\n\n\n\n\n1 VBox VM PXE Boot\n\n\n\n\n\n\nBootcfg API\n\n\nhttps://github.com/coreos/coreos-baremetal/blob/master/Documentation/api.md\n\n\n=\n Allow to validate each part of Bootcfg\n\n  * ipxe / ipxe script\n\n  * Ignition Config\n\n  * Metadata\n\n  * Images", 
            "title": "20160510 Coreosfest notes 1"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#1-coreos-baremetal", 
            "text": "", 
            "title": "1 - coreos-baremetal"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#overview", 
            "text": "https://github.com/coreos/coreos-baremetal  Coreos-baremetal: \n* Guides \n* 1 service ( bootcfg )  bootcfg : \n* Network boot config \n* Provisioning CoreOS clusters \n* Metadata", 
            "title": "Overview"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#coreos-baremetalbootcfg", 
            "text": "https://github.com/coreos/coreos-baremetal/blob/master/Documentation/bootcfg.md  HTTP server \n* iPXE scripts (Kernel + Initrd) \n* Ignitions / cloud-config config files (Bootstraping) \n* Metadata / Logic  /var/lib/bootcfg/\n |\n \u251c\u2500\u2500 assets/\n |   \u2514\u2500\u2500 coreos\n |        \u2514\u2500\u2500 VERSION\n |            \u251c\u2500\u2500 coreos_production_pxe.vmlinuz\n |            \u2514\u2500\u2500 coreos_production_pxe_image.cpio.gz\n |\n |\n |   // Network boot config\n \u251c\u2500\u2500 groups/\n \u2502   \u2514\u2500\u2500 default.json\n \u2502   \u2514\u2500\u2500 node1.json\n \u2502   \u2514\u2500\u2500 us-central1-a.json\n |\n \u2514\u2500\u2500 profiles/\n \u2502   \u2514\u2500\u2500 etcd.json\n \u2502   \u2514\u2500\u2500 worker.json\n |\n |\n |   // CoreOS Config specific\n \u251c\u2500\u2500 cloud\n \u2502   \u251c\u2500\u2500 cloud.yaml\n \u2502   \u2514\u2500\u2500 worker.sh\n \u2514\u2500\u2500 ignition\n     \u2514\u2500\u2500 hello.json\n     \u2514\u2500\u2500 etcd.yaml\n     \u2514\u2500\u2500 simple_networking.yaml   Groups  1 Profile  1  Selector  (HW / Metadata logics)   Metadata associated for Profile templating    Profil   Ignition / cloud-config  Network BOOT args (iPXE)", 
            "title": "coreos-baremetal/bootcfg"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#network-environment-setup", 
            "text": "NO DHCP  NO TFTP   =  Use DNSMasq", 
            "title": "Network Environment setup"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#2-tests", 
            "text": "", 
            "title": "2 - Tests"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#infra-setup", 
            "text": "1 CoreOS VM  BootCFG HTTP servers   Dnsmasq DHCP/TFTP server    1 VBox VM PXE Boot", 
            "title": "Infra setup"
        }, 
        {
            "location": "/2016/coreosfest/notes/20160510_Coreosfest_notes-1/#bootcfg-api", 
            "text": "https://github.com/coreos/coreos-baremetal/blob/master/Documentation/api.md  =  Allow to validate each part of Bootcfg \n  * ipxe / ipxe script \n  * Ignition Config \n  * Metadata \n  * Images", 
            "title": "Bootcfg API"
        }, 
        {
            "location": "/2016/fosdem/README/", 
            "text": "notes-fosdem-2016\n\n\nNotes from FOSDEM 2016\n\n\nFork, contribute :)", 
            "title": "notes-fosdem-2016"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/", 
            "text": "1-Minio_S3Like_ObjectStorage_inGo\n\n\n\n\n\n\n\nIntro\n\n\nDemo / Workshop nodes\n\n\nWhy Golang\n\n\nTips / Notes\n\n\n\n\n\n\n\nIntro\n\n\nSpeaker\n\n  - Worked on GlusterFS\n\n  - Now, new project: Minio\n\n\n\n\nFun-facts\n\n\nGo is great !\n\n- Static binary, can be put on server withour depending of some distro packages\n\n- Small, short, efficient: for CLI and admins tools\n\n=\n Allowed to create easily CoreUtils like commands to for client side usage of Minio\n\n\n\n\nDemo / Workshop nodes\n\n\n\n\nStart the Minio server locally\n\n\n\n\n./minio server ~/work\n\n  =\n Start a minio server process that \npublish\n the \nwork\n directory as an object storage bucket\n\n  =\n Start a WebUI: \nhttp://127.0.0.1:9001\n\n  =\n Everything from a static binary\n\n\n\n\n\n\nOps and usage\n\n\n\n\n\n\n$ mkdir\n$ mc cp -r DIR1 DR2             // Give a progress bar ncurse like\n$ mc session resume QWdqwden    // Resume when Network issue\n\n\n\n\nWhy Golang\n\n\nChoices ?\n\n\n\n\n\n\nC++ for Core and Google v8 for WebUI\n\n  - Was difficult to matained\n\n  - JS / C++ really different\n\n\n\n\n\n\nGo\n\n  - Efficient\n\n  - Popular, Nice community\n\n  - Suites our use case (+/- portable)\n\n\n\n\n\n\nRust\n\n  - Good language\n\n  - But can do samethings in Go\n\n\n\n\n\n\nHaskell\n\n  - Small comminuty\n\n  - Good if we made a ONLY hosted product\n\n\n\n\n\n\nC and Python\n\n\n\n\n\n\nC and Gnu Guile\n\n\n\n\n\n\nJava\n\n  - Good language, and runtime, and portability\n\n  - Some ObjectStorage are in Java (HDFS, \n)\n\n  - BUT jvm dependency \n\n  - BUT JVM Overhead for the Client CLI utilities\n\n\n\n\n\n\nSummary: \n\nGo\n\n- C like feeling\n\n- Python ease\n\n- Java \nportability\n\n\n\n\nAlso really need the ease of usage and the community (Aws s3 \nconcurent\n)\n\n\n\n\n\n\nTips / Notes\n\n\n\n\nWarning: $GOPATH / $GOROOT\n\n\nExpected: $GO15VENDOREXPERIMENT\n\n\nMakefile is your friend :)\n\n\nError Handling\n\n\nDebug   (go package)\n\n\nLogrus  (go package)", 
            "title": "1 Minio S3Like ObjectStorage inGo"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/#intro", 
            "text": "Speaker \n  - Worked on GlusterFS \n  - Now, new project: Minio", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/#fun-facts", 
            "text": "Go is great ! \n- Static binary, can be put on server withour depending of some distro packages \n- Small, short, efficient: for CLI and admins tools \n=  Allowed to create easily CoreUtils like commands to for client side usage of Minio", 
            "title": "Fun-facts"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/#demo-workshop-nodes", 
            "text": "Start the Minio server locally   ./minio server ~/work \n  =  Start a minio server process that  publish  the  work  directory as an object storage bucket \n  =  Start a WebUI:  http://127.0.0.1:9001 \n  =  Everything from a static binary    Ops and usage    $ mkdir\n$ mc cp -r DIR1 DR2             // Give a progress bar ncurse like\n$ mc session resume QWdqwden    // Resume when Network issue", 
            "title": "Demo / Workshop nodes"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/#why-golang", 
            "text": "Choices ?    C++ for Core and Google v8 for WebUI \n  - Was difficult to matained \n  - JS / C++ really different    Go \n  - Efficient \n  - Popular, Nice community \n  - Suites our use case (+/- portable)    Rust \n  - Good language \n  - But can do samethings in Go    Haskell \n  - Small comminuty \n  - Good if we made a ONLY hosted product    C and Python    C and Gnu Guile    Java \n  - Good language, and runtime, and portability \n  - Some ObjectStorage are in Java (HDFS,  ) \n  - BUT jvm dependency  \n  - BUT JVM Overhead for the Client CLI utilities    Summary:  \nGo \n- C like feeling \n- Python ease \n- Java  portability   Also really need the ease of usage and the community (Aws s3  concurent )", 
            "title": "Why Golang"
        }, 
        {
            "location": "/2016/fosdem/Go-devRoom/1-Minio_S3Like_ObjectStorage_inGo/#tips-notes", 
            "text": "Warning: $GOPATH / $GOROOT  Expected: $GO15VENDOREXPERIMENT  Makefile is your friend :)  Error Handling  Debug   (go package)  Logrus  (go package)", 
            "title": "Tips / Notes"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/", 
            "text": "1-Homebrew_differently\n\n\n\n\n\n\n\n1 - We use github fork-based contribution\n\n\n2 - Ruby DSL\n\n\n3 - Does not use ROOT user\n\n\n4 - Version manager\n\n\n5 - Avoid patching projects\n\n\n6 - Accepts niche / New projects\n\n\n\n\n\n\n\n1 - We use github fork-based contribution\n\n\nPro:\n\n++ Rely on community\n\n++ Rely on upstream / vanilla code\n\n++ Rely on code scm issues\n\n\nCons:\n\n\n Lots of contributor\n\n\n lots of PR to review\n\n\n2 - Ruby DSL\n\n\n\n\nStill Ruby (Allow quick and easy hack)\n\n\nStill a DSL \ndescribing\n\n=\n Easy\n\n\n\n\n3 - Does not use ROOT user\n\n\n\n\nLeverage /usr/local\n\n\nFocus on user in userspace\n\n\n\n\n4 - Version manager\n\n\n\n\nRely on Git\n\n\nUse prefix \npkg_name/version/\n\n\nAllow quick and easy upgrade / rollback process\n\n\n\n\n5 - Avoid patching projects\n\n\n\n\nMost vanilla as possible\n\n\nOr use TAP mechanisme\n\n\n\n\n6 - Accepts niche / New projects", 
            "title": "1 Homebrew differently"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#1-we-use-github-fork-based-contribution", 
            "text": "Pro: \n++ Rely on community \n++ Rely on upstream / vanilla code \n++ Rely on code scm issues  Cons:   Lots of contributor   lots of PR to review", 
            "title": "1 - We use github fork-based contribution"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#2-ruby-dsl", 
            "text": "Still Ruby (Allow quick and easy hack)  Still a DSL  describing \n=  Easy", 
            "title": "2 - Ruby DSL"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#3-does-not-use-root-user", 
            "text": "Leverage /usr/local  Focus on user in userspace", 
            "title": "3 - Does not use ROOT user"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#4-version-manager", 
            "text": "Rely on Git  Use prefix  pkg_name/version/  Allow quick and easy upgrade / rollback process", 
            "title": "4 - Version manager"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#5-avoid-patching-projects", 
            "text": "Most vanilla as possible  Or use TAP mechanisme", 
            "title": "5 - Avoid patching projects"
        }, 
        {
            "location": "/2016/fosdem/Lightning-talks/1-Homebrew_differently/#6-accepts-niche-new-projects", 
            "text": "", 
            "title": "6 - Accepts niche / New projects"
        }, 
        {
            "location": "/2016/fosdem/MySQL/1-MySQL_Orchestrator/", 
            "text": "1-MySQL_Orchestrator\n\n\nOrchestrator: Smart MySQL Replication management\n\n\n\n\n\n\n\nFeatures\n\n\nSummary\n\n\nNotes\n\n\n\n\n\n\n\nFeatures\n\n\n\n\n\n\nBuilt from real-world use cases\n\n\n\n\n\n\nSupport multi-layer Master/Slave replication topology\n\n\n\n\n\n\nTopology discovery from 1 server\n\n\n\n\n\n\nFailure detection Euristic approch    \n\n\n\n\nAsk each member of the cluster to get a full pictures\n\n\nShow slave status / show masterstatus\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAllow to support network split issue\n\n  - If orchestrator can not connect to 1 server, it may be a network issue\n\n  =\n Ask other servers of the cluster to make the descision\n\n\n\n\n\n\nRecovery + Promotion\n\n\nBy talking to each server: Can know which is the less late to promote\n\n\nDC aware\n\n\nAdmin weight\n\n\n\n\nSummary\n\n\n\n\nSupported\n\n\nMulti-DC\n\n\nVarious GTID flavour\n\n\nFlapping behavior (Ack, manual, downtiming)\n\n\n\n\nVarious combination of flavour and usecases\n\n\n\n\n\n\nLimitation\n\n\n\n\nMySQL Master/slave only (not galera)\n\n\nService discovery Hook support\n\n\nMulti-source replication (Not yet)\n\n\n\n\nNotes\n\n\n\n\nOrchestrator is HA\n\n\nHTTP Proxy + services / leader\n\n\n\n\nMySAL Proxy + MySAL backends\n\n\n\n\n\n\nVery accurate\n\n\n\n\nDetection \n 20s\n\n\nRecover before Nagios detection\n\n\n\n\nALOT (??)\n\n\n\n\n\n\nUsed in production\n\n\n\n\nOutbrains\n\n\nBooking\n\n\nGithub\n\n\nSqare\n\n\nBigDaddy\n\n\nbooks", 
            "title": "1 MySQL Orchestrator"
        }, 
        {
            "location": "/2016/fosdem/MySQL/1-MySQL_Orchestrator/#features", 
            "text": "Built from real-world use cases    Support multi-layer Master/Slave replication topology    Topology discovery from 1 server    Failure detection Euristic approch       Ask each member of the cluster to get a full pictures  Show slave status / show masterstatus        Allow to support network split issue \n  - If orchestrator can not connect to 1 server, it may be a network issue \n  =  Ask other servers of the cluster to make the descision    Recovery + Promotion  By talking to each server: Can know which is the less late to promote  DC aware  Admin weight", 
            "title": "Features"
        }, 
        {
            "location": "/2016/fosdem/MySQL/1-MySQL_Orchestrator/#summary", 
            "text": "Supported  Multi-DC  Various GTID flavour  Flapping behavior (Ack, manual, downtiming)   Various combination of flavour and usecases    Limitation   MySQL Master/slave only (not galera)  Service discovery Hook support  Multi-source replication (Not yet)", 
            "title": "Summary"
        }, 
        {
            "location": "/2016/fosdem/MySQL/1-MySQL_Orchestrator/#notes", 
            "text": "Orchestrator is HA  HTTP Proxy + services / leader   MySAL Proxy + MySAL backends    Very accurate   Detection   20s  Recover before Nagios detection   ALOT (??)    Used in production   Outbrains  Booking  Github  Sqare  BigDaddy  books", 
            "title": "Notes"
        }, 
        {
            "location": "/2016/fosdem/Virtualization/1-Containers_and_virtualization/", 
            "text": "1-Containers_and_virtualization\n\n\n\n\n\n\n\nIntro\n\n\nVirtu \n Atomic\n\n\nDemo\n\n\n\n\n\n\n\nhttps://fosdem.org/2016/schedule/event/virt_iaas_containers_and_virtualization/\n\n\nSpoiler: Containers and VMs can work together\n\n\n\n\nManage \nvirtualization / IaaS technologies\n \n\n\nWith containers to make them easier to deploy and manage. \n\n=\n Efforts to containerize things like oVirt and OpenStack, as well as the best way to run KVM virtual machines in privileged containers.\n\n\n\n\nIntro\n\n\n\n\nCloud / Virtualization was the new hotness\n\n\nThen orchestration of VMs\n\n\nNow Containers\n\n\nThen orchestrate Containers\n\n\n\n\nVirtu \n Atomic\n\n\n\n\nAtomic Project\n\n\nMinimal os\n\n\nEverything as a container\n\n\n\n\nNo packages\n\n  =\n CoreOS like\n\n  =\n From RedHat\n\n\n\n\n\n\nKoala Project\n\n\n\n\nOpenstack services as container\n\n\nDeployed with Ansible\n\n\n\n\nDemo\n\n\n\n\nWorkstation: Fedora23\n\n\nVagant \nVM\n Host: Atomic\n\n\nContainer\n: Fedora23 with Libvirt\n\n\nVagrant \nVM\n Host: Atomic", 
            "title": "1 Containers and virtualization"
        }, 
        {
            "location": "/2016/fosdem/Virtualization/1-Containers_and_virtualization/#intro", 
            "text": "Cloud / Virtualization was the new hotness  Then orchestration of VMs  Now Containers  Then orchestrate Containers", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/Virtualization/1-Containers_and_virtualization/#virtu-atomic", 
            "text": "Atomic Project  Minimal os  Everything as a container   No packages \n  =  CoreOS like \n  =  From RedHat    Koala Project   Openstack services as container  Deployed with Ansible", 
            "title": "Virtu &amp; Atomic"
        }, 
        {
            "location": "/2016/fosdem/Virtualization/1-Containers_and_virtualization/#demo", 
            "text": "Workstation: Fedora23  Vagant  VM  Host: Atomic  Container : Fedora23 with Libvirt  Vagrant  VM  Host: Atomic", 
            "title": "Demo"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/", 
            "text": "1-Beyond_cfg_mgnt\n\n\n\n\n\n\n\nHistory\n\n\n1\n\n\n2\n\n\n3\n\n\nNew era\n\n\n1\n\n\n2\n\n\n\n\n\n\n\nHistory\n\n\n1\n\n\n\n\nMain frames\n\n\nServers pizza box\n\n\nVMs\n\n\nContainers\n\n\n\n\nMooving through paradigm\n\n=\n To solve issues, we build tools\n\n\n2\n\n\n\n\nProvision the OS\n\n\nConfigure the software\n\n  1. SSH in a for loop\n\n  2. CFG mgnt (agent)\n\n\n\n\nTools list\n\n- cdist\n\n- isconf\n\n- lcfg\n\n- pikt\n\n\n3\n\n\n\n\nEverything new is supplementary\n\n=\n They DO NOT REPLACE previous tools\n\n=\n They are built \non\n them to solve NEW issues that have been raised and we did not even reach before\n\n\n\n\nNew era\n\n\n1\n\n\n\n\nPhase change of modern software\n\n\n\n\nWe add \nlayers\n of abstration during the phase of build / installation / management\n\n\n\n\n\n\nWhat does it mean ?\n\n\n\n\n\n\nModelling\n is the next step\n\n\n\n\n\n\nModelling is NOT:\n\n\n\n\nOrchestration, is \nencapsulation\n of business rules \n logic\n\n\n\n\n2\n\n\n\n\nModelling as a \nlangage\n\n\nMachine:      Constrained resource\n\n\nApplications: Definition of the lifecycle of a \nservice\n\n\nUnits:        An implementation of an application\n\n\nScale:        Number of units\n\n\nRelations:    How application communicate with each other\n\n\n\n\nThings that are still left over\n\n  - Resources\n\n  - storage\n\n  - Network\n\n\n\n\n\n\nModeling should be Agnostic\n\n=\n Modeling + configuration management\n\n\n\n\n\n\nModeling is description", 
            "title": "1 Beyond cfg mgnt"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#history", 
            "text": "", 
            "title": "History"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#1", 
            "text": "Main frames  Servers pizza box  VMs  Containers   Mooving through paradigm \n=  To solve issues, we build tools", 
            "title": "1"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#2", 
            "text": "Provision the OS  Configure the software \n  1. SSH in a for loop \n  2. CFG mgnt (agent)   Tools list \n- cdist \n- isconf \n- lcfg \n- pikt", 
            "title": "2"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#3", 
            "text": "Everything new is supplementary \n=  They DO NOT REPLACE previous tools \n=  They are built  on  them to solve NEW issues that have been raised and we did not even reach before", 
            "title": "3"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#new-era", 
            "text": "", 
            "title": "New era"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#1_1", 
            "text": "Phase change of modern software   We add  layers  of abstration during the phase of build / installation / management    What does it mean ?    Modelling  is the next step    Modelling is NOT:   Orchestration, is  encapsulation  of business rules   logic", 
            "title": "1"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/1-Beyond_cfg_mgnt/#2_1", 
            "text": "Modelling as a  langage  Machine:      Constrained resource  Applications: Definition of the lifecycle of a  service  Units:        An implementation of an application  Scale:        Number of units  Relations:    How application communicate with each other   Things that are still left over \n  - Resources \n  - storage \n  - Network    Modeling should be Agnostic \n=  Modeling + configuration management    Modeling is description", 
            "title": "2"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/", 
            "text": "2-Hardening_cfg_mgnt\n\n\n\n\n\n\n\nIntro\n\n\nWhere to start\n\n\n1 - Data laying around\n\n\n2 - Data encryption\n\n\n3 - Other stuff to keep in mind\n\n\n4 - Resources for Auditing\n\n\n5 - SSH\n\n\n\n\n\n\n\nIntro\n\n\n\n\nSecurity\n\n\nis hard\n\n\nputs constraints\n\n\n\n\nnobody cares because, functionaly, it works\n\n\n\n\n\n\nCfg mgnt is now the SPOF / Target (from security pov)\n\n\n\n\n\n\nWhere to start\n\n\n\n\nGoogle Search: \nHardening cfg mgnt\n\n\nOWASP\n\n\n1/2 Google groups asking question\n\n\n\n\n=\n NO real insight\n\n\n\n\nSecurity principles\n\n\n\n\n\n\n\n\n1 - Data laying around\n\n\nExample with Puppet\n\n  - Use Hiera\n\n  - Remove any credentials or data from Module\n\n  - Publish in 1 \ninit\n commit by removing the history\n\n\n2 - Data encryption\n\n\n\n\nExternal source\n\n\nHiera-eyaml\n\n\n\n\nChef-vault\n\n\n\n\n\n\nExternal secrets service\n\n\n\n\nCloudflares/Redoctober\n\n\n\n\nHashicorp/valult\n\n\n\n\n\n\nGit in repo content\n\n\n\n\ngit-crypt\n\n\n\n\n=\n Stay in control\n\n\n3 - Other stuff to keep in mind\n\n\n\n\nDo lint, reviews, continuously\n\n\nRbac\n\n\n\n\nchef-inspec\n\n\n\n\n\n\nAlso keep in mind the risk if people get access to you CFG mnt infra\n\n\n\n\nAgents reporting logs\n\n\nDisable diff of changes\n\n\n\n\n\n\n\n\n\n\nMonitor the activity of your apps\n\n\n\n\n40X / 50X\n\n\nLogs\n\n\n\n\nBehavior\n\n\n\n\n\n\nSecurity baseline\n\n\n\n\nBest practices\n\n\nDefined shared rules\n\n\n\n\n4 - Resources for Auditing\n\n\n\n\nhardening.io\n\n\nCIS (Central Internet Security)\n\n\nChef has integrated its own \nAudit\n mod\n\n\nSIMP Github organization\n\n\n\n\n5 - SSH\n\n\n\n\nEven if you use Agent based CFG mgnt\n\n\n\n\nSSH is / will still be there\n\n  =\n Check your config\n\n  =\n Rotate your keys\n\n\n\n\n\n\nHave you thought to:\n\n\n\n\nDisable SSH completly", 
            "title": "2 Hardening cfg mgnt"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#intro", 
            "text": "Security  is hard  puts constraints   nobody cares because, functionaly, it works    Cfg mgnt is now the SPOF / Target (from security pov)", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#where-to-start", 
            "text": "Google Search:  Hardening cfg mgnt  OWASP  1/2 Google groups asking question   =  NO real insight   Security principles", 
            "title": "Where to start"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#1-data-laying-around", 
            "text": "Example with Puppet \n  - Use Hiera \n  - Remove any credentials or data from Module \n  - Publish in 1  init  commit by removing the history", 
            "title": "1 - Data laying around"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#2-data-encryption", 
            "text": "External source  Hiera-eyaml   Chef-vault    External secrets service   Cloudflares/Redoctober   Hashicorp/valult    Git in repo content   git-crypt   =  Stay in control", 
            "title": "2 - Data encryption"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#3-other-stuff-to-keep-in-mind", 
            "text": "Do lint, reviews, continuously  Rbac   chef-inspec    Also keep in mind the risk if people get access to you CFG mnt infra   Agents reporting logs  Disable diff of changes      Monitor the activity of your apps   40X / 50X  Logs   Behavior    Security baseline   Best practices  Defined shared rules", 
            "title": "3 - Other stuff to keep in mind"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#4-resources-for-auditing", 
            "text": "hardening.io  CIS (Central Internet Security)  Chef has integrated its own  Audit  mod  SIMP Github organization", 
            "title": "4 - Resources for Auditing"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/2-Hardening_cfg_mgnt/#5-ssh", 
            "text": "Even if you use Agent based CFG mgnt   SSH is / will still be there \n  =  Check your config \n  =  Rotate your keys    Have you thought to:   Disable SSH completly", 
            "title": "5 - SSH"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/", 
            "text": "4-Complex_DNS_infrastructure\n\n\n\n\n\n\n\nIntro\n\n\nTalk insight\n\n\nTools we use\n\n\nOverview\n\n\nSparts\n\n\nThrift\n\n\nDNS\n\n\nTools\n\n\nVIP Advertisement\n\n\nDNS modifications\n\n\nContent management (Auto / Human)\n\n\nAuto generated\n\n\nConflicts\n\n\nExternal DNS\n\n\nCCL\n\n\nQuestion\n\n\n\n\n\n\n\nhttps://fosdem.org/2016/schedule/event/managing_complex_dns_environment/\n\n\nIntro\n\n\n\n\nSpeaker\n\n\nProduction Engineer, Cluster Infrastructure\n\n\nCluster life cycle / Provisioning (Add / remove capacity)\n\n\n\n\nManage\n\n\n\n\nDNS, DHCP, TFTP, OS images \n\n\n\n\n\n\n\n\nWhat is a cluster\n\n\n\n\n\n\nA group of machines that have the same \nrole\n and serve the same service\n\n\n\n\n\n\nDNS entries\n\n\n\n\nServers\n\n\n\n\nServices\n\n\n\n\n\n\nManaging DNS is NOT easy !\n\n\n\n\n\n\nTalk insight\n\n\n\n\nNot YAO cfgmngt talk\n\n=\n will talk about:\n\n\nMicroservices\n\n\nDistributed system config (zookeeper)\n\n\n\n\nPipelines\n\n\n\n\n\n\nBuilding blocks\n\n\n\n\nMostly FOS\n\n\nPython glue\n\n\n\n\nTools we use\n\n\nOverview\n\n\n\n\nBaremetal: chef\n\n\n\n\nContainer: LXC\n\n\n\n\n\n\nService: \nsparts\n\n\n\n\nDaemon\n\n\nGenerate Rules + execution\n\n\ninit or Runit for DNS daemons manager\n\n\n\n\nSparts\n\n\n\n\nBoilerplate\n\n\nCommon features\n\n\nTask, loggingm counters\n\n\nCLI fro free\n\n\nSubscribe on configuration updates\n\n=\n Facebook Github\n\n\n\n\nThrift\n\n\n\n\nAllow us to build API for services\n\n\nPart of Apache\n\n\n\n\n\n\nDNS\n\n\nTools\n\n\n\n\nStructure:\n\n\nTinyDNS:  Authority\n\n\n\n\nUnboud:   Recurser\n\n\n\n\n\n\nTinyDNS\n\n\n\n\nConfiguration easier\n\n\nReliable, secure, simple\n\n\n\n\nMAPs tweaked for our external LoadBalancer\n\n\n\n\n\n\nUnboud\n\n\n\n\nFast\n\n\nReliable,\n\n\nPerformance were good + Small memory foot print\n\n\nLocal cache\n\n\n\n\nVIP Advertisement\n\n\n\n\nBGP + ECMP\n\n\nExaBGP to annince VIP\n\n\nEasy Rollout :)\n\n\n\n\nDNS modifications\n\n\n\n\nMostly automated\n\n\n\n\nServers ask register / deregister\n\n\n\n\n\n\nConfig\n\n\n\n\nTinyDNS + Unbound\n\n\n2 config files\n\n\nEvery servers has ALL the config\n\n\n\n\n=\n Configurator\n\n  - Python DSL (Thrift interface for syntax, Python Config generator, Pyhton validator)\n\n  - Generate JSON\n\n  - Review process\n\n  - Zookeeper and proxies\n\n  - Local Proxy on server\n\n\n\n\nConfigurator\n\n\nWatch a configfile\n\n\nEnsure it is upToDate\n\n\n\n\nContent management (Auto / Human)\n\n\nAuto generated\n\n\n\n\nSparts + Rules\n\n\nRules = python classes\n\n\nGenerate DNS records\n\n\nStreamed to Zookeeper\n\n\n\n\n\n\nConfiguration is never Stale\n\n\n\n\nConflicts\n\n\n\n\nPresenter\n\n\nSparts + Rules\n\n\nRetrive config from Zookeeper and Human changes\n\n\n\n\nReconciliate\n\n\n\n\n\n\nDistribution\n\n\n\n\n\n\nBittorrent\n\n\n\n\n\n\nData Pipeline\n\n\n\n\nHuman / Robot\n\n\nConfigurator / (DNS mutator + zk nodes)\n\n\nPresenter\n\n\nPublisher\n\n  =\n Torrent file in ZK\n\n  =\n Seed torrent\n\n\n\n\n=\n Internal DNS\n\n\nExternal DNS\n\n\n\n\nBillion users ?\n\n\nLoad balance\n\n\nRecuce latency\n\n\n\n\n\n\n=\n \npoints of presence\n\n  - Closer to users\n\n\n\n\nHow redirect users ?\n\n\nMeasuring \nCloseness\n\n\nThanks to CDN and Uniq DNS name / users\n\n\n\n\n=\n Compute a MAP\n\n\nCCL\n\n\n\n\nSplit complex services in simple services that you combine\n\n\nNo state when possible\n\n\nReliability / Reproducibility\n\n=\n Becomes easier to manage\n\n\n\n\nQuestion\n\n\n\n\nConfig deployment ?\n\n\nThe torrent file is registred  into ZK\n\n\nThanks to the Local proxy we deploy the info\n\n\nBittorrent is the data transport mechanisme\n\n\n\n\nThirft is the interface to \n\n\n\n\n\n\nPipeline / Monitoring\n\n\n\n\n\n\nMonitor the TXT record timestamp\n\n\n\n\n\n\nConfig files\n\n\n\n\nAll records for external IPs\n\n\nAll records for Internal IPs\n\n\nAll records for servers\n\n\nEverything is synced accros the world", 
            "title": "4 Complex DNS infrastructure"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#intro", 
            "text": "Speaker  Production Engineer, Cluster Infrastructure  Cluster life cycle / Provisioning (Add / remove capacity)   Manage   DNS, DHCP, TFTP, OS images      What is a cluster    A group of machines that have the same  role  and serve the same service    DNS entries   Servers   Services    Managing DNS is NOT easy !", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#talk-insight", 
            "text": "Not YAO cfgmngt talk \n=  will talk about:  Microservices  Distributed system config (zookeeper)   Pipelines    Building blocks   Mostly FOS  Python glue", 
            "title": "Talk insight"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#tools-we-use", 
            "text": "", 
            "title": "Tools we use"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#overview", 
            "text": "Baremetal: chef   Container: LXC    Service:  sparts   Daemon  Generate Rules + execution  init or Runit for DNS daemons manager", 
            "title": "Overview"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#sparts", 
            "text": "Boilerplate  Common features  Task, loggingm counters  CLI fro free  Subscribe on configuration updates \n=  Facebook Github", 
            "title": "Sparts"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#thrift", 
            "text": "Allow us to build API for services  Part of Apache", 
            "title": "Thrift"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#dns", 
            "text": "", 
            "title": "DNS"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#tools", 
            "text": "Structure:  TinyDNS:  Authority   Unboud:   Recurser    TinyDNS   Configuration easier  Reliable, secure, simple   MAPs tweaked for our external LoadBalancer    Unboud   Fast  Reliable,  Performance were good + Small memory foot print  Local cache", 
            "title": "Tools"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#vip-advertisement", 
            "text": "BGP + ECMP  ExaBGP to annince VIP  Easy Rollout :)", 
            "title": "VIP Advertisement"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#dns-modifications", 
            "text": "Mostly automated   Servers ask register / deregister    Config   TinyDNS + Unbound  2 config files  Every servers has ALL the config   =  Configurator \n  - Python DSL (Thrift interface for syntax, Python Config generator, Pyhton validator) \n  - Generate JSON \n  - Review process \n  - Zookeeper and proxies \n  - Local Proxy on server   Configurator  Watch a configfile  Ensure it is upToDate", 
            "title": "DNS modifications"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#content-management-auto-human", 
            "text": "", 
            "title": "Content management (Auto / Human)"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#auto-generated", 
            "text": "Sparts + Rules  Rules = python classes  Generate DNS records  Streamed to Zookeeper    Configuration is never Stale", 
            "title": "Auto generated"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#conflicts", 
            "text": "Presenter  Sparts + Rules  Retrive config from Zookeeper and Human changes   Reconciliate    Distribution    Bittorrent    Data Pipeline   Human / Robot  Configurator / (DNS mutator + zk nodes)  Presenter  Publisher \n  =  Torrent file in ZK \n  =  Seed torrent   =  Internal DNS", 
            "title": "Conflicts"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#external-dns", 
            "text": "Billion users ?  Load balance  Recuce latency    =   points of presence \n  - Closer to users   How redirect users ?  Measuring  Closeness  Thanks to CDN and Uniq DNS name / users   =  Compute a MAP", 
            "title": "External DNS"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#ccl", 
            "text": "Split complex services in simple services that you combine  No state when possible  Reliability / Reproducibility \n=  Becomes easier to manage", 
            "title": "CCL"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/4-Complex_DNS_infrastructure/#question", 
            "text": "Config deployment ?  The torrent file is registred  into ZK  Thanks to the Local proxy we deploy the info  Bittorrent is the data transport mechanisme   Thirft is the interface to     Pipeline / Monitoring    Monitor the TXT record timestamp    Config files   All records for external IPs  All records for Internal IPs  All records for servers  Everything is synced accros the world", 
            "title": "Question"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/", 
            "text": "5-CfgMgnt_and_containers\n\n\n\n\n\n\n\nCfg Mgnt\n\n\nsystem management patterns\n\n\nCfg mgnt solved problems\n\n\nEmergent issues\n\n\nContainers\n\n\nOverview\n\n\nBenefits\n\n\nCfg mgnt \n Containers\n\n\nDelivery patterns\n\n\nUncontained\n\n\nContained\n\n\n\n\n\n\n\nSpeaker\n\n  - Previously Chef\n\n  - Worked with\n\n    + Ansible\n\n    + LXC / Docker\n\n  - Now\n\n    + Juju @canonical\n\n\nCfg Mgnt\n\n\nsystem management patterns\n\n\n\n\nDivergence\n\n\n\n\nSSH + Manual actions\n\n\n\n\n\n\nConvergence\n\n\n\n\nPuppet / Chef\n\n\n\n\nContinuously diverge and converge\n\n\n\n\n\n\nCongruence\n\n\n\n\nState \ndoes not change\n\n\n\n\nCfg mgnt solved problems\n\n\n\n\nStopped divergent delivery patterns\n\n\nEliminate snowflakes\n\n\nFramework to describe a system state\n\n\nSupport existing ecosystem (Distro, pkgs)\n\n\nAbstraction\n\n\n\n\nEmergent issues\n\n\n\n\nDomain specific needs\n\n\nContext sensitve knowledge\n\n\nDoes not solve people and organization issues\n\n\n\n\nContainers\n\n\nOverview\n\n\n\n\n\n\nNew parradigm that are \nconfused\n people\n\n\n\n\n\n\n2 Flavors\n\n\n\n\nApplication containers\n\n\nSystem containers\n\n\n\n\nBenefits\n\n\n\n\nEasy resources constraints\n\n\nDensity\n\n\nFast\n\n\nNo overhead\n\n\n\n\nCfg mgnt \n Containers\n\n\nDelivery patterns\n\n\nCase of study: Kubernetes deployment\n\n\nUncontained\n\n\n\n\nLots of LOC\n\n\nRequired Build Env\n\n\nDifferents model possible\n\n\nDifficult management of artifacts\n\n\n\n\nContained\n\n\n\n\nNo dedicated build env\n\n\nDelivery time reduced\n\n\nSame model than Google suggestion (Doc)", 
            "title": "5 CfgMgnt and containers"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#cfg-mgnt", 
            "text": "", 
            "title": "Cfg Mgnt"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#system-management-patterns", 
            "text": "Divergence   SSH + Manual actions    Convergence   Puppet / Chef   Continuously diverge and converge    Congruence   State  does not change", 
            "title": "system management patterns"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#cfg-mgnt-solved-problems", 
            "text": "Stopped divergent delivery patterns  Eliminate snowflakes  Framework to describe a system state  Support existing ecosystem (Distro, pkgs)  Abstraction", 
            "title": "Cfg mgnt solved problems"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#emergent-issues", 
            "text": "Domain specific needs  Context sensitve knowledge  Does not solve people and organization issues", 
            "title": "Emergent issues"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#containers", 
            "text": "", 
            "title": "Containers"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#overview", 
            "text": "New parradigm that are  confused  people    2 Flavors   Application containers  System containers", 
            "title": "Overview"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#benefits", 
            "text": "Easy resources constraints  Density  Fast  No overhead", 
            "title": "Benefits"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#cfg-mgnt-containers", 
            "text": "", 
            "title": "Cfg mgnt &amp; Containers"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#delivery-patterns", 
            "text": "Case of study: Kubernetes deployment", 
            "title": "Delivery patterns"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#uncontained", 
            "text": "Lots of LOC  Required Build Env  Differents model possible  Difficult management of artifacts", 
            "title": "Uncontained"
        }, 
        {
            "location": "/2016/fosdem/cfg-mgnt/5-CfgMgnt_and_containers/#contained", 
            "text": "No dedicated build env  Delivery time reduced  Same model than Google suggestion (Doc)", 
            "title": "Contained"
        }, 
        {
            "location": "/2016/fosdem/containers/0-k8s_over_swarm/", 
            "text": "k8s_over_swarm\n\n\n\n\nhttps://github.com/docker/swarm-frontends/\n\n\nk8s cluster\n\n\nMesos-marathon cluster\n\n\n\n\n\n\n\n\n\n\nDeploy + Administrate a Kubernetes / Mesos cluster over Swarm\n\n  - Thanks to \ndocker-compose\n file", 
            "title": "0 k8s over swarm"
        }, 
        {
            "location": "/2016/fosdem/containers/1-Lxd_introduction/", 
            "text": "1-Lxd_introduction\n\n\n\n\n\n\n\n1 - LXD\n\n\n\n\n\n\n\n1 - LXD\n\n\n\n\nWhat\ns LXD\n\n\nsimple CLI\n\n\nFast: LXC containers\n\n\nSecure: Use Kernel security feature and namespaces (next: capabilities support)\n\n\n\n\nScalable: Daemon + REST API running of multiple Hosts\n\n\n\n\n\n\nWhat LXD is NOT\n\n\n\n\nNOT virtualization (Even if \nfull\n OS running thanks to LXC)\n\n\nNOT a fork of LXC (Part of the team of LXC)\n\n\nNOT an application manager (not like K8s / Mesos marathon, \n)\n\n\n\n\n\n\nDemo\n\n\nhttps://linuxcontainers.org/lxd/try-it/\n\n\n\n\nAdmin:\n\n\nCan do HOT resources properties changes\n\n\n\n\nIntergation with Host FS for snapshoting (ZFS, BTRFS, soon LVM2)\n\n\n\n\n\n\nInteraction:\n\n\n\n\nNice CLI to interract with a running container\n\n\nlxc push file \ncontainer\n\n\nlxc pull \ncontainer\n:file \nhost\n:\nPATH\n\n\n\n\nlxc edit\n\n\n\n\n\n\nImages:\n\n\n\n\nBased on Git workflow with REPOS\n\n\nlxc remote\n\n\nlxc push\n\n\nlxc pull\n\n\n\n\n\n\n\n\n\n\nNext\n\n\nSupport for Docker container format\n\n\nImprove migration (running state)\n\n\nIntegration into openstack\n\n\n\n\nIntegration into Juju\n\n\n\n\n\n\nContribute\n\n\n\n\nGo\n\n\nAPI clients in Go and Python (Currently)\n\n\nApache2 Licence", 
            "title": "1 Lxd introduction"
        }, 
        {
            "location": "/2016/fosdem/containers/1-Lxd_introduction/#1-lxd", 
            "text": "What s LXD  simple CLI  Fast: LXC containers  Secure: Use Kernel security feature and namespaces (next: capabilities support)   Scalable: Daemon + REST API running of multiple Hosts    What LXD is NOT   NOT virtualization (Even if  full  OS running thanks to LXC)  NOT a fork of LXC (Part of the team of LXC)  NOT an application manager (not like K8s / Mesos marathon,  )", 
            "title": "1 - LXD"
        }, 
        {
            "location": "/2016/fosdem/containers/1-Lxd_introduction/#demo", 
            "text": "https://linuxcontainers.org/lxd/try-it/   Admin:  Can do HOT resources properties changes   Intergation with Host FS for snapshoting (ZFS, BTRFS, soon LVM2)    Interaction:   Nice CLI to interract with a running container  lxc push file  container  lxc pull  container :file  host : PATH   lxc edit    Images:   Based on Git workflow with REPOS  lxc remote  lxc push  lxc pull      Next  Support for Docker container format  Improve migration (running state)  Integration into openstack   Integration into Juju    Contribute   Go  API clients in Go and Python (Currently)  Apache2 Licence", 
            "title": "Demo"
        }, 
        {
            "location": "/2016/fosdem/containers/2-Pets_cattles_birds/", 
            "text": "2-Pets_cattles_birds\n\n\n\n\nConcept\n\n\nA unit of compute: a function of code\n\n\n\n\n=\n Lightning talk from Mesosphere \n\n=\n Took AWS Lamda as example", 
            "title": "2 Pets cattles birds"
        }, 
        {
            "location": "/2016/fosdem/containers/3-Whats_comming_up_in_containers_world/", 
            "text": "3-Whats_comming_up_in_containers_world\n\n\n\n\n\n\n\n1- Currently working on\n\n\n2 - Mount from User Namespaces\n\n\n\n\n\n\n\nIntegrating new feature in the Linux Kernel\n\n\n1- Currently working on\n\n\n\n\ncgroup namespaces: Target Kernel4.4\n\n\nEspecially needed to run Docker NESTED into LXC by example\n\n\nVirtualize the view of Cgroups from a running container\n\n\n\n\nMount cgroupfs\n\n\n\n\n\n\nNamespaced files capabilities\n\n\n\n\nAllow containers to lay down file caps\n\n\n\n\nWithout risking host being tricked\n\n\n\n\n\n\nTag \nsecurity.nscapability\n with a RootID\n\n\n\n\nKernerUID / RootID\n\n\nAllow more than 1 entries\n\n\n\n\n2 - Mount from User Namespaces\n\n\n\n\nVFS changes\n\n\nSuperblock user namespace: \ns_user_ns\n\n\nSUID translation support\n\n\n\n\nFile capabilities\n\n\n\n\n\n\nFS targeted: FUSE\n\n\n\n\nAlready support unpriviliged mounts\n\n\nU/G id translation\n\n\nPID translation\n\n\n\n\n\n\nFUSE Security concern\n\n\n\n\nFor Security reason: Unprivilieged mount only accessible to the unprivilged user who mounted the FS\n\n\n\n\n\n\n\n\nFS targeted: Ext4\n\n\nU/G id translation\n\n\nLimit privileged mount options\n\n\nJournal device access\n\n\n\n\nBacking store attacks\n\n\n\n\n\n\nFS targeted: LoopFS\n\n\n\n\nAllow user namespaces to allocate loop devices", 
            "title": "3 Whats comming up in containers world"
        }, 
        {
            "location": "/2016/fosdem/containers/3-Whats_comming_up_in_containers_world/#1-currently-working-on", 
            "text": "cgroup namespaces: Target Kernel4.4  Especially needed to run Docker NESTED into LXC by example  Virtualize the view of Cgroups from a running container   Mount cgroupfs    Namespaced files capabilities   Allow containers to lay down file caps   Without risking host being tricked    Tag  security.nscapability  with a RootID   KernerUID / RootID  Allow more than 1 entries", 
            "title": "1- Currently working on"
        }, 
        {
            "location": "/2016/fosdem/containers/3-Whats_comming_up_in_containers_world/#2-mount-from-user-namespaces", 
            "text": "VFS changes  Superblock user namespace:  s_user_ns  SUID translation support   File capabilities    FS targeted: FUSE   Already support unpriviliged mounts  U/G id translation  PID translation", 
            "title": "2 - Mount from User Namespaces"
        }, 
        {
            "location": "/2016/fosdem/containers/3-Whats_comming_up_in_containers_world/#fuse-security-concern", 
            "text": "For Security reason: Unprivilieged mount only accessible to the unprivilged user who mounted the FS     FS targeted: Ext4  U/G id translation  Limit privileged mount options  Journal device access   Backing store attacks    FS targeted: LoopFS   Allow user namespaces to allocate loop devices", 
            "title": "FUSE Security concern"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/", 
            "text": "4-Docker_for_developer\n\n\n\n\n\n\n\nIntro\n\n\nBasic usages\n\n\nUnit Tests\n\n\nOther use cases\n\n\nChallenges\n\n\n\n\n\n\n\nIntro\n\n\n\n\nDocker is popular\n\n\n\n\nFocus on deployment (with all k8s, swarm, ecs and stuff)\n\n\n\n\n\n\nYou can also use docker on a daily bases \n\n\n\n\n1 developer\n\n\n1 laptop\n\n\nWithout saying that you use Docker :)\n\n=\n Based on Jessie Frazel stuff\n\n\n\n\n\n\n\n\nBasic usages\n\n\n\n\n--rm\n\n\n--volume\n\n\n\n\ndocker run --rm \n\\\n\n                     --volume \n$WORKSPACE\n:/somewhere/in/the/container/according/to/the/software/best/practices\n                    maven:4.0.4\n                    package\n\n\n\n\n\n\nlinks\n\n\n-it\n\n\n\n\n=\n Ephemeral, reproducable, fast shell environment\n\n\nUnit Tests\n\n\nProble usecase\n\n\n\n\nTest Matrix\n\n\nMake a docker images for every system you need\n\n\nSame RUN command\n\n\nReproducable\n\n\n\n\nExplicit and versioned dependencies (when relying on system packages)\n\n\n\n\n\n\nComplex \nTopology\n testing (Infrastucture)\n\n\n\n\nRun the really important process component into a container\n\n\n\n\nUse compose + Links\n\n\n\n\n\n\nSimulating load\n\n\n\n\n\n\nEasy DB state reset\n\n\n\n\nNothing to \nreset\n\n\n\n\nOther use cases\n\n\n\n\nIntegration Tests\n\n\nMocking services  \n\n\nDemos\n\n\nData migration test\n\n\n\n\nData\n containers\n\n\n\n\n\n\nShared and reproducable environment in Team\n\n\n\n\nWith private registry\n\n\n\n\nChallenges\n\n\n\n\nShared stuff\n\n\nSecrets\n\n\nconfig\n\n\nEverything that is not \nRuntime\n related", 
            "title": "4 Docker for developers"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/#intro", 
            "text": "Docker is popular   Focus on deployment (with all k8s, swarm, ecs and stuff)    You can also use docker on a daily bases    1 developer  1 laptop  Without saying that you use Docker :) \n=  Based on Jessie Frazel stuff", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/#basic-usages", 
            "text": "--rm  --volume   docker run --rm  \\ \n                     --volume  $WORKSPACE :/somewhere/in/the/container/according/to/the/software/best/practices\n                    maven:4.0.4\n                    package   links  -it   =  Ephemeral, reproducable, fast shell environment", 
            "title": "Basic usages"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/#unit-tests", 
            "text": "Proble usecase   Test Matrix  Make a docker images for every system you need  Same RUN command  Reproducable   Explicit and versioned dependencies (when relying on system packages)    Complex  Topology  testing (Infrastucture)   Run the really important process component into a container   Use compose + Links    Simulating load    Easy DB state reset   Nothing to  reset", 
            "title": "Unit Tests"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/#other-use-cases", 
            "text": "Integration Tests  Mocking services    Demos  Data migration test   Data  containers    Shared and reproducable environment in Team   With private registry", 
            "title": "Other use cases"
        }, 
        {
            "location": "/2016/fosdem/containers/4-Docker_for_developers/#challenges", 
            "text": "Shared stuff  Secrets  config  Everything that is not  Runtime  related", 
            "title": "Challenges"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/", 
            "text": "5-rkt_container_mechanics\n\n\n\n\n\n\n\nIntro\n\n\nCLI tool\n\n\nStages\n\n\nContainers on Linux\n\n\nNamespaces\n\n\nCgroups\n\n\nQuestions\n\n\n\n\n\n\n\nSpeaker: Alban Crequy kinvolk.io\n\n\nIntro\n\n\nWhat is \nrkt\n ?\n\n* CLI tool\n\n* Go\n\n* Self contained\n\n\nCLI tool\n\n\n\n\nNo daemon\n\n\nFetch Images\n\n\nStart container\n\n\nGive the hand to the OS system process manager (SystemD on CoreOS)\n\n\n\n\nStages\n\n\n\n\n0\n\n\n\n\nDiscover/manage images\n\n\n\n\n\n\n1\n\n\n\n\nManage POD, THE \ncontainer\n\n\nHandle multiple Process / App that have to run together as a \nunit\n\n\nGet the dependencies / resources allocation requirements and constraints\n\n\n\n\n\n\nNotes\n\n \n\n\n2 implementations of stage 1 nowadays\n\n * systemd-nspaawn\n\n * KVM\n\n\n\n\n\n\n\n\n2\n\n\nThe process that compose the container\n\n\n\n\nContainers on Linux\n\n\nNamespaces\n\n\n\n\nViews of a system\n\n\nMount\n\n\nHostname\n\n\n\n\n\n\n\n\n\n\nNetwork support for \nrkt\n\n\n\n\nPlugin\n based on CNI: Container Networking Interface (From the Appc project)\n\n\nSupport HOST only network\n\n\n\n\nSupported plan from Kubernetes, Calico\n\n\n\n\n\n\nGoals\n\n\n\n\n\n\nIsolation\n\n\n\n\n\n\nNext\n\n\n\n\nUserID namespaces\n\n\nAllow Root that are not root :)\n\n\n\n\nCgroups\n\n\nQuestions", 
            "title": "5 Rkt container mechanics"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#intro", 
            "text": "What is  rkt  ? \n* CLI tool \n* Go \n* Self contained", 
            "title": "Intro"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#cli-tool", 
            "text": "No daemon  Fetch Images  Start container  Give the hand to the OS system process manager (SystemD on CoreOS)", 
            "title": "CLI tool"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#stages", 
            "text": "0   Discover/manage images    1   Manage POD, THE  container  Handle multiple Process / App that have to run together as a  unit  Get the dependencies / resources allocation requirements and constraints    Notes \n   2 implementations of stage 1 nowadays \n * systemd-nspaawn \n * KVM     2  The process that compose the container", 
            "title": "Stages"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#containers-on-linux", 
            "text": "", 
            "title": "Containers on Linux"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#namespaces", 
            "text": "Views of a system  Mount  Hostname      Network support for  rkt   Plugin  based on CNI: Container Networking Interface (From the Appc project)  Support HOST only network   Supported plan from Kubernetes, Calico    Goals    Isolation    Next   UserID namespaces  Allow Root that are not root :)", 
            "title": "Namespaces"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#cgroups", 
            "text": "", 
            "title": "Cgroups"
        }, 
        {
            "location": "/2016/fosdem/containers/5-Rkt_container_mechanics/#questions", 
            "text": "", 
            "title": "Questions"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/", 
            "text": "6-Twitter_infrastructure_with_containers\n\n\n\n\n\n\n\nTwitter Scale\n\n\nTHe problem\n\n\nTwitter Workflow\n\n\nScheduler\n\n\n\n\n\n\n\nTwitter Scale\n\n\n10^5 : Containers\n\n10^4 : Hosts\n\n10^3 : Jobs / servcices\n\n10^2 : Teams\n\n10^1 : SREs/Team\n\n\nTHe problem\n\n\nTwitter focus;\n\n* Scale -\n efficiency\n\n\n\n\nTips:\n\n\n\n\nDon\nt care of efficency if you are not at that scale\n\n\nCare about easiness and usage\n\n\n\n\n\n\n\n\nTwitter Workflow\n\n\n\n\nMesos\n\n\nAurora (service / jobs)\n\n\n\n\n\n\nUser define a need in Json\n\n\nSubmit to the scheduler\n\n\nEnjoy\n\n\n\n\nScheduler\n\n\n\n\nLinux default: CFS", 
            "title": "6 Twitter infrastructure with containers"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/#twitter-scale", 
            "text": "10^5 : Containers \n10^4 : Hosts \n10^3 : Jobs / servcices \n10^2 : Teams \n10^1 : SREs/Team", 
            "title": "Twitter Scale"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/#the-problem", 
            "text": "Twitter focus; \n* Scale -  efficiency", 
            "title": "THe problem"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/#tips", 
            "text": "Don t care of efficency if you are not at that scale  Care about easiness and usage", 
            "title": "Tips:"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/#twitter-workflow", 
            "text": "Mesos  Aurora (service / jobs)    User define a need in Json  Submit to the scheduler  Enjoy", 
            "title": "Twitter Workflow"
        }, 
        {
            "location": "/2016/fosdem/containers/6-Twitter_infrastructure_with_containers/#scheduler", 
            "text": "Linux default: CFS", 
            "title": "Scheduler"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/", 
            "text": "20160614_SecureCloud_Hashicorp-tools\n\n\n\n\n\n\n\n1- Intro\n\n\n1.1 - Needs\n\n\n1.2 - Tools\n\n\n2 - Challenges\n\n\nChallenge 1 - Images\n\n\nChallenge 2 - Infra provisioning\n\n\nChallenge 3 - Bootstraping + Configuration of instances\n\n\nChallenge 4 - Secrets management\n\n\n3 - Ccl\n\n\n\n\n\n\n\n1- Intro\n\n\n1.1 - Needs\n\n\nExample:\n\n* Team 1 needs a \nk8s\n based development environment\n\n\n=\n \nEnvironment contract\n\n\nASAP\n\n- Automated\n\n- Separate config / code\n\n- API driven (Cloud and tools)\n\n- Prefer modular, OpenSource tooling\n\n\n2 Main key features\n\n- Self service functionality\n\n- Automated environment generation\n\n\n1.2 - Tools\n\n\nPacker\n\n- Images / Build\n\n\nTerraform\n\n- Modeling infrastructure BLOCKS\n\n\nCFGMGNT\n\n- Ansible, Chef, Puppet, \n\n\nVault\n\n- Secrets\n\n\n2 - Challenges\n\n\nChallenge 1 - Images\n\n\nChallenge 2 - Infra provisioning\n\n\nDemo / Example: Terraform\n\n\n\n\nManifests \nterraform.tf\n\n\nOpenVPN Compute instance\n\n\nVariables extracted iin \ninputs.tf\n file\n\n\nModules structure\n\n\n\n\n+ aws/\n  + simpled-dns/\n    + core.tf\n    + inputs.tf\n    + outputs.tf\n\n\n\n\nKey points\n\n- Multi-cloud support\n\n-\n Can go from 1 provider to an other\n\n-\n Can conbine x providers\n\n\nChallenge 3 - Bootstraping + Configuration of instances\n\n\nBootstraping with \nCloud-init\n\n- Hooks into cloud providers Metadata service\n\n- Allow User-data\n\n\n=\n Terraform can publish User-data into Cloud provider\n\n\n\n\nCommants\n\n\nGoal was to bootstrap the \ncfg mgnt\n tool ASAP on raw image\n\n\n\n\n\n\nChallenge 4 - Secrets management\n\n\nLesson learn: DONT run your own !\n\n\nVault\n\n- Unified API for multiple backends\n\n- Policies (permissions)\n\n- Audits\n\n\n=\n The unified and centralized place to manage secrets\n\n\nIntegration effort\n\n- Vary according tools\n\n- BUT since API, at some point we can do / workaround the lack of existing tool\n\n\nProcess:\n\n* Admins can unseal / create mount policyes\n\n* Other users can Changes the values\n\n* User\n\n  1. User submit \nCreate environment\n request\n\n  2. Spin-up environment\n\n    + GET IAAS Creds \n\n    + (No existing tool, we write some stuff : \nhttps://gitjub.com/opencredo/terrahelp\n)\n\n\nBenefits:\n\n- Centralised\n\n- Flexible Backends\n\n- API\n\n\nFully Automated ? The bootstraping problem:\n\n* Vault custom policy\n\n* IAAS : Cloud-init\n\n  - Not secure (Plain text file on instance FS)\n\n  -\n \ncubbyhole\n temp token for bootstraping\n\n\n3 - Ccl\n\n\n\n\nDev can create env in minutes\n\n\nAddress concerns about moving / locking provider\n\n\nStart leverage promise of cloud\n\n\nLeson learn for chosing the RIGHT cloud for the Job (Rather than 1 size fits all)\n\n\n\n\nThe only constant in life, is change", 
            "title": "20160614 1 SecureCloud Hashicorp tools"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#1-intro", 
            "text": "", 
            "title": "1- Intro"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#11-needs", 
            "text": "Example: \n* Team 1 needs a  k8s  based development environment  =   Environment contract  ASAP \n- Automated \n- Separate config / code \n- API driven (Cloud and tools) \n- Prefer modular, OpenSource tooling  2 Main key features \n- Self service functionality \n- Automated environment generation", 
            "title": "1.1 - Needs"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#12-tools", 
            "text": "Packer \n- Images / Build  Terraform \n- Modeling infrastructure BLOCKS  CFGMGNT \n- Ansible, Chef, Puppet,   Vault \n- Secrets", 
            "title": "1.2 - Tools"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#2-challenges", 
            "text": "", 
            "title": "2 - Challenges"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#challenge-1-images", 
            "text": "", 
            "title": "Challenge 1 - Images"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#challenge-2-infra-provisioning", 
            "text": "Demo / Example: Terraform   Manifests  terraform.tf  OpenVPN Compute instance  Variables extracted iin  inputs.tf  file  Modules structure   + aws/\n  + simpled-dns/\n    + core.tf\n    + inputs.tf\n    + outputs.tf  Key points \n- Multi-cloud support \n-  Can go from 1 provider to an other \n-  Can conbine x providers", 
            "title": "Challenge 2 - Infra provisioning"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#challenge-3-bootstraping-configuration-of-instances", 
            "text": "Bootstraping with  Cloud-init \n- Hooks into cloud providers Metadata service \n- Allow User-data  =  Terraform can publish User-data into Cloud provider", 
            "title": "Challenge 3 - Bootstraping + Configuration of instances"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#commants", 
            "text": "Goal was to bootstrap the  cfg mgnt  tool ASAP on raw image", 
            "title": "Commants"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#challenge-4-secrets-management", 
            "text": "Lesson learn: DONT run your own !  Vault \n- Unified API for multiple backends \n- Policies (permissions) \n- Audits  =  The unified and centralized place to manage secrets  Integration effort \n- Vary according tools \n- BUT since API, at some point we can do / workaround the lack of existing tool  Process: \n* Admins can unseal / create mount policyes \n* Other users can Changes the values \n* User \n  1. User submit  Create environment  request \n  2. Spin-up environment \n    + GET IAAS Creds  \n    + (No existing tool, we write some stuff :  https://gitjub.com/opencredo/terrahelp )  Benefits: \n- Centralised \n- Flexible Backends \n- API  Fully Automated ? The bootstraping problem: \n* Vault custom policy \n* IAAS : Cloud-init \n  - Not secure (Plain text file on instance FS) \n  -   cubbyhole  temp token for bootstraping", 
            "title": "Challenge 4 - Secrets management"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_1_SecureCloud_Hashicorp-tools/#3-ccl", 
            "text": "Dev can create env in minutes  Address concerns about moving / locking provider  Start leverage promise of cloud  Leson learn for chosing the RIGHT cloud for the Job (Rather than 1 size fits all)   The only constant in life, is change", 
            "title": "3 - Ccl"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/", 
            "text": "20160614_Nomad-ClusterScheduler\n\n\n\n\n\n\n\n0 - Intro\n\n\n0.1 - Overview\n\n\n0.2 - Role of Scheduler\n\n\n0.3 - Nomad\n\n\n0.4 - Nomad job\n\n\n0.5 - Data model\n\n\n0.5.1 - Evaluation\n\n\n0.5.2 - Allocation\n\n\n\n\n\n\n1 - Fault tolerence\n\n\n2 - Traffic shaping\n\n\n3 - Ops insights\n\n\n4 - Delivery + Scheduling\n\n\nScheduling (server side)\n\n\nClient side\n\n\n\n\n\n\n5 - Scale system administrations\n\n\n6 - Resource utilization\n\n\nCCL\n\n\n\n\n\n\n\n0 - Intro\n\n\n0.1 - Overview\n\n\nSpeaker\n\n- Several start-ups before\n\n- Comes from Netflix\n\n- Works on Nomad @Hashicorp\n\n  + Tested several stuff (Mesos on AWS)\n\n\nWebOps\n\n\nNetflix\n\n- Already a really good deployment tool\n\n- But still 15min to create a cluster\n\n\n0.2 - Role of Scheduler\n\n\n\n\nImprove Fault tolerence\n\n\nAid traffic shaping accross DC and geographies\n\n\nOperation insights\n\n\nUniform deployment\n\n\nScale system administration and operations\n\n\nIncrease resource utilization\n\n\n\n\n0.3 - Nomad\n\n\nDistributed cluster scheduler\n\n- Multi DC based\n\n- Multi region support\n\n- Flexible workload\n\n- Job priorities\n\n- Bin packing\n\n- Large scale\n\n- Ops simple\n\n\nOthers:\n\n- Inspired by Google Omega\n\n- Optimistic concurrency\n\n- State coordination built-in (no external dependencies like Zookeeper, Etcd, Consul)\n\n- Pluggable\n\n\nBuilt-on top of\n\n- Gossip (Serf)\n\n- Consensus (Raft from Consul)\n\n\n=\n Built on Experience\n\n\n0.4 - Nomad job\n\n\nHCL (Json like)\n\n\n0.5 - Data model\n\n\n\n\nEvaluation\n\n\nAllocation\n\n\nExecution\n\n\n\n\n0.5.1 - Evaluation\n\n\nCan produces a lot of allocation requests\n\n\nType of Jobs\n\n- Service\n\n- Batch\n\n- System\n\n\n0.5.2 - Allocation\n\n\nCurrent state of the system\n\n\n1 - Fault tolerence\n\n\nShould be the core goal of a scheduler\n\n\nTypes of failures:\n\n* Recoverable failures\n\n* Driver errors\n\n* Scheduler failures\n\n\n2 - Traffic shaping\n\n\nStates:\n\n* Multi-DC are the new normal\n\n* Active ~ Active accross regions needs for reliability\n\n* Cluster have to be resizable\n\n* Needs for 1 Control Pane for all\n\n\nWith Nomad\n\n* 1 same jobfile can target multiple DC (Awarness)\n\n* Unified interface for clouds\n\n* \n\n\n3 - Ops insights\n\n\nShould:\n\n\n\n\nHelp troubleshooting quickly and easily\n\n\nStatus\n\n\n\n\nEvents of a task\n\n\n\n\n\n\nLogs management\n\n\n\n\nRotation\n\n\nStreaming to cliens\n\n\n\n\nForward to remote sink\n\n\n\n\n\n\nFS inspections\n\n\n\n\n\n\nInspect FS of a task\n\n\n\n\n\n\nOps runtime metrics\n\n\n\n\nResources usage (Tasks, nodess, schedulers)\n\n\nForward to remote sink\n\n\n\n\n4 - Delivery + Scheduling\n\n\nUniform delivery semantic\n\n- Support for various DRIVERS\n\nSmart rolling updates\n\nJob priotiries\n\nConcurrent scheduling\n\n\nScheduling (server side)\n\n\nConcurrent + Optimistic\n\n- Evaluation -\n Broker -\n Scheduluer (PLAN)-\n aAllocation\n\n- PLAN\n\n\nClient side\n\n\nBroad OS support\n\nHost fingerprint\n\nPluggable drivers\n\n\n5 - Scale system administrations\n\n\nxxx\n\n- \n\n\nSystem jobs\n\n- Run on every nodes\n\n- Greate for INFRA / Monitoring / Logging auditing software\n\n\nMaintainance primitives\n\n- Drain allocations from 1 node to an other\n\n\nNodes / worker status\n\n\n6 - Resource utilization\n\n\nBin packing\n\nBlocked evaluation if no more resources\n\nAPI for cluster usage querying (COMMING)\n\n\nCCL\n\n\nBenchmark to PUSH nomad\n\n-\n 1.000.000 Containers\n\n-\n Scheduled, allocated, Started \n 300s\n\n\nComming soon\n\n* Features\n\n  - Persistent volumes\n\n  - Networking\n\n  - ACLs\n\n  - Premption\n\n* Integration\n\n  - Vault\n\n  - Auto-scaling\n\n  - Charge-back", 
            "title": "20160614 2 Nomad ClusterScheduler"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#0-intro", 
            "text": "", 
            "title": "0 - Intro"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#01-overview", 
            "text": "Speaker \n- Several start-ups before \n- Comes from Netflix \n- Works on Nomad @Hashicorp \n  + Tested several stuff (Mesos on AWS)  WebOps  Netflix \n- Already a really good deployment tool \n- But still 15min to create a cluster", 
            "title": "0.1 - Overview"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#02-role-of-scheduler", 
            "text": "Improve Fault tolerence  Aid traffic shaping accross DC and geographies  Operation insights  Uniform deployment  Scale system administration and operations  Increase resource utilization", 
            "title": "0.2 - Role of Scheduler"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#03-nomad", 
            "text": "Distributed cluster scheduler \n- Multi DC based \n- Multi region support \n- Flexible workload \n- Job priorities \n- Bin packing \n- Large scale \n- Ops simple  Others: \n- Inspired by Google Omega \n- Optimistic concurrency \n- State coordination built-in (no external dependencies like Zookeeper, Etcd, Consul) \n- Pluggable  Built-on top of \n- Gossip (Serf) \n- Consensus (Raft from Consul)  =  Built on Experience", 
            "title": "0.3 - Nomad"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#04-nomad-job", 
            "text": "HCL (Json like)", 
            "title": "0.4 - Nomad job"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#05-data-model", 
            "text": "Evaluation  Allocation  Execution", 
            "title": "0.5 - Data model"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#051-evaluation", 
            "text": "Can produces a lot of allocation requests  Type of Jobs \n- Service \n- Batch \n- System", 
            "title": "0.5.1 - Evaluation"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#052-allocation", 
            "text": "Current state of the system", 
            "title": "0.5.2 - Allocation"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#1-fault-tolerence", 
            "text": "Should be the core goal of a scheduler  Types of failures: \n* Recoverable failures \n* Driver errors \n* Scheduler failures", 
            "title": "1 - Fault tolerence"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#2-traffic-shaping", 
            "text": "States: \n* Multi-DC are the new normal \n* Active ~ Active accross regions needs for reliability \n* Cluster have to be resizable \n* Needs for 1 Control Pane for all  With Nomad \n* 1 same jobfile can target multiple DC (Awarness) \n* Unified interface for clouds \n*", 
            "title": "2 - Traffic shaping"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#3-ops-insights", 
            "text": "Should:   Help troubleshooting quickly and easily  Status   Events of a task    Logs management   Rotation  Streaming to cliens   Forward to remote sink    FS inspections    Inspect FS of a task    Ops runtime metrics   Resources usage (Tasks, nodess, schedulers)  Forward to remote sink", 
            "title": "3 - Ops insights"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#4-delivery-scheduling", 
            "text": "Uniform delivery semantic \n- Support for various DRIVERS \nSmart rolling updates \nJob priotiries \nConcurrent scheduling", 
            "title": "4 - Delivery + Scheduling"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#scheduling-server-side", 
            "text": "Concurrent + Optimistic \n- Evaluation -  Broker -  Scheduluer (PLAN)-  aAllocation \n- PLAN", 
            "title": "Scheduling (server side)"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#client-side", 
            "text": "Broad OS support \nHost fingerprint \nPluggable drivers", 
            "title": "Client side"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#5-scale-system-administrations", 
            "text": "xxx \n-   System jobs \n- Run on every nodes \n- Greate for INFRA / Monitoring / Logging auditing software  Maintainance primitives \n- Drain allocations from 1 node to an other  Nodes / worker status", 
            "title": "5 - Scale system administrations"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#6-resource-utilization", 
            "text": "Bin packing \nBlocked evaluation if no more resources \nAPI for cluster usage querying (COMMING)", 
            "title": "6 - Resource utilization"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_2_Nomad-ClusterScheduler/#ccl", 
            "text": "Benchmark to PUSH nomad \n-  1.000.000 Containers \n-  Scheduled, allocated, Started   300s  Comming soon \n* Features \n  - Persistent volumes \n  - Networking \n  - ACLs \n  - Premption \n* Integration \n  - Vault \n  - Auto-scaling \n  - Charge-back", 
            "title": "CCL"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_3_BringMamothToCI/", 
            "text": "20160614_3_BringMamothToCI\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - Service Discovery\n\n\n3 - Secrets management\n\n\n\n\n\n\n\n1 - Intro\n\n\n\n\n2 - Service Discovery\n\n\nConsul\n\n\nInfra:\n\n* Docker-swarm\n\n* Consul-registrator\n\n* Consul\n\n\n3 Environments:\n\n* 1 Docker-swarm cluster / env\n\n* Other \nConsul KV store\n for \nAutomation\n\n  - 1 cluster between each environment\n\n  - KV values are:\n\n     + CI variables: Version of containers\n\n     + Apps variables: Configuration\n\n\n=\n Dynamic\n\n\n3 - Secrets management\n\n\nVault\n\n\nMain use case: \n\n1. Dynamic / On demand credentials generation\n\n  =\n Each instance has dedicated creds\n\n  =\n Great Auditlog\n\n  =\n No hard-coded values\n\n\n\n\nSame for Users, during troubleshooting needs\n\n\nPKI Management", 
            "title": "20160614 3 BringMamothToCI"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_3_BringMamothToCI/#1-intro", 
            "text": "", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_3_BringMamothToCI/#2-service-discovery", 
            "text": "Consul  Infra: \n* Docker-swarm \n* Consul-registrator \n* Consul  3 Environments: \n* 1 Docker-swarm cluster / env \n* Other  Consul KV store  for  Automation \n  - 1 cluster between each environment \n  - KV values are: \n     + CI variables: Version of containers \n     + Apps variables: Configuration  =  Dynamic", 
            "title": "2 - Service Discovery"
        }, 
        {
            "location": "/2016/hashiconf/day1/20160614_3_BringMamothToCI/#3-secrets-management", 
            "text": "Vault  Main use case:  \n1. Dynamic / On demand credentials generation \n  =  Each instance has dedicated creds \n  =  Great Auditlog \n  =  No hard-coded values   Same for Users, during troubleshooting needs  PKI Management", 
            "title": "3 - Secrets management"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/", 
            "text": "20160615_1_Keynotes\n\n\n\n\n\n\n\n1 - Resources efficiency - A comeback story\n\n\n2 - OS Hardening - With Packer\n\n\nCommon OS defaults\n\n\nBest practices\n\n\nAudit ?\n\n\nConclusion\n\n\n\n\n\n\n\n1 - Resources efficiency - A comeback story\n\n\nFirst\n\n* IoT + Mobiles are rising\n\n* DC needs will x4\n\n\nIndustry preoccupation ?\n\n* Money: No\n\n* Time: Yes\n\n  -\n Be Fast\n\n  -\n Be Aglite\n\n\nEvolution\n\n* VM\n\n* Cloud\n\n  - Elastic Infra\n\n  -\n Need more\n\n  -\n By more\n\n  -\n Instant, on demand\n\n\nBUT\n\n* VM still slow\n\n* Try to be more efficient\n\n  -\n Containers !\n\n\nNext ?\n\n* Ochestrators\n\n* Schedulers\n\n* Intelligent infra requires MetaData\n\n  - Tags your images, containers, everythong !\n\n  - \nhttp://Metabadger.com\n\n  - \nhttp://blog.microscaling.com/2016/06/the-joy-of-organising-container-image_6.html\n\n\n2 - OS Hardening - With Packer\n\n\nCommon OS defaults\n\n\n\n\nPermission issues\n\n\nDefault \numask\n and \n/tmp\n with no \nnoexec\n, \nnosuid\n\n\n\n\nBut where are best practices ?\n\n\nBest practices\n\n\nCIS / STIG organizations\n\n- \nhttps://github.com/docker/docker-bench-security\n\n\nExample:\n\n* GPG checks for remote content\n\n* Remove every remote access softwares if you do not need them\n\n* Kernel parameters\n\n\nBUT do NOT implement everything without understanding what you are doing !\n\n\nUse AUTOMATED system\n\n- Puppet, Ansible, Chef, \n\n- Packer\n\n- Tests\n\n\nFS isolation\n\n- CIS recommands different mount points for:\n\n  + \n/var\n\n  + \n/var/log\n\n  + \n/var/log/audit\n\n  + \n/tmp\n\n  =\n Prevent resource exhaustion in case of compromission\n\n\nAudit ?\n\n\nCIS / STIG profiles\n\n- \nopenscap\n / \noscap\n tools\n\n-\n Produce XML / HTML reports\n\n\nConclusion\n\n\n\n\nLeverage Images heritages\n\n\nCheck your Trust\n\n\nAutomate provisionning\n\n\nAudit (Different process that implementation)", 
            "title": "20160615 1 Keynotes"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#1-resources-efficiency-a-comeback-story", 
            "text": "First \n* IoT + Mobiles are rising \n* DC needs will x4  Industry preoccupation ? \n* Money: No \n* Time: Yes \n  -  Be Fast \n  -  Be Aglite  Evolution \n* VM \n* Cloud \n  - Elastic Infra \n  -  Need more \n  -  By more \n  -  Instant, on demand  BUT \n* VM still slow \n* Try to be more efficient \n  -  Containers !  Next ? \n* Ochestrators \n* Schedulers \n* Intelligent infra requires MetaData \n  - Tags your images, containers, everythong ! \n  -  http://Metabadger.com \n  -  http://blog.microscaling.com/2016/06/the-joy-of-organising-container-image_6.html", 
            "title": "1 - Resources efficiency - A comeback story"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#2-os-hardening-with-packer", 
            "text": "", 
            "title": "2 - OS Hardening - With Packer"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#common-os-defaults", 
            "text": "Permission issues  Default  umask  and  /tmp  with no  noexec ,  nosuid   But where are best practices ?", 
            "title": "Common OS defaults"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#best-practices", 
            "text": "CIS / STIG organizations \n-  https://github.com/docker/docker-bench-security  Example: \n* GPG checks for remote content \n* Remove every remote access softwares if you do not need them \n* Kernel parameters  BUT do NOT implement everything without understanding what you are doing !  Use AUTOMATED system \n- Puppet, Ansible, Chef,  \n- Packer \n- Tests  FS isolation \n- CIS recommands different mount points for: \n  +  /var \n  +  /var/log \n  +  /var/log/audit \n  +  /tmp \n  =  Prevent resource exhaustion in case of compromission", 
            "title": "Best practices"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#audit", 
            "text": "CIS / STIG profiles \n-  openscap  /  oscap  tools \n-  Produce XML / HTML reports", 
            "title": "Audit ?"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_1_Keynotes/#conclusion", 
            "text": "Leverage Images heritages  Check your Trust  Automate provisionning  Audit (Different process that implementation)", 
            "title": "Conclusion"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_2_Containers-swap/", 
            "text": "20160615_2_Containers-swap\n\n\n\n\n\n\n\nPrinciples\n\n\nBootstapping etcd\n\n\n\n\n\n\n\nPrinciples\n\n\nBuild OS images\n\n- Testable\n\n- Repeatable\n\n- Faster to instantiate\n\n- More reliable (Time to install packages, remote dependencies \n)\n\n\nTips:\n* Use a \nminimale\n OS image\n\n\n\n\nCluster all the things\n\n- No single point of failure\n\n\nBootstapping Etcd\n\n- Discovery service ?\n\n  + DONT depend on the Public service\n\n  + Host your own private one\n\n  =\n BUT still something to maintain \n while bootstrapping ?\n\n- DNS\n\n  + Too statics\n\n\n=\n Etcd bootstraping service:\n\n\nBootstapping etcd\n\n\nShell script running on each CoreOS instance\n\n- Relying on AWS Auto-scaling feature\n\n\n* Determin if a cluster exists\n  - query local ETCD running\n* If not, start one\n  - query AWS autoscaling group\n  - get list of existings nodes\n  - Wait till there are enough members to create a cluster\n* Otherwhise, join\n  - query AWS autoscaling group\n  - get list of existings nodes\n\n\n\n\n=\n \nExternal\n Metadata store is the Key !!!", 
            "title": "20160615 2 Containers swap"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_2_Containers-swap/#principles", 
            "text": "Build OS images \n- Testable \n- Repeatable \n- Faster to instantiate \n- More reliable (Time to install packages, remote dependencies  )  Tips:\n* Use a  minimale  OS image  Cluster all the things \n- No single point of failure  Bootstapping Etcd \n- Discovery service ? \n  + DONT depend on the Public service \n  + Host your own private one \n  =  BUT still something to maintain   while bootstrapping ? \n- DNS \n  + Too statics  =  Etcd bootstraping service:", 
            "title": "Principles"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_2_Containers-swap/#bootstapping-etcd", 
            "text": "Shell script running on each CoreOS instance \n- Relying on AWS Auto-scaling feature  * Determin if a cluster exists\n  - query local ETCD running\n* If not, start one\n  - query AWS autoscaling group\n  - get list of existings nodes\n  - Wait till there are enough members to create a cluster\n* Otherwhise, join\n  - query AWS autoscaling group\n  - get list of existings nodes  =   External  Metadata store is the Key !!!", 
            "title": "Bootstapping etcd"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/", 
            "text": "20160615_3_Secrets-WithContainers\n\n\n\n\n\n\n\nAgenda\n\n\nTerms\n\n\nContainers world\n\n\nSecrets management tools\n\n\nSI for containers\n\n\n\n\n\n\n\nAgenda\n\n\nWhat it is about:\n\n* Paradigm + Pattern\n\n* Workflow\n\n\nWhat it is not:\n\n* A TRUTH guide\n\n* A deep dive\n\n\nTerms\n\n\nSecurity:\n\n- Risk management\n\n- Accepting some\n\n- Guarding\n\n\nRisk:\n\n- Increase with system complexity\n\n\nSecret:\n\n- Elevate risk if exposed\n\n- Threat\n\n\nDiff secrets / identifiers\n\n- Identifier should not be discoled but are not secrets\n\n\nTrust:\n\n- Entity\n\n- Circle of Trust\n\n- Chain of Trust\n\n\nCircle of Trust\n\n- Storage of secrets\n\n\nChain of trust\n\n- Set of linkgs where the secret travels through\n\n- A link is an interception point\n\n\nContainers world\n\n\nClassic actors:\n\n- Scheduler\n\n- Scheduler Agent\n\n- Container\n\n- Secret Management\n\n\n=\n Lots of moving part\n\n\nProblems / Criteria\n\n1. Secrets protection\n\n2. Secrets distribution\n\n3. Complexity\n\n4. Access detection\n\n5. Break-glass procedure (If compromised)\n\n  - Revoke everything - Stop access to secret\n\n  - Audit and check your trust chain\n\n  - Rotate new keys\n\n\nSecrets management tools\n\n\nMany exists\n\nBUT, not made for dynamic environment\n\n\nSI for containers\n\n\nVault\n\n- Primitives for dynamic cred mgnt\n\n\nVault + Scheduluer = \n3\n\n- Scheduler are source of TRUTH\n\n\nPreconditions\n\n- Unlimited / Limited use-count\n\n- Limited TTL (Renewable)\n\n- Access / Actions Policies\n\n- Scope\n\n\nExample:\n\n\nScheduler\n- Create a new Instance\n- Sunmit to Agent\n\nScheduler agent\n- Has Token\n  + Scope: App_token_create\n- Contact Vault\n- Get a TMP token of the container\n  + Use count 1\n  + TTL 30s\n- Provide the Token to the container\n\nContainer\n- Contact Vault with temp token\n  + Use count =\n 0\n- Gets a new dedicated token\n  + Use count: unlimitted\n  + TTL 1h\n- Use this token to get Creds for every 3rd service it needs to connect at starting\n  =\n TTL 1h\n\n\n\n\n=\n We got:\n\n- Limited time for secrets\n\n- Limited access\n\n- Limited usage", 
            "title": "20160615 3 Secrets WithContainers"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/#agenda", 
            "text": "What it is about: \n* Paradigm + Pattern \n* Workflow  What it is not: \n* A TRUTH guide \n* A deep dive", 
            "title": "Agenda"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/#terms", 
            "text": "Security: \n- Risk management \n- Accepting some \n- Guarding  Risk: \n- Increase with system complexity  Secret: \n- Elevate risk if exposed \n- Threat  Diff secrets / identifiers \n- Identifier should not be discoled but are not secrets  Trust: \n- Entity \n- Circle of Trust \n- Chain of Trust  Circle of Trust \n- Storage of secrets  Chain of trust \n- Set of linkgs where the secret travels through \n- A link is an interception point", 
            "title": "Terms"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/#containers-world", 
            "text": "Classic actors: \n- Scheduler \n- Scheduler Agent \n- Container \n- Secret Management  =  Lots of moving part  Problems / Criteria \n1. Secrets protection \n2. Secrets distribution \n3. Complexity \n4. Access detection \n5. Break-glass procedure (If compromised) \n  - Revoke everything - Stop access to secret \n  - Audit and check your trust chain \n  - Rotate new keys", 
            "title": "Containers world"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/#secrets-management-tools", 
            "text": "Many exists \nBUT, not made for dynamic environment", 
            "title": "Secrets management tools"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_3_Secrets-WithContainers/#si-for-containers", 
            "text": "Vault \n- Primitives for dynamic cred mgnt  Vault + Scheduluer =  3 \n- Scheduler are source of TRUTH  Preconditions \n- Unlimited / Limited use-count \n- Limited TTL (Renewable) \n- Access / Actions Policies \n- Scope  Example:  Scheduler\n- Create a new Instance\n- Sunmit to Agent\n\nScheduler agent\n- Has Token\n  + Scope: App_token_create\n- Contact Vault\n- Get a TMP token of the container\n  + Use count 1\n  + TTL 30s\n- Provide the Token to the container\n\nContainer\n- Contact Vault with temp token\n  + Use count =  0\n- Gets a new dedicated token\n  + Use count: unlimitted\n  + TTL 1h\n- Use this token to get Creds for every 3rd service it needs to connect at starting\n  =  TTL 1h  =  We got: \n- Limited time for secrets \n- Limited access \n- Limited usage", 
            "title": "SI for containers"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/", 
            "text": "20160615_4_HashiStack\n\n\n\n\n\n\n\n0 - Overview\n\n\n1 - Stack\n\n\n2 - Demo\n\n\n2.0 - Initial State\n\n\n2.1 - Vault config\n\n\n2.2 - Nomad\n\n\nJob: HashiApp\n\n\nJob: Consul\n\n\nNomad scheduling\n\n\n\n\n\n\n2.3 - Fabio\n\n\n3 - Scale / Update\n\n\n4 - FAQ\n\n\n4.1 - k8s / Nomad\n\n\n4.2 -\n\n\n\n\n\n\n\n0 - Overview\n\n\nhttps://www.hashiconf.eu/talks/hyperscale-computing-with-grpc-and-the-hashiStack.html\n\n\n\n\nHow to build an \nhyperscale\n applications from the ground up using the \nHashiStack\n\n* Nomad,\n\n* Vault,\n\n* and Consul\n\n\n\n\n\n\n1 - Stack\n\n\n1 Server:\n\n- Consul\n\n- Nomad\n\n- Vault\n\n\n4 Clients\n\n- Consul\n\n- Nomad\n\n\n\n\nDNSMasq local to get \n.consul\n resolution\n\n\n\n\n2 - Demo\n\n\n2.0 - Initial State\n\n\n\n\nConsul UI, everything is OK\n\n\n3 Consul\n\n\n3 Nomad\n\n\n1 Vault\n\n\n\n\n\n\nVault\n can auto-register to Nomad / Consul since \n0.6.0\n\n\n\n\n\n\n$ nomad status\n\nNo job is running\n\n\n\n\n2.1 - Vault config\n\n\nPolicy\n\n- Secrets for the App:  Read, list\n\n- Secrets for MySQL:    Read, list\n\n- Token:                DONT forget to add an \nupdate\n policy for \nrenew\n\n\n\n\nIt is the responsability of the App to renew Token when needed\n\n\n\n\n\n\n2.2 - Nomad\n\n\nJob: HashiApp\n\n\nJob names\n\nUpdate          // For deployment\n\nGroup           // For Clustering\n\nTask \nHashiApp\n\n- Driver = exec\n- Env with VAULT tokens\n- Artifact\n\nResources       // For Nomad Scheduler\n- CPU\n- Memory\n- Network\n\nService         // For Consul registering\n- Name\n- Tags\n- Checks\n- ...\n\n\n\n\nJob: Consul\n\n\nA SYSTEM job definition to have a \nConsul\n agent running on EVERY Nomad workers\n\n\nNomad scheduling\n\n\n$ nomad plan job/consul.job\n\n\n$ nomad run job/consul.job\n\n\n\n\n2.3 - Fabio\n\n\nFabio desc:\n\n* A loadbalancer / router\n\n* Integrated with Consul\n\n  - Use services\n\n  - Tags\n\n  - \n\n\nFabio usage:\n\n* Nomad SYSTEM job\n\n* Binded to local consul\n\n=\n Every Fabio instances see the same things\n\n\n3 - Scale / Update\n\n\n\n\nUpdate Job\n\n\n$ nomad plan\n\n\n$ nomad run\n\n\n\n\nWhat happens:\n\n- Nomad:\n\n  + Services are started\n\n  + Registered in Consul\n\n- Consul\n\n  + Do Health checks\n\n  + check pass greens\n\n  + Services are in Consul catalog\n\n- Fabio\n\n  + list Services\n\n\n4 - FAQ\n\n\n4.1 - k8s / Nomad\n\n\nHashiTools are good pieces\n\nk8s is a platform manager\n\n\n=\n We are looking to integrate \nVault\n + \nNomad\n (Scheduling) into k8s\n\n\n4.2 -", 
            "title": "20160615 4 HashiStack"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#0-overview", 
            "text": "https://www.hashiconf.eu/talks/hyperscale-computing-with-grpc-and-the-hashiStack.html   How to build an  hyperscale  applications from the ground up using the  HashiStack \n* Nomad, \n* Vault, \n* and Consul", 
            "title": "0 - Overview"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#1-stack", 
            "text": "1 Server: \n- Consul \n- Nomad \n- Vault  4 Clients \n- Consul \n- Nomad   DNSMasq local to get  .consul  resolution", 
            "title": "1 - Stack"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#2-demo", 
            "text": "", 
            "title": "2 - Demo"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#20-initial-state", 
            "text": "Consul UI, everything is OK  3 Consul  3 Nomad  1 Vault    Vault  can auto-register to Nomad / Consul since  0.6.0    $ nomad status\n\nNo job is running", 
            "title": "2.0 - Initial State"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#21-vault-config", 
            "text": "Policy \n- Secrets for the App:  Read, list \n- Secrets for MySQL:    Read, list \n- Token:                DONT forget to add an  update  policy for  renew   It is the responsability of the App to renew Token when needed", 
            "title": "2.1 - Vault config"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#22-nomad", 
            "text": "", 
            "title": "2.2 - Nomad"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#job-hashiapp", 
            "text": "Job names\n\nUpdate          // For deployment\n\nGroup           // For Clustering\n\nTask  HashiApp \n- Driver = exec\n- Env with VAULT tokens\n- Artifact\n\nResources       // For Nomad Scheduler\n- CPU\n- Memory\n- Network\n\nService         // For Consul registering\n- Name\n- Tags\n- Checks\n- ...", 
            "title": "Job: HashiApp"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#job-consul", 
            "text": "A SYSTEM job definition to have a  Consul  agent running on EVERY Nomad workers", 
            "title": "Job: Consul"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#nomad-scheduling", 
            "text": "$ nomad plan job/consul.job\n\n\n$ nomad run job/consul.job", 
            "title": "Nomad scheduling"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#23-fabio", 
            "text": "Fabio desc: \n* A loadbalancer / router \n* Integrated with Consul \n  - Use services \n  - Tags \n  -   Fabio usage: \n* Nomad SYSTEM job \n* Binded to local consul \n=  Every Fabio instances see the same things", 
            "title": "2.3 - Fabio"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#3-scale-update", 
            "text": "Update Job  $ nomad plan  $ nomad run   What happens: \n- Nomad: \n  + Services are started \n  + Registered in Consul \n- Consul \n  + Do Health checks \n  + check pass greens \n  + Services are in Consul catalog \n- Fabio \n  + list Services", 
            "title": "3 - Scale / Update"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#4-faq", 
            "text": "", 
            "title": "4 - FAQ"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#41-k8s-nomad", 
            "text": "HashiTools are good pieces \nk8s is a platform manager  =  We are looking to integrate  Vault  +  Nomad  (Scheduling) into k8s", 
            "title": "4.1 - k8s / Nomad"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_4_HashiStack/#42-", 
            "text": "", 
            "title": "4.2 -"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/", 
            "text": "20160615_5_Fabio\n\n\n\n\n\n\n\n1 - Why\n\n\n2 - Routing in micro services\n\n\n3 - Fabio insight\n\n\n3.1 - Consul integration\n\n\n3.2 - Config language\n\n\n4 - Advantages\n\n\n5 - Today (new features)\n\n\n5.1 - Desc\n\n\n5.2 - Demo\n\n\n6 - Next\n\n\n7 - Stats\n\n\n8 - FAQ\n\n\n8.1 - HTTP 2.0 ?\n\n\n8.2 - A/B testing with Traffic Shapping ?\n\n\n8.3 - Fabio / Traeffic\n\n\n\n\n\n\n\n1 - Why\n\n\nYet an other LB ?\n\n- Nginx, Varnish, Apache, HAporyx, Traeffic, \n\n\n@Ebay\n\n* \nConsul\n integration\n\n* Time to write 1 dedicated\n\n\n2 - Routing in micro services\n\n\n1 URL Path\n\n1 Service\n\n\nBUT:\n\n* Highly dynamic / Short life instances\n\n* Dynamic PORT allocation\n\n\n3 - Fabio insight\n\n\n3.1 - Consul integration\n\n\n\n\nUse service-name\n\n\nUse TAGS as \nPath\n for routing table\n\n\n\n\nurlprefix-PATH\n\n\n3.2 - Config language\n\n\nCan overwrite Fabio dynamic config manually\n\n- DSL (Human readable)\n\n\nRouting table\n\n- \nroute add SERVICE_NAME PATH DESTINATION:PORT\n\n- \nroute dell SERVICE_NAME PATH DESTINATION:PORT\n\n\nTraffic shapping\n\n- \nroute weight\n\n\n4 - Advantages\n\n\nEasy:\n\n\n\n\n\n\nProvisionning service\n\n-\n Registering automatically\n\n-\n DONE\n\n\n\n\n\n\nNeed manual changes sometime\n\n-\n DONE\n\n\n\n\n\n\nWorks everywhere\n\n-\n Cloud\n\n-\n Laptop\n\n-\n HW\n\n\n\n\n\n\nRefactoring\n\n\n\n\n\n\n5 - Today (new features)\n\n\n5.1 - Desc\n\n\nCmd line args\n\nDynamic SSL certs management\n\nVault support\n\nSNI Support\n\n\n5.2 - Demo\n\n\nVault integration\n\n\n./fabio ... -type=vault;cert=secret/fabio/certs/DOMAIN\n;...\n\n\n\n\n6 - Next\n\n\n\n\n\n\n\n\n65k outbound connections\n\n\n\n\n\n\nRefactor \nurlpreifx-tag\n\n\nAdditionnal backends\n\n\n\n\n7 - Stats\n\n\n\n\nGo 1.6 + Httputil.ReverseProxy\n\n\n4.000 Lines\n\n\n1 Release / Month\n\n\nSrc\n\n\nBinary\n\n\n\n\nDocker image\n\n\n\n\n\n\nUsed in production (since Sept 2015)\n\n\n\n\nKijiji Italy (1200 rqps)\n\n\nEbay NL (15000 rqps)\n\n\n\n\nhttps://github.com/eBay/fabio\n\n\n8 - FAQ\n\n\n8.1 - HTTP 2.0 ?\n\n\nNot yet\n\n\n8.2 - A/B testing with Traffic Shapping ?\n\n\nWe use it to switch from Java -\n Go\n\n\nBUT\n\n- No reall tips on HOW to measure stuff with Fabio\n\n- Too much \nglue\n\n\n8.3 - Fabio / Traeffic\n\n\nTraeffic\n\n- feature rich\n\n\nBut at the time I though about OpenSourcing Fabio, was not so advanced", 
            "title": "20160615 5 Fabio"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#1-why", 
            "text": "Yet an other LB ? \n- Nginx, Varnish, Apache, HAporyx, Traeffic,   @Ebay \n*  Consul  integration \n* Time to write 1 dedicated", 
            "title": "1 - Why"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#2-routing-in-micro-services", 
            "text": "1 URL Path \n1 Service  BUT: \n* Highly dynamic / Short life instances \n* Dynamic PORT allocation", 
            "title": "2 - Routing in micro services"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#3-fabio-insight", 
            "text": "", 
            "title": "3 - Fabio insight"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#31-consul-integration", 
            "text": "Use service-name  Use TAGS as  Path  for routing table   urlprefix-PATH", 
            "title": "3.1 - Consul integration"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#32-config-language", 
            "text": "Can overwrite Fabio dynamic config manually \n- DSL (Human readable)  Routing table \n-  route add SERVICE_NAME PATH DESTINATION:PORT \n-  route dell SERVICE_NAME PATH DESTINATION:PORT  Traffic shapping \n-  route weight", 
            "title": "3.2 - Config language"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#4-advantages", 
            "text": "Easy:    Provisionning service \n-  Registering automatically \n-  DONE    Need manual changes sometime \n-  DONE    Works everywhere \n-  Cloud \n-  Laptop \n-  HW    Refactoring", 
            "title": "4 - Advantages"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#5-today-new-features", 
            "text": "", 
            "title": "5 - Today (new features)"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#51-desc", 
            "text": "Cmd line args \nDynamic SSL certs management \nVault support \nSNI Support", 
            "title": "5.1 - Desc"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#52-demo", 
            "text": "Vault integration  ./fabio ... -type=vault;cert=secret/fabio/certs/DOMAIN ;...", 
            "title": "5.2 - Demo"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#6-next", 
            "text": "65k outbound connections    Refactor  urlpreifx-tag  Additionnal backends", 
            "title": "6 - Next"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#7-stats", 
            "text": "Go 1.6 + Httputil.ReverseProxy  4.000 Lines  1 Release / Month  Src  Binary   Docker image    Used in production (since Sept 2015)   Kijiji Italy (1200 rqps)  Ebay NL (15000 rqps)   https://github.com/eBay/fabio", 
            "title": "7 - Stats"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#8-faq", 
            "text": "", 
            "title": "8 - FAQ"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#81-http-20", 
            "text": "Not yet", 
            "title": "8.1 - HTTP 2.0 ?"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#82-ab-testing-with-traffic-shapping", 
            "text": "We use it to switch from Java -  Go  BUT \n- No reall tips on HOW to measure stuff with Fabio \n- Too much  glue", 
            "title": "8.2 - A/B testing with Traffic Shapping ?"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_5_Fabio/#83-fabio-traeffic", 
            "text": "Traeffic \n- feature rich  But at the time I though about OpenSourcing Fabio, was not so advanced", 
            "title": "8.3 - Fabio / Traeffic"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/", 
            "text": "20160615_6_Consul-LoveStory\n\n\n\n\n\n\n\n1 - Ideal Operator workflow\n\n\n2 - Before\n\n\nTech stack\n\n\nConstat\n\n\nFeeling\n\n\n3 - New\n\n\n3.1 - Replace Chef\n\n\n3.2 - Replace Munin\n\n\n3.3 - Replace HAproxy\n\n\n3.4 - Monitoring / Alerts\n\n\n4 - Ccl\n\n\n5 - Demos\n\n\n6 - Next\n\n\n\n\n\n\n\n1 - Ideal Operator workflow\n\n\n2 - Before\n\n\n@Bondary\n\n\nTech stack\n\n\n\n\nChef provisionning\n\n\n\n\nMunin Metrics\n\n\n\n\n\n\n32 Services\n\n\n\n\n3 Instances of each\n\n\nHaproxy fanout\n\n\n\n\nConstat\n\n\nCFG mgnt was 75% of product\ns code base \n\n\nFeeling\n\n\nCfg mgnt is a delivery vehicule for unfinished work\n\n- Like connecting dots \n\n- Sucks but is sustainable\n\n\nUNTIL\n\n- Pivot to merge software stacks\n\n- People leaves\n\n- Saas, but sometime 1 client want premise\n\n  + Need bootstrapping capabilities\n\n\n3 - New\n\n\n3.1 - Replace Chef\n\n\nReplaced by :\n\n- Serf (no server)\n\n- little tool \ncascade .5\n  (hone made)\n\n\n=\n SUCCESS\n\n=\n Less painfull\n\n=\n More suitable for our needs\n\n\n3.2 - Replace Munin\n\n\nRemove active checks\n\nGO passive checks\n\n=\n Sensu\n\n\n3.3 - Replace HAproxy\n\n\nBy Consul\n\n\nReservation\n\n- We already have ZK\n\n- No NODE tags\n\n- Events over Serf ?\n\n\nSelling points\n\n- ZK overloaded by Kafka\n\n- Nice API\n\n- Nice Patterns and usability\n\n- Nice langage clients\n\n- Semantic for service discovery\n\n\n=\n Remove Haproxy by Nginx\n\n\n3.4 - Monitoring / Alerts\n\n\nChecks are now in Consul\n\n-\n Agregate checks and make actionnable\n\n-\n \nconsul-alerts\n Opensource tool\n\n\n4 - Ccl\n\n\n\n\nCfg is minimal\n\n\nBootstrap = Create Node + install packages\n\n\nSustainable (service registering + Monitoring)\n\n\n\n\n5 - Demos\n\n\n6 - Next\n\n\nConfig\n\n- Static values (only changes per environment)\n\n  + In Consul\n\n- Dynamic values\n\n  + In Consul\n\n- Secrets\n\n  + In Vault\n\n\nhttps://github.com/boundary/cascade\n         (Transitional orchestration shell)\n\n\nhttps://github.com/boundary/chef-cascade\n    (Chef code as packages)", 
            "title": "20160615 6 Consul LoveStory"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#1-ideal-operator-workflow", 
            "text": "", 
            "title": "1 - Ideal Operator workflow"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#2-before", 
            "text": "@Bondary", 
            "title": "2 - Before"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#tech-stack", 
            "text": "Chef provisionning   Munin Metrics    32 Services   3 Instances of each  Haproxy fanout", 
            "title": "Tech stack"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#constat", 
            "text": "CFG mgnt was 75% of product s code base", 
            "title": "Constat"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#feeling", 
            "text": "Cfg mgnt is a delivery vehicule for unfinished work \n- Like connecting dots  \n- Sucks but is sustainable  UNTIL \n- Pivot to merge software stacks \n- People leaves \n- Saas, but sometime 1 client want premise \n  + Need bootstrapping capabilities", 
            "title": "Feeling"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#3-new", 
            "text": "", 
            "title": "3 - New"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#31-replace-chef", 
            "text": "Replaced by : \n- Serf (no server) \n- little tool  cascade .5   (hone made)  =  SUCCESS \n=  Less painfull \n=  More suitable for our needs", 
            "title": "3.1 - Replace Chef"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#32-replace-munin", 
            "text": "Remove active checks \nGO passive checks \n=  Sensu", 
            "title": "3.2 - Replace Munin"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#33-replace-haproxy", 
            "text": "By Consul  Reservation \n- We already have ZK \n- No NODE tags \n- Events over Serf ?  Selling points \n- ZK overloaded by Kafka \n- Nice API \n- Nice Patterns and usability \n- Nice langage clients \n- Semantic for service discovery  =  Remove Haproxy by Nginx", 
            "title": "3.3 - Replace HAproxy"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#34-monitoring-alerts", 
            "text": "Checks are now in Consul \n-  Agregate checks and make actionnable \n-   consul-alerts  Opensource tool", 
            "title": "3.4 - Monitoring / Alerts"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#4-ccl", 
            "text": "Cfg is minimal  Bootstrap = Create Node + install packages  Sustainable (service registering + Monitoring)", 
            "title": "4 - Ccl"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#5-demos", 
            "text": "", 
            "title": "5 - Demos"
        }, 
        {
            "location": "/2016/hashiconf/day2/20160615_6_Consul-LoveStory/#6-next", 
            "text": "Config \n- Static values (only changes per environment) \n  + In Consul \n- Dynamic values \n  + In Consul \n- Secrets \n  + In Vault  https://github.com/boundary/cascade          (Transitional orchestration shell)  https://github.com/boundary/chef-cascade     (Chef code as packages)", 
            "title": "6 - Next"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/", 
            "text": "20171018_DockerConEu17_1\n\n\n\n\n\n\n\n1 - Tips \n Tricks\n\n\n1.1 - Docker Cli\n\n\n1.2 - Docker Build\n\n\n1.3 - Docker Run\n\n\n1.4 - Docker in Docker\n\n\n\n\n\n\n\n1 - Tips \n Tricks\n\n\n1.1 - Docker Cli\n\n\n\n\nFormat \ndocker ps\n output to get more readability\n\n\n\n\ndocker.json\n config file can keep format preferences\n\n\n\n\n\n\nUse new cli\n\n\n\n\ndocker container\n,\n\n\ndocker image\n,\n\n\ndocker volume\n,\n\n\ndocker network\n,\n\n\n\n\n\n\n\n\n\n\nPrune\n\n\n\n\ndocker container prune\n\n\ndocker image prune\n\n\n\n\n\n\n1.2 - Docker Build\n\n\n\n\nLitgh Base imnage\n\n\nOrder layer smartly\n\n\n\n\n1.3 - Docker Run\n\n\n\n\nRO FS\n\n\n\n\ndocker run .... \\\n  --read-only \\\n  --tmpfs /var/run \\\n  --tmpfs /var/cache \\\n  ...\n\n\n\n\n\n\nDo not run as root\n\n\ngosu\n\n\n\n\n1.4 - Docker in Docker\n\n\n\n\nUse volume and unix socket (docker / Display / \n)", 
            "title": "20171018_DockerConEu17_1"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/#1-tips-tricks", 
            "text": "", 
            "title": "1 - Tips &amp; Tricks"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/#11-docker-cli", 
            "text": "Format  docker ps  output to get more readability   docker.json  config file can keep format preferences    Use new cli   docker container ,  docker image ,  docker volume ,  docker network ,      Prune   docker container prune  docker image prune", 
            "title": "1.1 - Docker Cli"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/#12-docker-build", 
            "text": "Litgh Base imnage  Order layer smartly", 
            "title": "1.2 - Docker Build"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/#13-docker-run", 
            "text": "RO FS   docker run .... \\\n  --read-only \\\n  --tmpfs /var/run \\\n  --tmpfs /var/cache \\\n  ...   Do not run as root  gosu", 
            "title": "1.3 - Docker Run"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_1/#14-docker-in-docker", 
            "text": "Use volume and unix socket (docker / Display /  )", 
            "title": "1.4 - Docker in Docker"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/", 
            "text": "20171018_DockerConEu17_2\n\n\n\n\n\n\n\n2 - Deep dive into overlay network\n\n\n2.1 - Intro\n\n\nWhat is an overlay network ?\n\n\nHow does it work ?\n\n\nDemo\n\n\n\n\n\n\n2.2 - VxLan Controle planes\n\n\n2.3 - BGP dynamic Control Plane\n\n\nIntro\n\n\nBGP\n\n\nDemo\n\n\n\n\n\n\n2.4 - Getting out of VxLan\n\n\n\n\n\n\n\n2 - Deep dive into overlay network\n\n\nhttps://github.com/lbernail/dockercon2017\n\n\n2.1 - Intro\n\n\nWhat is an overlay network ?\n\n\nHow does it work ?\n\n\nDemo\n\n\nMerged in Kernel\n\n-\n Available through \nclassic\n tools\n\n\n\n\nip netns add overns\n                         # Create a NetSapce\n\n\nip netns exec overns ip link create ...\n     # Create link in the NS\n\n\nip netns exec overns ip address add ...\n     # Give address\n\n\n\n\n2.2 - VxLan Controle planes\n\n\n\n\nMultiCast\n\n\nP2P\n\n\nUser-land\n\n\n\n\n=\n Video: DockerCon Austin 2017\n\n\n2.3 - BGP dynamic Control Plane\n\n\nIntro\n\n\nOption4\n\n\n++\n\n* \nbgpd\n + \nevpn\n for L2 and L3\n\n* Standards and supported by SDNs\n\n\n\n* Requieres knowledge\n\n\nBGP\n\n\n\n\nRouting protocol\n\n\nIs an EGP\n\n\nWe will ise \niBGP\n (Internal)\n\n\n\n\nEvpn\n\n* Parts of MG-BGP\n\n\nDemo\n\n\n\n\nQuagga\n\n\nContainerized (\ncumulusnetworks/quagga:CL3.2.1_evpn\n)\n\n\n\n\n--priviliged\n\n\n\n\n\n\nVxLan setup\n\n\n\n\nBridge\n\n\nVxLAN ID\n\n\n\n\nIn the QUAGGA container\n\n\n\n\n\n\nConnect 2 containers through Quagga\n\n\n\n\neth0 -\n br42\n\n\n\n\n-\n ControlPlane\n\n-\n DataPlane\n\n\nAdvantage\n\n* ARP\n\n* Broadcat (DHCP)\n\n* Multicast\n\n\n2.4 - Getting out of VxLan\n\n\n\n\nGateway machine running Quagga\n\n\nAdding a \nveth\n with NAT", 
            "title": "20171018_DockerConEu17_2"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#2-deep-dive-into-overlay-network", 
            "text": "https://github.com/lbernail/dockercon2017", 
            "title": "2 - Deep dive into overlay network"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#21-intro", 
            "text": "", 
            "title": "2.1 - Intro"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#what-is-an-overlay-network", 
            "text": "", 
            "title": "What is an overlay network ?"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#how-does-it-work", 
            "text": "", 
            "title": "How does it work ?"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#demo", 
            "text": "Merged in Kernel \n-  Available through  classic  tools   ip netns add overns                          # Create a NetSapce  ip netns exec overns ip link create ...      # Create link in the NS  ip netns exec overns ip address add ...      # Give address", 
            "title": "Demo"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#22-vxlan-controle-planes", 
            "text": "MultiCast  P2P  User-land   =  Video: DockerCon Austin 2017", 
            "title": "2.2 - VxLan Controle planes"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#23-bgp-dynamic-control-plane", 
            "text": "", 
            "title": "2.3 - BGP dynamic Control Plane"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#intro", 
            "text": "Option4  ++ \n*  bgpd  +  evpn  for L2 and L3 \n* Standards and supported by SDNs  \n* Requieres knowledge", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#bgp", 
            "text": "Routing protocol  Is an EGP  We will ise  iBGP  (Internal)   Evpn \n* Parts of MG-BGP", 
            "title": "BGP"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#demo_1", 
            "text": "Quagga  Containerized ( cumulusnetworks/quagga:CL3.2.1_evpn )   --priviliged    VxLan setup   Bridge  VxLAN ID   In the QUAGGA container    Connect 2 containers through Quagga   eth0 -  br42   -  ControlPlane \n-  DataPlane  Advantage \n* ARP \n* Broadcat (DHCP) \n* Multicast", 
            "title": "Demo"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_2/#24-getting-out-of-vxlan", 
            "text": "Gateway machine running Quagga  Adding a  veth  with NAT", 
            "title": "2.4 - Getting out of VxLan"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_3/", 
            "text": "20171018_DockerConEu17_3\n\n\n\n\n\n\n\n3 - Container Orchestration\n\n\n3.0 - Intro\n\n\n\n\n\n\n\n3 - Container Orchestration\n\n\n3.0 - Intro\n\n\n\n\nOrchestration is about convergence\n\n\nExpress \ndesired state\n / Current state", 
            "title": "20171018_DockerConEu17_3"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_3/#3-container-orchestration", 
            "text": "", 
            "title": "3 - Container Orchestration"
        }, 
        {
            "location": "/2017/DockerCon/20171018_DockerConEu17_3/#30-intro", 
            "text": "Orchestration is about convergence  Express  desired state  / Current state", 
            "title": "3.0 - Intro"
        }, 
        {
            "location": "/2017/kubecon/README/", 
            "text": "README\n\n\n\n\nNotes taken during the CNCF / KubeCon EU17\n\n\nhttps://cloudnativeeu2017.sched.com/", 
            "title": "README"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/", 
            "text": "20170329_kubeCon_1.1\n\n\n\n\n\n\n\n1 - Keynotes\n\n\n1.1 - CNCF opening\n\n\n1.2 - ContainerD joins CNCF\n\n\n1.3 - Rkt joins CNCF\n\n\n1.4 - K8s 1.6\n\n\n1.5 - Future of Containers security\n\n\n1.6 - Open-AI\n\n\n1.7 - Networking - Tigera\n\n\n1.8 - Prometheus\n\n\n\n\n\n\n\n1 - Keynotes\n\n\n1.1 - CNCF opening\n\n\n1.2 - ContainerD joins CNCF\n\n\n1.3 - Rkt joins CNCF\n\n\n1.4 - K8s 1.6\n\n\nFeatures\n\n* Scale till 5000 Nodes / cluster\n\n* RBAC: Mutli-users / Tenant cluster sharing per Namespace\n\n  - ViewOnly\n\n  - Access\n\n  - ClusterAdmin\n\n  - \n\n* Controlled scheduling\n\n  - Node / pod afinity\n\n  - Taints / Tolerations (Forgiveness and Auto eviction)\n\n  - Custom scheduler\n\n* Dynamic Storage provisioning\n\n  - Support for Cloud Storage service\n\n\nRoadMap:\n\n* Larger clusters\n\n* Network Policy\n\n* Statefull upgrades\n\n* Multi-workloads\n\n* Cloud Providers\n\n* Service Catalog\n\n\n1.5 - Future of Containers security\n\n\n\n\nRBAC is not tuned on by default.\n\n\nWe have to see as a community what we want to do.\n\n\nEven if RBAC is tuned-on, it doesn\nt mean Apps are using it.\n\n\n\n\n1.6 - Open-AI\n\n\n1.7 - Networking - Tigera\n\n\n1.8 - Prometheus", 
            "title": "20170329_kubeCon_1.1"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#1-keynotes", 
            "text": "", 
            "title": "1 - Keynotes"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#11-cncf-opening", 
            "text": "", 
            "title": "1.1 - CNCF opening"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#12-containerd-joins-cncf", 
            "text": "", 
            "title": "1.2 - ContainerD joins CNCF"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#13-rkt-joins-cncf", 
            "text": "", 
            "title": "1.3 - Rkt joins CNCF"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#14-k8s-16", 
            "text": "Features \n* Scale till 5000 Nodes / cluster \n* RBAC: Mutli-users / Tenant cluster sharing per Namespace \n  - ViewOnly \n  - Access \n  - ClusterAdmin \n  -  \n* Controlled scheduling \n  - Node / pod afinity \n  - Taints / Tolerations (Forgiveness and Auto eviction) \n  - Custom scheduler \n* Dynamic Storage provisioning \n  - Support for Cloud Storage service  RoadMap: \n* Larger clusters \n* Network Policy \n* Statefull upgrades \n* Multi-workloads \n* Cloud Providers \n* Service Catalog", 
            "title": "1.4 - K8s 1.6"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#15-future-of-containers-security", 
            "text": "RBAC is not tuned on by default.  We have to see as a community what we want to do.  Even if RBAC is tuned-on, it doesn t mean Apps are using it.", 
            "title": "1.5 - Future of Containers security"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#16-open-ai", 
            "text": "", 
            "title": "1.6 - Open-AI"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#17-networking-tigera", 
            "text": "", 
            "title": "1.7 - Networking - Tigera"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.1/#18-prometheus", 
            "text": "", 
            "title": "1.8 - Prometheus"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/", 
            "text": "20170329_kubeCon_1.2\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.1 - Powering Public Infrastructure with Kubernetes\n\n\nContext\n\n\nThe Stack\n\n\nCFG MGMT ?\n\n\nChallenges\n\n\nDev\nOps collaboration\n\n\nCI/CD\n\n\nDeployment\n\n\nStill challenging\n\n\nFuture\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.1 - Powering Public Infrastructure with Kubernetes\n\n\nContext\n\n\nBusiness: Deutch Bahn public display of Trains departures, arrivals and misc\n\n\n2 years ago: Not a lot of products and possibility\n\nNowadays: Plenty of solutions !\n\n\nThe Stack\n\n\n\n\nCoreOS\n\n\nAlpine Linux within Docker\n\n\nK8s Clustering\n\n\nK8s Orchestrations\n\n\n\n\nCFG MGMT ?\n\n\n\n\nFor applications deployment: We did want to go with cfgmnt (Puppet, Chef, Ansible or whatever)\n\n\nFor underlying Infra: Why not\n\n\n\n\nChallenges\n\n\n\n\nBaremetal\n\n\nPrivate Cloud\n\n\nPublic Cloud\n\n\n\n\nDev\nOps collaboration\n\n\n\n\nDelivering \nkubectl\n access to devs\n\n\nRunning on AWS but not using the Saas: Raw IAAS\n\n\n1 Portable API\n\n\n\n\nCI/CD\n\n\n\n\nLots of products and new stuff\n\n\nWe went with Gitlab workers\n\n\n\n\nDeployment\n\n\n\n\nNamespaces as \nisolated environments\n\n\nHelm Charts + Go Templating + k8s objects (Config map, secrets,\n\n\n\n\nStill challenging\n\n\n\n\nk8s Bootstraping\n\n\nk8s Upgrade\n\n\nDebugging capacities\n\n\nLegacy apps\n\n\nBeware of appealing alpha or incubator features\n\n\n\n\nFuture\n\n\n\n\nFederations\n\n\nFailover\n\n\nMove to RBAC with k8s  1.6", 
            "title": "20170329_kubeCon_1.2"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#21-powering-public-infrastructure-with-kubernetes", 
            "text": "", 
            "title": "2.1 - Powering Public Infrastructure with Kubernetes"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#context", 
            "text": "Business: Deutch Bahn public display of Trains departures, arrivals and misc  2 years ago: Not a lot of products and possibility \nNowadays: Plenty of solutions !", 
            "title": "Context"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#the-stack", 
            "text": "CoreOS  Alpine Linux within Docker  K8s Clustering  K8s Orchestrations", 
            "title": "The Stack"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#cfg-mgmt", 
            "text": "For applications deployment: We did want to go with cfgmnt (Puppet, Chef, Ansible or whatever)  For underlying Infra: Why not", 
            "title": "CFG MGMT ?"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#challenges", 
            "text": "Baremetal  Private Cloud  Public Cloud", 
            "title": "Challenges"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#devops-collaboration", 
            "text": "Delivering  kubectl  access to devs  Running on AWS but not using the Saas: Raw IAAS  1 Portable API", 
            "title": "Dev&amp;Ops collaboration"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#cicd", 
            "text": "Lots of products and new stuff  We went with Gitlab workers", 
            "title": "CI/CD"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#deployment", 
            "text": "Namespaces as  isolated environments  Helm Charts + Go Templating + k8s objects (Config map, secrets,", 
            "title": "Deployment"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#still-challenging", 
            "text": "k8s Bootstraping  k8s Upgrade  Debugging capacities  Legacy apps  Beware of appealing alpha or incubator features", 
            "title": "Still challenging"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.1/#future", 
            "text": "Federations  Failover  Move to RBAC with k8s  1.6", 
            "title": "Future"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/", 
            "text": "20170329_kubeCon_1.2.2\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.2 - Reworking your teams for k8s (@Yahoo)\n\n\nStarting point\n\n\nYahoo\n\n\nPath Forward\n\n\nHow to\n\n\nBumps in the road\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.2 - Reworking your teams for k8s (@Yahoo)\n\n\nStarting point\n\n\n\n\nVery large on-prem server infra\n\n\nJava, Node, many more\n\n\nMinimal containerization\n\n\nSome \ninternal cloud\n\n\nTraditionnal\n ops\n\n\n\n\nYahoo\n\n\n\n\nInternet player for a long time\n\n\nBig player in the industry\n\n\nContributer\n\n\nHadoop\n\n\nNodeJS\n\n\n\n\n\n\nbut NO CLOUD yet \n\n\nPath Forward\n\n\nContext:\n\n\nPod (Yahoo style):\n\n* Apps +\n\n* Auth sidecar\n\n* Logging sidecar\n\n* Metrics sidecar\n\n\nQuestions\n\n\n\n\nIs it worth it ?\n\n\nDo we have the skillset to manage it ?\n\n\nHow much work will it be to adapt my systems ?\n\n\nIs it ready for \nmass market\n ?\n\n\n\n\nTech Challenges\n\n\n\n\nDoes the App start fast enough to live under orchestration ?\n\n\nAre there IP constraints ?\n\n\nLimit Cluster access but still make debugging easy ?\n\n\n\n\nHow to\n\n\nRule 1: Know thyself\n\n* Which App can be moved first\n\n* Resources needs\n\n* Availability needs\n\n\nRule2: Build a Team\n\n* Practical on-call experience\n\n  + Everyone: Team Leads, Fronts, back, SRE, \n\n* Key experts literature\n\n  + Monitoring\n\n  + Logging\n\n* Brown Bag learning\n\n  + Build an internal community\n\n\nBumps in the road\n\n\n\n\nPackages management issues\n\n\nOS compatibility\n\n\nSet sane DEFAULTS\n\n\n\n\n=\n Tune for real world use cases", 
            "title": "20170329_kubeCon_1.2.2"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#22-reworking-your-teams-for-k8s-yahoo", 
            "text": "", 
            "title": "2.2 - Reworking your teams for k8s (@Yahoo)"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#starting-point", 
            "text": "Very large on-prem server infra  Java, Node, many more  Minimal containerization  Some  internal cloud  Traditionnal  ops", 
            "title": "Starting point"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#yahoo", 
            "text": "Internet player for a long time  Big player in the industry  Contributer  Hadoop  NodeJS    but NO CLOUD yet", 
            "title": "Yahoo"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#path-forward", 
            "text": "", 
            "title": "Path Forward"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#context", 
            "text": "Pod (Yahoo style): \n* Apps + \n* Auth sidecar \n* Logging sidecar \n* Metrics sidecar", 
            "title": "Context:"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#questions", 
            "text": "Is it worth it ?  Do we have the skillset to manage it ?  How much work will it be to adapt my systems ?  Is it ready for  mass market  ?", 
            "title": "Questions"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#tech-challenges", 
            "text": "Does the App start fast enough to live under orchestration ?  Are there IP constraints ?  Limit Cluster access but still make debugging easy ?", 
            "title": "Tech Challenges"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#how-to", 
            "text": "Rule 1: Know thyself \n* Which App can be moved first \n* Resources needs \n* Availability needs  Rule2: Build a Team \n* Practical on-call experience \n  + Everyone: Team Leads, Fronts, back, SRE,  \n* Key experts literature \n  + Monitoring \n  + Logging \n* Brown Bag learning \n  + Build an internal community", 
            "title": "How to"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.2/#bumps-in-the-road", 
            "text": "Packages management issues  OS compatibility  Set sane DEFAULTS   =  Tune for real world use cases", 
            "title": "Bumps in the road"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/", 
            "text": "20170329_kubeCon_1.2.3\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.3 - Hosting and managing K8s clusters @Google\n\n\nGKE\n\n\nChallenges\n\n\nHow: Creating a cluster\n\n\nHow: Upgrading\n\n\nHow: Repairs\n\n\nRunning you Apps\n\n\nAuto-scaling\n\n\nGlobal network: AKA federation\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.3 - Hosting and managing K8s clusters @Google\n\n\nGKE\n\n\nHosted and Managed vanilla K8s clusters\n\n\nChallenges\n\n\nCreate / Destroy k8s cluster\n\nEasily, quickly\n\n\nHow: Creating a cluster\n\n\n\n\nBuild an Koos image\n\n  + k8s Optimized OS\n\n  + Based on Chromium OS\n\n\nCreate 1 Master VM\n\n  + Configure using Metadata server\n\n\nCreate Nodes VM\n\n  + In an MIG (Auto-scaling groups)\n\n  + Create a NodePool (that contains the MIG)\n\n\nNetworking Magic\n\n  + Overlay network for the Pods\n\n  + Routes to reach the master\n\n\n\n\n=\n 3 min\n\n\nHow: Upgrading\n\n\nMonitoring:\n\n* Watch the master VM\n\n* Watch the Kube-system\n\n* Watch the nodes VM\n\n* Watch the MIG\n\n\nNotes:\n\n* ECTD is store on persistent disk\n\n* Master has STATIC IP\n\n\nProcess:\n\n* Master:\n\n  + Shutdown the master\n\n  + Restart the master with new Image base\n\n* Nodes:\n\n  + Update MIG template\n\n  + Pick a node and drain it\n\n  + MIG recreate a new Node according to the MIG template\n\n\nOR: Use the NodePool feature\n\n  + New Pool with same / new MIG template\n\n  + Blue/green style\n\n\nHow: Repairs\n\n\n?\n\n\nRunning you Apps\n\n\nLeverage GKE Metadata as labels\n\n\nAuto-scaling\n\n\nPods: OK\n\n\nNodes: SIG\n\n* Cluster autoscaling is a signal that k8s will emit\n\n* Cloud provider will have to support it\n\n\nGlobal network: AKA federation\n\n\nFederated Ingress !\n\nSoon", 
            "title": "20170329_kubeCon_1.2.3"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#23-hosting-and-managing-k8s-clusters-google", 
            "text": "", 
            "title": "2.3 - Hosting and managing K8s clusters @Google"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#gke", 
            "text": "Hosted and Managed vanilla K8s clusters", 
            "title": "GKE"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#challenges", 
            "text": "Create / Destroy k8s cluster \nEasily, quickly", 
            "title": "Challenges"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#how-creating-a-cluster", 
            "text": "Build an Koos image \n  + k8s Optimized OS \n  + Based on Chromium OS  Create 1 Master VM \n  + Configure using Metadata server  Create Nodes VM \n  + In an MIG (Auto-scaling groups) \n  + Create a NodePool (that contains the MIG)  Networking Magic \n  + Overlay network for the Pods \n  + Routes to reach the master   =  3 min", 
            "title": "How: Creating a cluster"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#how-upgrading", 
            "text": "Monitoring: \n* Watch the master VM \n* Watch the Kube-system \n* Watch the nodes VM \n* Watch the MIG  Notes: \n* ECTD is store on persistent disk \n* Master has STATIC IP  Process: \n* Master: \n  + Shutdown the master \n  + Restart the master with new Image base \n* Nodes: \n  + Update MIG template \n  + Pick a node and drain it \n  + MIG recreate a new Node according to the MIG template  OR: Use the NodePool feature \n  + New Pool with same / new MIG template \n  + Blue/green style", 
            "title": "How: Upgrading"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#how-repairs", 
            "text": "?", 
            "title": "How: Repairs"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#running-you-apps", 
            "text": "Leverage GKE Metadata as labels", 
            "title": "Running you Apps"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#auto-scaling", 
            "text": "Pods: OK  Nodes: SIG \n* Cluster autoscaling is a signal that k8s will emit \n* Cloud provider will have to support it", 
            "title": "Auto-scaling"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.3/#global-network-aka-federation", 
            "text": "Federated Ingress ! \nSoon", 
            "title": "Global network: AKA federation"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/", 
            "text": "20170329_kubeCon_1.2.4\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.4 - k8s @Digital Ocean - A platform for the future\n\n\nContext\n\n\nNew tools\n\n\nRefining the solution\n\n\nResults\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.4 - k8s @Digital Ocean - A platform for the future\n\n\n\n\nOps is Hard\n\n\n\n\nContext\n\n\nDigital Ocean history\n\n* Early 2013: What ops mean ?\n\n* Late 2013: Embedded operations \nDevops\n ?\n\n* Early 2014: Centrealized Ops team\n\n* Early 2015: Operations as advisors\n\n* Mid 2015: \nYou build it, you run it\n\n\nTech changes\n\n* 2013: Monolith\n\n* 2014: Intro to services\n\n* 2015: Service all the things\n\n\nNew tools\n\n\nRequirements\n\n* Active community\n\n* API Driven\n\n* 1st class for containers support\n\n* Production grade (beeing used by other companies)\n\n\nEvaluation:\n\n* Nomad\n\n* Swam\n\n* Marathon\n\n* K8s\n\n* \n\n\n=\n k8s\n\n\nRefining the solution\n\n\nGuiding principles\n\n* Curate a simplified User experience\n\n* Make deployments declarative\n\n=\n \nDocc\n: DigitalOceanCentralController\n\n\nManifest:\n\n* Describe app runtime requirements\n\n* Integrate external systems\n\n* Establish ownership\n\n=\n Translated into k8s deployment\n\n\nWhy do this ?\n\n* Allows separations of responsabilities\n\n* Promotes decoupling\n\n=\n Integration and abstractions\n\n\nResults\n\n\nDeployment is no longer the bottleneck\n\n* New apps are deployed in hours\n\n* Existing apps deployed / updated in minutes\n\n=\n Developers are much happier", 
            "title": "20170329_kubeCon_1.2.4"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#24-k8s-digital-ocean-a-platform-for-the-future", 
            "text": "Ops is Hard", 
            "title": "2.4 - k8s @Digital Ocean - A platform for the future"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#context", 
            "text": "Digital Ocean history \n* Early 2013: What ops mean ? \n* Late 2013: Embedded operations  Devops  ? \n* Early 2014: Centrealized Ops team \n* Early 2015: Operations as advisors \n* Mid 2015:  You build it, you run it  Tech changes \n* 2013: Monolith \n* 2014: Intro to services \n* 2015: Service all the things", 
            "title": "Context"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#new-tools", 
            "text": "Requirements \n* Active community \n* API Driven \n* 1st class for containers support \n* Production grade (beeing used by other companies)  Evaluation: \n* Nomad \n* Swam \n* Marathon \n* K8s \n*   =  k8s", 
            "title": "New tools"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#refining-the-solution", 
            "text": "Guiding principles \n* Curate a simplified User experience \n* Make deployments declarative \n=   Docc : DigitalOceanCentralController  Manifest: \n* Describe app runtime requirements \n* Integrate external systems \n* Establish ownership \n=  Translated into k8s deployment  Why do this ? \n* Allows separations of responsabilities \n* Promotes decoupling \n=  Integration and abstractions", 
            "title": "Refining the solution"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.4/#results", 
            "text": "Deployment is no longer the bottleneck \n* New apps are deployed in hours \n* Existing apps deployed / updated in minutes \n=  Developers are much happier", 
            "title": "Results"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/", 
            "text": "20170329_kubeCon_1.2.5\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.5 - Storage cluster in k8s\n\n\nIntro\n\n\nk8s with converged storage ?\n\n\nRook\n\n\nExtending k8s\n\n\nDemo\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.5 - Storage cluster in k8s\n\n\nIntro\n\n\nk8s integration with external Storage (AWS, GCE, Gluster, Ceph, \n) is ok\n\nk8s hosting Storage system is an other story\n\n\nk8s with converged storage ?\n\n\nk8s dedicated Storage cluster\n\nor\n\nk8s within a shared cluster\n\n\nRook\n\n\n\n\nOpenSource\n\n\nStorage system:\n\n\nFile\n\n\nBlock\n\n\nObject\n\n\nIntegrated to run in k8s\n\n\n\n\nhttps://blog.rook.io/rook-operator-first-class-storage-for-kubernetes-2d0288831175\n\n\nBattle tested SDS\n\n* Based on CEPH\n\n\nExtending k8s\n\n\nRook is an k8s \nOperator\n for Ceph\n\n\nDemo\n\n\n\n\nDeploying a Ceph cluster\n\n\nAutoscaling Ceph cluster with K8s nodes cluster size", 
            "title": "20170329_kubeCon_1.2.5"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#25-storage-cluster-in-k8s", 
            "text": "", 
            "title": "2.5 - Storage cluster in k8s"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#intro", 
            "text": "k8s integration with external Storage (AWS, GCE, Gluster, Ceph,  ) is ok \nk8s hosting Storage system is an other story", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#k8s-with-converged-storage", 
            "text": "k8s dedicated Storage cluster \nor \nk8s within a shared cluster", 
            "title": "k8s with converged storage ?"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#rook", 
            "text": "OpenSource  Storage system:  File  Block  Object  Integrated to run in k8s   https://blog.rook.io/rook-operator-first-class-storage-for-kubernetes-2d0288831175  Battle tested SDS \n* Based on CEPH", 
            "title": "Rook"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#extending-k8s", 
            "text": "Rook is an k8s  Operator  for Ceph", 
            "title": "Extending k8s"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.5/#demo", 
            "text": "Deploying a Ceph cluster  Autoscaling Ceph cluster with K8s nodes cluster size", 
            "title": "Demo"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/", 
            "text": "20170329_kubeCon_1.2.6\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.6 - OnPremise k8s\n\n\nIntro\n\n\nAccessing Pods\n\n\nArchitectures\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.6 - OnPremise k8s\n\n\nIntro\n\n\nBusiness: Technology is the problem\n\nTech: Monolith is the problem\n\n-\n Micro services\n\n\nResults:\n\n* 1 Problem\n\n* 1000 Problems\n\n\nReal life:\n\n* Provisioning is too long\n\n* 1000 VMs take time to administrate\n\n* \n\n\nNew stack\n\n* CoreOS\n\n* k8s\n\n\nAccessing Pods\n\n\nNetworkPorts / Ingress\n\n\nNP:\n\n* Dynamicly allocated by k8s\n\n* With 1 LB in front (F5), too dynamic\n\n\nIngress:\n\n* Dedicated LB watching k8s API\n\n\nWARNING Notes: Ingress and reloads\n\n\n\n\nNginx ingress (default with k8s)\n\n\nDo upstream node lexical ordering based on IPs\n\n\nWhen each Ingress is reloaded on evey k8s nodes, the 1st pod get all the traffic\n\n-\n FAILS\n\n-\n Triggers and other reload and remove the failed pod from the upstream node list\n\n-\n The new 1st pod fails \n\n\n\n\nArchitectures\n\n\n1 Pod =\n\n* App\n\n* FluentD\n\n* CollectD + Carbon\n\n\n2 k8s clusters:\n\n* Dev / QA\n\n* Preprod / Prod\n\n\nEach App have 1 dedicated namespace / environment\n\n* app1-dev\n\n* app1-qa\n\n* app1-preprod\n\n* qpp1-prod\n\n\nNotes\n\n\nZero Downtime deployment:\n\n* Leverage the \nlifecycle:preStop\n statement of a deployment object\n\n\nCI/CD\n\n* CI is fully automated\n\n* CD has \ngates\n (Manual validation)\n\n* Every stages have canaries pods", 
            "title": "20170329_kubeCon_1.2.6"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#26-onpremise-k8s", 
            "text": "", 
            "title": "2.6 - OnPremise k8s"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#intro", 
            "text": "Business: Technology is the problem \nTech: Monolith is the problem \n-  Micro services  Results: \n* 1 Problem \n* 1000 Problems  Real life: \n* Provisioning is too long \n* 1000 VMs take time to administrate \n*   New stack \n* CoreOS \n* k8s", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#accessing-pods", 
            "text": "NetworkPorts / Ingress  NP: \n* Dynamicly allocated by k8s \n* With 1 LB in front (F5), too dynamic  Ingress: \n* Dedicated LB watching k8s API", 
            "title": "Accessing Pods"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#warning-notes-ingress-and-reloads", 
            "text": "Nginx ingress (default with k8s)  Do upstream node lexical ordering based on IPs  When each Ingress is reloaded on evey k8s nodes, the 1st pod get all the traffic \n-  FAILS \n-  Triggers and other reload and remove the failed pod from the upstream node list \n-  The new 1st pod fails", 
            "title": "WARNING Notes: Ingress and reloads"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#architectures", 
            "text": "1 Pod = \n* App \n* FluentD \n* CollectD + Carbon  2 k8s clusters: \n* Dev / QA \n* Preprod / Prod  Each App have 1 dedicated namespace / environment \n* app1-dev \n* app1-qa \n* app1-preprod \n* qpp1-prod", 
            "title": "Architectures"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.2.6/#notes", 
            "text": "Zero Downtime deployment: \n* Leverage the  lifecycle:preStop  statement of a deployment object  CI/CD \n* CI is fully automated \n* CD has  gates  (Manual validation) \n* Every stages have canaries pods", 
            "title": "Notes"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/", 
            "text": "20170329_kubeCon_1.3\n\n\n\n\n\n\n\n3 - Keynotes - End of the day\n\n\n3.1 - What makes the community happy\n\n\n3.2 - CoreOS / Tectonic / Helm\n\n\n3.3 - Philips Hue - Powered by k8s\n\n\n3.4 - Workload colocation on k8s\n\n\n3.5 - Helm\n\n\n3.6 - Closing of the day\n\n\n\n\n\n\n\n3 - Keynotes - End of the day\n\n\n3.1 - What makes the community happy\n\n\nk8s:\n\n* Since day 1 Open Source\n\n* Backed by Google engineers\n\n* Designed to run anywhere\n\n* Community has the opportunity to drive the project roadmap (SIGs)\n\n\nContributions:\n\n* 1 year ago: 67% was Google\n\n* Today: Google backed to 42%\n\n\nDid Google drop k8s ?\n\n-\n Nope\n\n-\n More and more Google\ns engineers working on it (GKE)\n\nBUT more and more contributors\n\n\nOpenStandards:\n\n* For user benefits\n\n* Composable systems\n\n* Open defined interfaces\n\n  + CNI\n\n  + OCI\n\n  + CSI (storage)\n\n  + OpenServiceBrocker API\n\n\nVelocity / Stability\n\n* Deals with SIGs projects integrations\n\n* Ease contributors onboarding\n\n\n3.2 - CoreOS / Tectonic / Helm\n\n\nLightning talk / demo about App-registry\n\n\n3.3 - Philips Hue - Powered by k8s\n\n\n3.4 - Workload colocation on k8s\n\n\n3.5 - Helm\n\n\nOPS Evolutions\n\n* SSH and do manual things\n\n* CFG mgnt\n\n* Containers + k8s\n\n\nDev point of views\n\n* More and more difficult\n\n* From 1 monolith to plenty of micro services\n\n* Plenty of tools to lean\n\n  + Minikube on local machine\n\n  + kubectl\n\n  + Build my App\n\n  + Build my Container\n\n  + Understand the k8s concepts\n\n  + Write my \ndeployment.yml\n\n\n=\n We need a fast and easy wayt to distribute an App: Helm\n\n\n3.6 - Closing of the day", 
            "title": "20170329_kubeCon_1.3"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#3-keynotes-end-of-the-day", 
            "text": "", 
            "title": "3 - Keynotes - End of the day"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#31-what-makes-the-community-happy", 
            "text": "k8s: \n* Since day 1 Open Source \n* Backed by Google engineers \n* Designed to run anywhere \n* Community has the opportunity to drive the project roadmap (SIGs)  Contributions: \n* 1 year ago: 67% was Google \n* Today: Google backed to 42%  Did Google drop k8s ? \n-  Nope \n-  More and more Google s engineers working on it (GKE) \nBUT more and more contributors  OpenStandards: \n* For user benefits \n* Composable systems \n* Open defined interfaces \n  + CNI \n  + OCI \n  + CSI (storage) \n  + OpenServiceBrocker API  Velocity / Stability \n* Deals with SIGs projects integrations \n* Ease contributors onboarding", 
            "title": "3.1 - What makes the community happy"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#32-coreos-tectonic-helm", 
            "text": "Lightning talk / demo about App-registry", 
            "title": "3.2 - CoreOS / Tectonic / Helm"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#33-philips-hue-powered-by-k8s", 
            "text": "", 
            "title": "3.3 - Philips Hue - Powered by k8s"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#34-workload-colocation-on-k8s", 
            "text": "", 
            "title": "3.4 - Workload colocation on k8s"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#35-helm", 
            "text": "OPS Evolutions \n* SSH and do manual things \n* CFG mgnt \n* Containers + k8s  Dev point of views \n* More and more difficult \n* From 1 monolith to plenty of micro services \n* Plenty of tools to lean \n  + Minikube on local machine \n  + kubectl \n  + Build my App \n  + Build my Container \n  + Understand the k8s concepts \n  + Write my  deployment.yml  =  We need a fast and easy wayt to distribute an App: Helm", 
            "title": "3.5 - Helm"
        }, 
        {
            "location": "/2017/kubecon/day1/20170329_kubeCon_1.3/#36-closing-of-the-day", 
            "text": "", 
            "title": "3.6 - Closing of the day"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/", 
            "text": "20170330_kubeCon_1.1\n\n\n\n\n\n\n\n1 - Keynotes\n\n\n1.1 - Why should I care about CNCF\n\n\n1.2 - Cloud Native in the Enterprise\n\n\n1.3 - Scaling k8s user base\n\n\n1.4 -  k8s clusters federations\n\n\nIntro\n\n\nk8s cluster federation\n\n\nDemo\n\n\n\n\n\n\n\n\n\n\n\n1 - Keynotes\n\n\n1.1 - Why should I care about CNCF\n\n\nA home for amazing projects\n\n\nMutualize:\n\n* Efforts\n\n* Best practices\n\n\n\n\nCNCF is for Applications / OpenStack is for Infrastructure\n\n\n\n\nCNCF is about:\n\n* Speed,\n\n* Freedom,\n\n* Trust\n\n\n\n\nNetflix introduced Speed 10 years ago, its now a need for every companies\n\nWhat tools can we trust to achieve that ?\n\nWhy a foundation -\n Freedom\n\n\n\n\n=\n CNCF\n\n\nFreedom is also about choice of tools\n\n* ContainerD / Rkt\n\n\n\n\nOn the contrary to OpenStack where it is all or nothing,\n\nCNCF is tools that can bee used allone but works well together\n\n\n\n\nMore tools to come in CNCF\n\n* etcd ?\n\n* Traefik ?\n\n* \n\n\n1.2 - Cloud Native in the Enterprise\n\n\nBest practices and innovation\n\n\nAdvantages: OK\n\nHow:\n\n* Start with the \nright\n app for containerization\n\n* Strategy for cluster Management / Routing\n\n* Strategy for release and sync of underling tools (Registry global access)\n\n\nAdvices:\n\n* Think about your network\n\n* Simplify hybrid app mgnt\n\n* Simplify storage mgnt\n\n\n1.3 - Scaling k8s user base\n\n\nRefine the user experience\n\n* More significat error messages\n\n* Events access\n\n* \nkubectl logs\n inteface on services / individual pods\n\n* \n\n\nBut sill let space for advanced users\n\n* Newton / Einstein\n\n\n1.4 -  k8s clusters federations\n\n\nIntro\n\n\n1 Node: OK, CNCF does not mean any thing\n\n2 Nodes: Bash for loops is OK\n\n10+ Nodes: There is a story there\n\n\nWe started with CFG MGNT\n\n* A way to group \nnodes\n\n* An abstractions layer but still with 1 node acting as 1 thing\n\n\nThen k8s: 1 cluster = \nNode federation\n\n\n\n\nCfg Mgnt + \nDevOps\n = Group Therapie for ineficient tools\n\n\nKelsey Hightower\n\n\n\n\nk8s cluster federation\n\n\nIdeas:\n\n* Keep 1 k8s cluster / usage / cloud provider\n\n* Provie a \nFederation\n client\n\n-\n Keep it simple\n\n\nWhen we use federation ?\n\n* App that span on multiple clusters\n\n\nWhat kind of objects should be federated ?\n\n* StatefulSet ?\n\n* Jobs ?\n\n* \n\n\nDemo\n\n\nGlobal service and load spread with geoIP", 
            "title": "20170330_kubeCon_1.1"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#1-keynotes", 
            "text": "", 
            "title": "1 - Keynotes"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#11-why-should-i-care-about-cncf", 
            "text": "A home for amazing projects  Mutualize: \n* Efforts \n* Best practices   CNCF is for Applications / OpenStack is for Infrastructure   CNCF is about: \n* Speed, \n* Freedom, \n* Trust   Netflix introduced Speed 10 years ago, its now a need for every companies \nWhat tools can we trust to achieve that ? \nWhy a foundation -  Freedom   =  CNCF  Freedom is also about choice of tools \n* ContainerD / Rkt   On the contrary to OpenStack where it is all or nothing, \nCNCF is tools that can bee used allone but works well together   More tools to come in CNCF \n* etcd ? \n* Traefik ? \n*", 
            "title": "1.1 - Why should I care about CNCF"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#12-cloud-native-in-the-enterprise", 
            "text": "Best practices and innovation  Advantages: OK \nHow: \n* Start with the  right  app for containerization \n* Strategy for cluster Management / Routing \n* Strategy for release and sync of underling tools (Registry global access)  Advices: \n* Think about your network \n* Simplify hybrid app mgnt \n* Simplify storage mgnt", 
            "title": "1.2 - Cloud Native in the Enterprise"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#13-scaling-k8s-user-base", 
            "text": "Refine the user experience \n* More significat error messages \n* Events access \n*  kubectl logs  inteface on services / individual pods \n*   But sill let space for advanced users \n* Newton / Einstein", 
            "title": "1.3 - Scaling k8s user base"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#14-k8s-clusters-federations", 
            "text": "", 
            "title": "1.4 -  k8s clusters federations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#intro", 
            "text": "1 Node: OK, CNCF does not mean any thing \n2 Nodes: Bash for loops is OK \n10+ Nodes: There is a story there  We started with CFG MGNT \n* A way to group  nodes \n* An abstractions layer but still with 1 node acting as 1 thing  Then k8s: 1 cluster =  Node federation   Cfg Mgnt +  DevOps  = Group Therapie for ineficient tools  Kelsey Hightower", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#k8s-cluster-federation", 
            "text": "Ideas: \n* Keep 1 k8s cluster / usage / cloud provider \n* Provie a  Federation  client \n-  Keep it simple  When we use federation ? \n* App that span on multiple clusters  What kind of objects should be federated ? \n* StatefulSet ? \n* Jobs ? \n*", 
            "title": "k8s cluster federation"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_1.1/#demo", 
            "text": "Global service and load spread with geoIP", 
            "title": "Demo"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/", 
            "text": "20170330_kubeCon_2.1\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.1 - CoreDNS\n\n\nIntro\n\n\nWhat is Service discovery ?\n\n\nDNS protocol\n\n\nCoreDNS Demo\n\n\nProject / Issues\n\n\nSkyDNS / CoreDNS\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.1 - CoreDNS\n\n\nIntro\n\n\nWas involved in \nSkyDNS\n\nUsing \netcd\n as a backend\n\n\nSkyDNS was 1 simple function wich did everything\n\n-\n Wanted to rework Skydns\n\n-\n Forked and started the \nCoreDNS\n thing\n\n\nWhat is Service discovery ?\n\n\nDNS protocol has a SRV record\n\n\nSERVICE\n._\nPROTO\n.domain.com. SRV priority weight port target.\n\n\nDNS protocol\n\n\nDNS started as a simple protocol\n\n* (very) old  RFCs\n\n* Lots of RFCs that extended the protocol\n\n* RFCs are not respected by everybody\n\n\nCoreDNS Demo\n\n\nBasics\n\n* Reads ETCD records\n\n* Translate as DNS records\n\n\nSupports GRPC\n\n\nProject / Issues\n\n\n\n\nDig is your only friend\n\n\nDNS is old and not easy to use, debug, explore \n\n\nUDP for 512 Bytes max\n\n\nFallback to TCP\n\n\nBut oftenly blocked by FWs\n\n\nMAX 64k bytes\n\n\nAfter -\n you are screwed\n\n\n\n\nSkyDNS / CoreDNS\n\n\nMutli zones support in CoreDNS", 
            "title": "20170330_kubeCon_2.1"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#21-coredns", 
            "text": "", 
            "title": "2.1 - CoreDNS"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#intro", 
            "text": "Was involved in  SkyDNS \nUsing  etcd  as a backend  SkyDNS was 1 simple function wich did everything \n-  Wanted to rework Skydns \n-  Forked and started the  CoreDNS  thing", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#what-is-service-discovery", 
            "text": "DNS protocol has a SRV record  SERVICE ._ PROTO .domain.com. SRV priority weight port target.", 
            "title": "What is Service discovery ?"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#dns-protocol", 
            "text": "DNS started as a simple protocol \n* (very) old  RFCs \n* Lots of RFCs that extended the protocol \n* RFCs are not respected by everybody", 
            "title": "DNS protocol"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#coredns-demo", 
            "text": "Basics \n* Reads ETCD records \n* Translate as DNS records  Supports GRPC", 
            "title": "CoreDNS Demo"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#project-issues", 
            "text": "Dig is your only friend  DNS is old and not easy to use, debug, explore   UDP for 512 Bytes max  Fallback to TCP  But oftenly blocked by FWs  MAX 64k bytes  After -  you are screwed", 
            "title": "Project / Issues"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.1/#skydns-coredns", 
            "text": "Mutli zones support in CoreDNS", 
            "title": "SkyDNS / CoreDNS"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/", 
            "text": "20170330_kubeCon_2.2\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.2 - Kubernetes Day 2: Cluster Operations\n\n\n1 - Architecture\n\n\n2 - Failure in HA\n\n\n3 - Monitoring\n\n\n4 - Backup and Recovery\n\n\n5 - Upgrade\n\n\n6 - Management nodes\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.2 - Kubernetes Day 2: Cluster Operations\n\n\nOutline\n\n1. Cluster Architecture\n\n2. Failure in HA\n\n3. Monitorning + Altering\n\n4. Backup + DR\n\n5. Upgrades\n\n6. Scaling and Management\n\n\nhttps://github.com/philips/kubernetes-day-2\n\n\n1 - Architecture\n\n\nToday there are \nplenty tools to deploy k8s\n\n* Tectonic installer\n\n* kargo\n\n* kops\n\n* terraform\n\n\n2 - Failure in HA\n\n\nMaster nodes HA\n\n\nControl plane\n is a classic web app\n\n* API Server\n\n* Controller\n\n* \n\n-\n kube in kube ?\n\n\nEtcd\n\n* is kind of \nspecial\n\n* is a DB, statefull, on disk state is uniq\n\n* clustered\n\n\n=\n \netcd-operator\n\n* Observe\n\n* Decide\n\n* Act\n\n\n\n\nEtcd runs as a Pod\n\nScheduled on Masters thanks to labels and afinity\n\nk8s has been setup to allow scheduling of some services on the masters\n\n\n\n\nAPI server\n\n* Run N API servers (Same that ectd servers by example)\n\n* Macth ECTD / API server failure domains\n\n* Maintain a load balancer\n\n\n\n\nRisk if API downtime:\n\nRO: No new workloads\n\nDown: Visibility\n\n\n\n\nScheduler and Controller\n\n* Statefull but backed by API server\n\n* Leader elected -\n you can run nore than 1\n\n* Use anti-afinity to avoid 2 instances on the same node\n\n\n\n\nIn case of scheduler failure\n\n-\n You have to re-launch manually 1 scheduler Pod\n\n\n\n\nDNS\n\n* \n\n\nInteresing concepts\n\n\n\n\nLabels\n\n\nFailure domains\n\n\nAntiAfinity\n\n\n-o jsonpath\n query\n\n\n\n\n3 - Monitoring\n\n\nThanks to prometheus\n\n* Nodes\n\n* ControlPlane: etcd key metrics\n\n* \n\n\n4 - Backup and Recovery\n\n\n-\n See github nodes\n\n\n5 - Upgrade\n\n\nSelf hosted architecture shine for that purpose\n\n\n6 - Management nodes\n\n\nUse standard tools like terraform or", 
            "title": "20170330_kubeCon_2.2"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#22-kubernetes-day-2-cluster-operations", 
            "text": "Outline \n1. Cluster Architecture \n2. Failure in HA \n3. Monitorning + Altering \n4. Backup + DR \n5. Upgrades \n6. Scaling and Management  https://github.com/philips/kubernetes-day-2", 
            "title": "2.2 - Kubernetes Day 2: Cluster Operations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#1-architecture", 
            "text": "Today there are  plenty tools to deploy k8s \n* Tectonic installer \n* kargo \n* kops \n* terraform", 
            "title": "1 - Architecture"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#2-failure-in-ha", 
            "text": "", 
            "title": "2 - Failure in HA"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#master-nodes-ha", 
            "text": "Control plane  is a classic web app \n* API Server \n* Controller \n*  \n-  kube in kube ?  Etcd \n* is kind of  special \n* is a DB, statefull, on disk state is uniq \n* clustered  =   etcd-operator \n* Observe \n* Decide \n* Act   Etcd runs as a Pod \nScheduled on Masters thanks to labels and afinity \nk8s has been setup to allow scheduling of some services on the masters   API server \n* Run N API servers (Same that ectd servers by example) \n* Macth ECTD / API server failure domains \n* Maintain a load balancer   Risk if API downtime: \nRO: No new workloads \nDown: Visibility   Scheduler and Controller \n* Statefull but backed by API server \n* Leader elected -  you can run nore than 1 \n* Use anti-afinity to avoid 2 instances on the same node   In case of scheduler failure \n-  You have to re-launch manually 1 scheduler Pod   DNS \n*", 
            "title": "Master nodes HA"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#interesing-concepts", 
            "text": "Labels  Failure domains  AntiAfinity  -o jsonpath  query", 
            "title": "Interesing concepts"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#3-monitoring", 
            "text": "Thanks to prometheus \n* Nodes \n* ControlPlane: etcd key metrics \n*", 
            "title": "3 - Monitoring"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#4-backup-and-recovery", 
            "text": "-  See github nodes", 
            "title": "4 - Backup and Recovery"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#5-upgrade", 
            "text": "Self hosted architecture shine for that purpose", 
            "title": "5 - Upgrade"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.2/#6-management-nodes", 
            "text": "Use standard tools like terraform or", 
            "title": "6 - Management nodes"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/", 
            "text": "20170330_kubeCon_2.3\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.3 - High Availability Kubernetes on Bare Metal\n\n\nAPI server HA\n\n\nSolutions ?\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.3 - High Availability Kubernetes on Bare Metal\n\n\nBare-metal = Big questions\n\n\nAPI server HA\n\n\n\n\nkube-proxy\n takes only 1 API server argument, which one to use ?\n\n\nHard Coded value,  how, when update ?\n\n\nIf \nlb\n, how make it HA ?\n\n\n\n\nSolutions ?\n\n\nDNS ?\n\n* TTL \n Cache issue\n\n* DNS become SPOF\n\n-\n NO way\n\n\nCoroSync\n \n \npacemaker\n\n* VIP on master nodes for Controllers \n API Servers\n\n* VIP on etcd nodes\n\n\n\n\nThis setup does not affect k8s behavior\n\n\nif VIP lands on \nstand by\n Controller, which has a leader election,\n\n-\n The request is forwarded to the leader\n\n\nif VIP lands on \nleader\n,\n\n-\n Cool, no hop\n\n\n\n\nNotes\n\n\nCoroSync needs \nBroadCast\n and \nMulticast\n\n-\n You need \nmore than 1 network link", 
            "title": "20170330_kubeCon_2.3"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/#23-high-availability-kubernetes-on-bare-metal", 
            "text": "Bare-metal = Big questions", 
            "title": "2.3 - High Availability Kubernetes on Bare Metal"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/#api-server-ha", 
            "text": "kube-proxy  takes only 1 API server argument, which one to use ?  Hard Coded value,  how, when update ?  If  lb , how make it HA ?", 
            "title": "API server HA"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/#solutions", 
            "text": "DNS ? \n* TTL   Cache issue \n* DNS become SPOF \n-  NO way  CoroSync     pacemaker \n* VIP on master nodes for Controllers   API Servers \n* VIP on etcd nodes   This setup does not affect k8s behavior  if VIP lands on  stand by  Controller, which has a leader election, \n-  The request is forwarded to the leader  if VIP lands on  leader , \n-  Cool, no hop", 
            "title": "Solutions ?"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.3/#notes", 
            "text": "CoroSync needs  BroadCast  and  Multicast \n-  You need  more than 1 network link", 
            "title": "Notes"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/", 
            "text": "20170330_kubeCon_2.4\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.4 - Postgres in k8s\n\n\nConcepts\n\n\nSolutions\n\n\nHelm !\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.4 - Postgres in k8s\n\n\nConcepts\n\n\nCentralized controllers / SideCar container in the Pod\n\n\n\n\nCentralized controllers: Operator pattern\n\n\nSideCar container: The patroni exemple below\n\n\n\n\nSolutions\n\n\nhttps://github.com/zalando/patroni\n\n\n\n\nHA solution for Postgres in container environment\n\n\nThere is a Helm chart that uses Patroni and Spilo to provision a five-node PostgreSQL HA cluster in a Kubernetes\n\n\n\n\nhttps://github.com/zalando/spilo\n\n\nhttps://tech.zalando.com/blog/zalandos-patroni-a-template-for-high-availability-postgresql/\n\n\nhttps://tech.zalando.com/blog/zalandos-patroni-a-template-for-high-availability-postgresql/\n\n\nHelm !\n\n\nHelm is a great solutions that allows:\n\nParameters in K8s manifests", 
            "title": "20170330_kubeCon_2.4"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/#24-postgres-in-k8s", 
            "text": "", 
            "title": "2.4 - Postgres in k8s"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/#concepts", 
            "text": "Centralized controllers / SideCar container in the Pod   Centralized controllers: Operator pattern  SideCar container: The patroni exemple below", 
            "title": "Concepts"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/#solutions", 
            "text": "https://github.com/zalando/patroni   HA solution for Postgres in container environment  There is a Helm chart that uses Patroni and Spilo to provision a five-node PostgreSQL HA cluster in a Kubernetes   https://github.com/zalando/spilo  https://tech.zalando.com/blog/zalandos-patroni-a-template-for-high-availability-postgresql/  https://tech.zalando.com/blog/zalandos-patroni-a-template-for-high-availability-postgresql/", 
            "title": "Solutions"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.4/#helm", 
            "text": "Helm is a great solutions that allows: \nParameters in K8s manifests", 
            "title": "Helm !"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.5/", 
            "text": "20170330_kubeCon_2.5\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.5 - Writing a custom controller\n\n\nControllers\n\n\nExamples: \nNode Reboot operator\n\n\n\n\n\n\n\n\n\n\n\n2 - Presentations\n\n\n2.5 - Writing a custom controller\n\n\nControllers\n\n\nWhat is a k8s Controller ?\n\n* A \nreconsiliation\n program\n\n* Looping to ensure a state\n\n\nk8s internal controllers\n\n* Working in \nk8s-controller-manager\n\n\nTiers Controllers\n\n* \nkube dns\n\n* \noperatos*\n\n* \n\n\nExamples: \nNode Reboot operator\n\n\nComponents:\n\n* Reboot agent (On all nodes)\n\n* Reboot controller\n\n-\n Schedule reboot via kubectl reboot ....\n\n\nReboot agent\n\n* Daemonset\n\n* Watch node annotations on k8s API: \nreboot-needed\n\n* Counts \nunavailable nodes\n on k8s API\n\n\nController\n\n* \nhttps://github.com/aaronlevy.kube-controller-demo", 
            "title": "20170330_kubeCon_2.5"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.5/#2-presentations", 
            "text": "", 
            "title": "2 - Presentations"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.5/#25-writing-a-custom-controller", 
            "text": "", 
            "title": "2.5 - Writing a custom controller"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.5/#controllers", 
            "text": "What is a k8s Controller ? \n* A  reconsiliation  program \n* Looping to ensure a state  k8s internal controllers \n* Working in  k8s-controller-manager  Tiers Controllers \n*  kube dns \n*  operatos* \n*", 
            "title": "Controllers"
        }, 
        {
            "location": "/2017/kubecon/day2/20170330_kubeCon_2.5/#examples-node-reboot-operator", 
            "text": "Components: \n* Reboot agent (On all nodes) \n* Reboot controller \n-  Schedule reboot via kubectl reboot ....  Reboot agent \n* Daemonset \n* Watch node annotations on k8s API:  reboot-needed \n* Counts  unavailable nodes  on k8s API  Controller \n*  https://github.com/aaronlevy.kube-controller-demo", 
            "title": "Examples: \"Node Reboot operator\""
        }, 
        {
            "location": "/2017/srecon/README/", 
            "text": "SRE Con EMA 17\n\n\n\n\n\n\nhttps://www.usenix.org/conference/srecon17europe\n\n\nhttps://www.usenix.org/conference/srecon17europe/program\n\n\n\n\nSession I have attended:\n\n* \nhttps://srecon17emea.sched.com/xakraz", 
            "title": "SRE Con EMA 17"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/", 
            "text": "20170830_srecon17_1\n\n\n\n\n\n\n\n1 - Google - Care and Feeding SRE\n\n\n1.1 - Goals / Activity\n\n\n1.2 - Changes and gain for the Company\n\n\n1.3 - Approach\n\n\nCcls\n\n\n2 - Postmortems / Perimortem\n\n\n\n\n\n\n\n1 - Google - Care and Feeding SRE\n\n\n1.1 - Goals / Activity\n\n\nReliability:\n\n* Fire\n\n* Proactive work\n\n\n\n\nif you spend more than 50% of your time \nfixing\n things,\n\n-\n your service is behaing wrongly\n\n\n\n\n\n\nFireman Ops:            Reaction\n\nProactive Engineering:  Planification\n\n\n1.2 - Changes and gain for the Company\n\n\nReliability:\n\n* Balance between 99,999 / Velocity of your product changes\n\n\n1.3 - Approach\n\n\nSLA, SLO, SLI\n\n\nBest intentions assomptions\n\n* Failures will happen, nomatter what we do\n\n* Make people confident, people are doing the best they can\n\n* Nobody can be an expert about everything, always\n\n\nCcls\n\n\nSRE = Org architecture\n\n  - Behavior and collaboration between teams\n\n\nThink about Values\n\n\n2 - Postmortems / Perimortem\n\n\n\n\nDiversity and inclusion\n\n\n\n\n\n\nAcceptance\n\n* Denial\n\n* Ange\n\n* Bargaining\n\n* Depression\n\n* Acceptance \n-----\n\n\nSocial \n cultural context\n\n\nActions\n\n* Start training and share knowledge\n\n* Sponsorship / Mentoring program\n\n* Create an \ninclusive\n culture\n\n\nCCL:\n\n* SRE is all about discipline\n\n* Record, Track, adapt and learn", 
            "title": "20170830_srecon17_1"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#1-google-care-and-feeding-sre", 
            "text": "", 
            "title": "1 - Google - Care and Feeding SRE"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#11-goals-activity", 
            "text": "Reliability: \n* Fire \n* Proactive work   if you spend more than 50% of your time  fixing  things, \n-  your service is behaing wrongly    Fireman Ops:            Reaction \nProactive Engineering:  Planification", 
            "title": "1.1 - Goals / Activity"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#12-changes-and-gain-for-the-company", 
            "text": "Reliability: \n* Balance between 99,999 / Velocity of your product changes", 
            "title": "1.2 - Changes and gain for the Company"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#13-approach", 
            "text": "SLA, SLO, SLI  Best intentions assomptions \n* Failures will happen, nomatter what we do \n* Make people confident, people are doing the best they can \n* Nobody can be an expert about everything, always", 
            "title": "1.3 - Approach"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#ccls", 
            "text": "SRE = Org architecture \n  - Behavior and collaboration between teams  Think about Values", 
            "title": "Ccls"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_1/#2-postmortems-perimortem", 
            "text": "Diversity and inclusion    Acceptance \n* Denial \n* Ange \n* Bargaining \n* Depression \n* Acceptance  -----  Social   cultural context  Actions \n* Start training and share knowledge \n* Sponsorship / Mentoring program \n* Create an  inclusive  culture  CCL: \n* SRE is all about discipline \n* Record, Track, adapt and learn", 
            "title": "2 - Postmortems / Perimortem"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/", 
            "text": "20170830_srecon17_2.1\n\n\n\n\n\n\n\nLoad traffic shedding\n\n\n1 - Timeline\n\n\n2 - Threats\n\n\n3 - Principles\n\n\nSustain peak performance\n\n\nIsolation\n\n\nCriticality\n\n\nCost based\n\n\nRetries\n\n\n\n\n\n\n4 - Techniques\n\n\n5 - Solutions\n\n\n6 - Impacts on Operations\n\n\nBatch handling\n\n\n\n\n\n\nEnaabling application-level and business decisions\n\n\n\n\n\n\n\nLoad traffic shedding\n\n\n1 - Timeline\n\n\n\n\n2009 - 3 Main Gmail outages\n\n\n2010 - 1st Load sheding implementation\n\n\n2013 - Generaliszation\n\n\n\n\n2 - Threats\n\n\n\n\nSingle Task overload\n\n\n\n\nCausing cascading failure\n\n\n\n\n\n\nCluster overload\n\n\n\n\n\n\nWhole Service overload\n\n\n\n\n\n\n3 - Principles\n\n\nSustain peak performance\n\n\n\n\nAccept you will throw erros\n\n\n\n\n\n\nTask should have a \nnormal\n capacity, what ever the traffic it receives\n\n*\n\n\n\n\nIsolation\n\n\nCriticality\n\n\n\n\nQOS of requests (Not all requests are born equals)\n\n\nBatch, Async, Sheddable, Critical\n\n\n\n\nCost based\n\n\nQPS is not enought to measure cost\n\n-\n transition to \nCost-per-second\n\n\n\n\nExample\n\n\nSame query != Cost\n\n\n\n\n1 user with 1 friend -\n 1 Q = 0.1\n\n\n1 user with 10 0000 friends -\n Q is much eavier\n\n\n\n\n\n\n\n\nRetries\n\n\n\n\nAlways implement retries logici\n\n\n\n\nWarning: poor retry logic\n\n\n4 - Techniques\n\n\nClient-side\n\n* Efficient BUT\n\n* What if not every client implement the stuff, you still get overloaded\n\n\nServer-side\n\n* The most efficient to mitigate load\n\n\nNotes:\n\n- Event loop latency monitor (\nTwisted\n in python)\n\n- Resources utilizations\n\n- Rate limiting \n\n\n-\n you can then \ncompose\n task load tracker with all these metrics\n\n\n5 - Solutions\n\n\nHaproxy\n\n- Max-in-flight-requests\n\n- Health-checking\n\n- Circuit-breaker\n\n\n\n\nOthers L4 proxies\n\n\n\n\n\n\nNeed some tunning,\n\n\n\n\nSoftware:\n\n* Akka (Connection wrappers)\n\n\nhttps://github.com/juju/ratelimit\n\n- Token based implementation\n\n- Client / Server side\n\n- Go based\n\n\nhttps://lyft.github.io/envoy\n\n- \nLet me wrapp it for you\n category\n\n- HTTP/2 + GRPC interface\n\n- Proxy things\n\n- Good for \nlegacy\n transition\n\n\nhttps://linked.io\n\n- Same principle that Envoy\n\n- Can be deployed as an \nagent\n\n\n6 - Impacts on Operations\n\n\nBatch handling\n\n\nEnaabling application-level and business decisions\n\n\nProvision below peak traffic \n drop non-priority traffic\n\n\n-\n Allows to control \nproblems\n and not be in an emergency situation", 
            "title": "20170830_srecon17_2.1"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#load-traffic-shedding", 
            "text": "", 
            "title": "Load traffic shedding"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#1-timeline", 
            "text": "2009 - 3 Main Gmail outages  2010 - 1st Load sheding implementation  2013 - Generaliszation", 
            "title": "1 - Timeline"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#2-threats", 
            "text": "Single Task overload   Causing cascading failure    Cluster overload    Whole Service overload", 
            "title": "2 - Threats"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#3-principles", 
            "text": "", 
            "title": "3 - Principles"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#sustain-peak-performance", 
            "text": "Accept you will throw erros    Task should have a  normal  capacity, what ever the traffic it receives \n*", 
            "title": "Sustain peak performance"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#isolation", 
            "text": "", 
            "title": "Isolation"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#criticality", 
            "text": "QOS of requests (Not all requests are born equals)  Batch, Async, Sheddable, Critical", 
            "title": "Criticality"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#cost-based", 
            "text": "QPS is not enought to measure cost \n-  transition to  Cost-per-second", 
            "title": "Cost based"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#example", 
            "text": "Same query != Cost   1 user with 1 friend -  1 Q = 0.1  1 user with 10 0000 friends -  Q is much eavier", 
            "title": "Example"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#retries", 
            "text": "Always implement retries logici   Warning: poor retry logic", 
            "title": "Retries"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#4-techniques", 
            "text": "Client-side \n* Efficient BUT \n* What if not every client implement the stuff, you still get overloaded  Server-side \n* The most efficient to mitigate load  Notes: \n- Event loop latency monitor ( Twisted  in python) \n- Resources utilizations \n- Rate limiting   -  you can then  compose  task load tracker with all these metrics", 
            "title": "4 - Techniques"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#5-solutions", 
            "text": "Haproxy \n- Max-in-flight-requests \n- Health-checking \n- Circuit-breaker   Others L4 proxies    Need some tunning,   Software: \n* Akka (Connection wrappers)  https://github.com/juju/ratelimit \n- Token based implementation \n- Client / Server side \n- Go based  https://lyft.github.io/envoy \n-  Let me wrapp it for you  category \n- HTTP/2 + GRPC interface \n- Proxy things \n- Good for  legacy  transition  https://linked.io \n- Same principle that Envoy \n- Can be deployed as an  agent", 
            "title": "5 - Solutions"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#6-impacts-on-operations", 
            "text": "", 
            "title": "6 - Impacts on Operations"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#batch-handling", 
            "text": "", 
            "title": "Batch handling"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.1/#enaabling-application-level-and-business-decisions", 
            "text": "Provision below peak traffic   drop non-priority traffic  -  Allows to control  problems  and not be in an emergency situation", 
            "title": "Enaabling application-level and business decisions"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.2/", 
            "text": "20170830_srecon17_2.2\n\n\n\n\n\n\n\nMonitoring alert fatigue\n\n\n\n\n\n\n\nMonitoring alert fatigue", 
            "title": "20170830_srecon17_2.2"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.2/#monitoring-alert-fatigue", 
            "text": "", 
            "title": "Monitoring alert fatigue"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/", 
            "text": "20170830_srecon17_2.3\n\n\n\n\n\n\n\nScriptable LoadBalancer\n\n\n1 - Middleware LB\n\n\nOverview\n\n\nScriptable LBs\n\n\nWhy Scriptable LBs\n\n\n\n\n\n\n2 - Edge cache servers\n\n\n3 - Getting Clever\n\n\n4 - Tips / take away\n\n\n\n\n\n\n\nScriptable LoadBalancer\n\n\n1 - Middleware LB\n\n\nOverview\n\n\n\n\nClassic LBs\n\n\nCustom LBs (Fb, Google, Fastly, Cloudlfare)\n\n\n\n\n-\n In between: \nScriptable loadbalancer\n\n\nScriptable LBs\n\n\n\n\nnginScript: Nginx.com properitary lanagage\n\n\nOpenResty (Nginx + Lua)\n\n\nHaproxy + lua (since recently)\n\n\n\n\nWhy Scriptable LBs\n\n\nBegining\n\n* WebServer\n\n* AppServer\n\n* DBServer\n\n\nThen:\n\n* DB shards\n\n\nThen:\n\n* MultiDCs\n\n\n-\n Issue: WAN access between AppServer -\n DB Shard in an other DC \n\n\n=\n Went with Nginx +lua\n\n\n\n\nImplemented dynamic config with ZK\n\n\n\n\n2 - Edge cache servers\n\n\n3 - Getting Clever\n\n\nUsing \nECMP\n\n\n4 - Tips / take away\n\n\nDeterministric config\n\n- Remove Config from the cfg Mgmt tool to make the conf closer the App that needs it, and match the App lifecycle\n\n\nTesting\n\n- Using all the tooling and automation existing in Dev helps a lot\n\n\nStating / Production Deployment\n\n- Treat it like an other App\n\n- Multiple environment\n\n- As close as they can be", 
            "title": "20170830_srecon17_2.3"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#scriptable-loadbalancer", 
            "text": "", 
            "title": "Scriptable LoadBalancer"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#1-middleware-lb", 
            "text": "", 
            "title": "1 - Middleware LB"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#overview", 
            "text": "Classic LBs  Custom LBs (Fb, Google, Fastly, Cloudlfare)   -  In between:  Scriptable loadbalancer", 
            "title": "Overview"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#scriptable-lbs", 
            "text": "nginScript: Nginx.com properitary lanagage  OpenResty (Nginx + Lua)  Haproxy + lua (since recently)", 
            "title": "Scriptable LBs"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#why-scriptable-lbs", 
            "text": "Begining \n* WebServer \n* AppServer \n* DBServer  Then: \n* DB shards  Then: \n* MultiDCs  -  Issue: WAN access between AppServer -  DB Shard in an other DC   =  Went with Nginx +lua   Implemented dynamic config with ZK", 
            "title": "Why Scriptable LBs"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#2-edge-cache-servers", 
            "text": "", 
            "title": "2 - Edge cache servers"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#3-getting-clever", 
            "text": "Using  ECMP", 
            "title": "3 - Getting Clever"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.3/#4-tips-take-away", 
            "text": "Deterministric config \n- Remove Config from the cfg Mgmt tool to make the conf closer the App that needs it, and match the App lifecycle  Testing \n- Using all the tooling and automation existing in Dev helps a lot  Stating / Production Deployment \n- Treat it like an other App \n- Multiple environment \n- As close as they can be", 
            "title": "4 - Tips / take away"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/", 
            "text": "20170830_srecon17_2.4\n\n\n\n\n\n\n\nAnycast is not loadbalancing\n\n\n1 - Start the story\n\n\n2 - What is AnyCast ?\n\n\n3 - Catch ?\n\n\n4 - Don\nt believe in Magic\n\n\n\n\n\n\n\nAnycast is not loadbalancing\n\n\n1 - Start the story\n\n\nHow do you scale ?\n\n* Start with 1 Server\n\n* More servers\n\n* Tell clients about new servers ?\n\n\n2 - What is AnyCast ?\n\n\nWhat\n\n- Network magic\n\n- Configure the same IP on multiple devices\n\n- Let the \nnetwrk\n decide wich server receive the request\n\n\nPros:\n\n- Simple client config\n\n- Simple horizontal scaling\n\n- Low dependency\n\n\nGood for:\n\n- Staless app\n\n- \nSimple\n HA\n\n\n3 - Catch ?\n\n\nNOT lbs\n\n- no control where is the request going\n\n- no way to \nstop accepting new request\n BUT \nkeep serving existing\n\n- no traffic shapping\n\n\nMonitoring ?\n\n- Probe are basics\n\n- Not easy to know from wich server the issue comes from\n\n- May fall in a case were no backend is the closest for the Anycast probe \n\n\nFailure modes\n\n- By default, no check of pool member ships\n\n- No nothion on Pool\n\n\n4 - Don\nt believe in Magic\n\n\nAnycast is usefull\n\n\nBut\n\n- it is load DISTRIBUTION", 
            "title": "20170830_srecon17_2.4"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/#anycast-is-not-loadbalancing", 
            "text": "", 
            "title": "Anycast is not loadbalancing"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/#1-start-the-story", 
            "text": "How do you scale ? \n* Start with 1 Server \n* More servers \n* Tell clients about new servers ?", 
            "title": "1 - Start the story"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/#2-what-is-anycast", 
            "text": "What \n- Network magic \n- Configure the same IP on multiple devices \n- Let the  netwrk  decide wich server receive the request  Pros: \n- Simple client config \n- Simple horizontal scaling \n- Low dependency  Good for: \n- Staless app \n-  Simple  HA", 
            "title": "2 - What is AnyCast ?"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/#3-catch", 
            "text": "NOT lbs \n- no control where is the request going \n- no way to  stop accepting new request  BUT  keep serving existing \n- no traffic shapping  Monitoring ? \n- Probe are basics \n- Not easy to know from wich server the issue comes from \n- May fall in a case were no backend is the closest for the Anycast probe   Failure modes \n- By default, no check of pool member ships \n- No nothion on Pool", 
            "title": "3 - Catch ?"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.4/#4-dont-believe-in-magic", 
            "text": "Anycast is usefull  But \n- it is load DISTRIBUTION", 
            "title": "4 - Don't believe in Magic"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.5/", 
            "text": "20170830_srecon17_2.5\n\n\n\n\n\n\n\nLoad testing\n\n\n\n\n\n\n\nLoad testing\n\n\nAbstratction\n\n* can take complex systems with simpler concepts", 
            "title": "20170830_srecon17_2.5"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.5/#load-testing", 
            "text": "Abstratction \n* can take complex systems with simpler concepts", 
            "title": "Load testing"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/", 
            "text": "20170830_srecon17_2.6\n\n\n\n\n\n\n\nRumDNS @LinkedIN\n\n\n1 - Performances\n\n\n2 - Components\n\n\nCDN\n\n\nPOPs\n\n\nDNS\n\n\n\n\n\n\n3 - Tools we use\n\n\n\n\n\n\n\nRumDNS @LinkedIN\n\n\n1 - Performances\n\n\nPerformance is everything\n\n\n\n\nIf the loading time \n 10s, user go away\n\n\n\n\n\n\n2 - Components\n\n\nCDN\n\n\nGlobal\n\n* Akamai\n\n* LinkedIN homemade\n\n* Edgecast\n\n\nMutliple CDNs ?\n\n* Performance reasons\n\n* Redundoncy\n\n*\n\n\nPOPs\n\n\nPoint Of Presence\n\n-\n Reduce the hops between User and Servers\n\n\nPops :\n\n* Great Perfomances gains (900ms !!!)\n\n* Cheep\n\n-\n We built a lots of Pops around the world\n\n\nDNS\n\n\nMulti DNS ?\n\n\nBenefits\n\n* Perfs\n\n* Availability\n\n\nBUT\n\n* Needs a lot of automation\n\n\nGeoDNS\n\n* EDNS protocol\n\n* Allow traffic Steering\n\n\n-\n Rum Steering\n\n\n3 - Tools we use\n\n\nDNS records webUI\n\n\nCDN Regression tool\n\n- SSL Certs\n\n- Caching headers\n\n- Compression\n\n- Origin cache headers validation\n\n\nPurgeTool\n\n- Selfservice portal for every CDNs\n\n- Purge status tracking\n\n\nUse Catchpoint service\n\n- Selenium scripts", 
            "title": "20170830_srecon17_2.6"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#rumdns-linkedin", 
            "text": "", 
            "title": "RumDNS @LinkedIN"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#1-performances", 
            "text": "Performance is everything   If the loading time   10s, user go away", 
            "title": "1 - Performances"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#2-components", 
            "text": "", 
            "title": "2 - Components"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#cdn", 
            "text": "Global \n* Akamai \n* LinkedIN homemade \n* Edgecast  Mutliple CDNs ? \n* Performance reasons \n* Redundoncy \n*", 
            "title": "CDN"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#pops", 
            "text": "Point Of Presence \n-  Reduce the hops between User and Servers  Pops : \n* Great Perfomances gains (900ms !!!) \n* Cheep \n-  We built a lots of Pops around the world", 
            "title": "POPs"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#dns", 
            "text": "Multi DNS ?  Benefits \n* Perfs \n* Availability  BUT \n* Needs a lot of automation  GeoDNS \n* EDNS protocol \n* Allow traffic Steering  -  Rum Steering", 
            "title": "DNS"
        }, 
        {
            "location": "/2017/srecon/day1/20170830_srecon17_2.6/#3-tools-we-use", 
            "text": "DNS records webUI  CDN Regression tool \n- SSL Certs \n- Caching headers \n- Compression \n- Origin cache headers validation  PurgeTool \n- Selfservice portal for every CDNs \n- Purge status tracking  Use Catchpoint service \n- Selenium scripts", 
            "title": "3 - Tools we use"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/", 
            "text": "20170831_srecon17_1.1\n\n\n\n\n\n\n\nDeployment in age of MicroServices\n\n\n0 - Agenda\n\n\n1 - Type of changes\n\n\n2 - Why change ?\n\n\n3 - What can go wrong ?\n\n\n4 - Best practices\n\n\n5 - Naming\n\n\n\n\n\n\n\nDeployment in age of MicroServices\n\n\n0 - Agenda\n\n\n\n\nType of changes\n\n\nWhy changes\n\n\nWhat can go wrong\n\n\nBest practices\n\n\nNaming\n\n\n\n\n1 - Type of changes\n\n\nvarious kind of changes\n\n* Client\n\n* Server\n\n* Static config\n\n* Dynamic config\n\n\nAnd various time to recovery\n\n* Minutes,\n\n* Hours,\n\n* Days,\n\n* \n\n\n2 - Why change ?\n\n\nReliabilty:\n\n\n\n\nSafest thing to do is NOT CHANGE ANY THING !\n\n\n\n\n\n\nOn the contrary:\n\n\n\n\nA boat in an harbor is safe,\n\nBut that\ns not what boat are built for\n\n\n\n\n\n\n-\n Not rewarding to not change anything\n\n-\n No new feature, no business development\n\n-\n No optiomizations\n\n-\n No evolutions\n\n....\n\n\n3 - What can go wrong ?\n\n\nLots of business relativ issues:\n\n* Direct revenu loss\n\n* User-confidence loss\n\n* Pay-users complains\n\n* \n\n\n4 - Best practices\n\n\nThe optimal rollout is:\n\n* Staged\n\n* Progressive with metrics monitoring\n\n* Revertable\n\n* Transparent (No 1 man knowledge, no truk man, \n)\n\n* Automtic\n\n* Well understood by everyone\n\n\nDetect badness as early as possible:\n\n* High quality tests\n\n* Build often\n\n* A/B Testing\n\n* Automate to reduce human errors\n\n* \nHold back\n, make changes funel complete (locked from changes till the end of the release)\n\n* Deploy often\n\n* Make it easy to rollback\n\n* Backward compatibility\n\n\n5 - Naming\n\n\nConsistant naming scheme:\n\n\nEverybody needs to have the same word for the same thing\n\n* Stages\n\n* Actions\n\n* App name\n\n\nExamples\n\n\nServer name\nbinary release\nMachines\nload-balancing\n...", 
            "title": "20170831_srecon17_1.1"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#deployment-in-age-of-microservices", 
            "text": "", 
            "title": "Deployment in age of MicroServices"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#0-agenda", 
            "text": "Type of changes  Why changes  What can go wrong  Best practices  Naming", 
            "title": "0 - Agenda"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#1-type-of-changes", 
            "text": "various kind of changes \n* Client \n* Server \n* Static config \n* Dynamic config  And various time to recovery \n* Minutes, \n* Hours, \n* Days, \n*", 
            "title": "1 - Type of changes"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#2-why-change", 
            "text": "Reliabilty:   Safest thing to do is NOT CHANGE ANY THING !    On the contrary:   A boat in an harbor is safe, \nBut that s not what boat are built for    -  Not rewarding to not change anything \n-  No new feature, no business development \n-  No optiomizations \n-  No evolutions \n....", 
            "title": "2 - Why change ?"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#3-what-can-go-wrong", 
            "text": "Lots of business relativ issues: \n* Direct revenu loss \n* User-confidence loss \n* Pay-users complains \n*", 
            "title": "3 - What can go wrong ?"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#4-best-practices", 
            "text": "The optimal rollout is: \n* Staged \n* Progressive with metrics monitoring \n* Revertable \n* Transparent (No 1 man knowledge, no truk man,  ) \n* Automtic \n* Well understood by everyone  Detect badness as early as possible: \n* High quality tests \n* Build often \n* A/B Testing \n* Automate to reduce human errors \n*  Hold back , make changes funel complete (locked from changes till the end of the release) \n* Deploy often \n* Make it easy to rollback \n* Backward compatibility", 
            "title": "4 - Best practices"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.1/#5-naming", 
            "text": "Consistant naming scheme:  Everybody needs to have the same word for the same thing \n* Stages \n* Actions \n* App name  Examples  Server name\nbinary release\nMachines\nload-balancing\n...", 
            "title": "5 - Naming"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.2/", 
            "text": "20170831_srecon17_1.2\n\n\n\n\n\n\n\nResiliency testing\n\n\nOverview\n\n\nGamedays\n\n\nToxiproxy\n\n\n\n\n\n\n\nResiliency testing\n\n\nOverview\n\n\nReproduce \nincident\n state\n\n\nBUT\n\n- Controlled impacts\n\n- Highly automated\n\n- Accessibility\n\n\n\n\nResiliency is a product concern\n\n\n\n\n\n\nTools\n\n* Incident (not really a tool)\n\n* Gamedays (Test hypothesis in real)\n\n* Chaos Engineering (Explore edgecases)\n\n\nGamedays\n\n\nResiliency Matrix\n\n* Tech component / User feature impact\n\n\nToxiproxy\n\n\nOverview\n\n* Developped by Shopify\n\n* In between the App and DB (By example)\n\n* Configure the proxy to inject various kind of failures\n\n\n-\n Use your Resiliency matrix and implement tests :)", 
            "title": "20170831_srecon17_1.2"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.2/#resiliency-testing", 
            "text": "", 
            "title": "Resiliency testing"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.2/#overview", 
            "text": "Reproduce  incident  state  BUT \n- Controlled impacts \n- Highly automated \n- Accessibility   Resiliency is a product concern    Tools \n* Incident (not really a tool) \n* Gamedays (Test hypothesis in real) \n* Chaos Engineering (Explore edgecases)", 
            "title": "Overview"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.2/#gamedays", 
            "text": "Resiliency Matrix \n* Tech component / User feature impact", 
            "title": "Gamedays"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.2/#toxiproxy", 
            "text": "Overview \n* Developped by Shopify \n* In between the App and DB (By example) \n* Configure the proxy to inject various kind of failures  -  Use your Resiliency matrix and implement tests :)", 
            "title": "Toxiproxy"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/", 
            "text": "20170831_srecon17_1.3\n\n\n\n\n\n\n\nOne ring to rules them all\n\n\n1 - Origin\n\n\n2 - How to converge to a \nstandard\n solution ?\n\n\n3 - Tech bits\n\n\n4 - What\ns next / How to spread ?\n\n\n5 - Hard things learned\n\n\n\n\n\n\n\nOne ring to rules them all\n\n\n1 - Origin\n\n\nWhy Rollout automation are reinvented all the time ?\n\n\n\n\nEvery SRE thinks that their service is \nspecial\n and no common things can work\n\n\nEvery SRE are lazy, impatient and \nHaris\n\n\nEvery SRE thinks he can solve the problem, quickly, as a little fix\n\n\n\n\nBreak the circle of :\n\n-\n short term project\n\n-\n More long term tech debt, hacks, incidents \n\n\nThe situation:\n\n* SRE spends a non-trivial amount of time keeping rollout automation running \n\n* Write \nmonolithc\n and overly specific automation\n\n\n2 - How to converge to a \nstandard\n solution ?\n\n\nInvestment:\n\n-\n Implement a standard product\n\n\nWe already have a \nstandard\n framework to write automation !\n\n\nThere is a difference between:\n\n- standard framework\n\n- Common product\n\n\n1st principle\n\n* Decompose monolithic automation in microservices\n\n\n2nd\n\n* Tests\n\n\n3rd\n\n* Unprivileged\n\n\n-\n Design for all services\n\n-\n validate with MVP\n\n\n3 - Tech bits\n\n\nHosted and Shared\n\n* Scheduler\n\n* lock        (fined grained)\n\n* Policies    (To prevent issues)\n\n* Store       (Metadata)\n\n* UI\n\n\nCustom to the automated system\n\n* Executor\n\n* Other actions\n\n\n4 - What\ns next / How to spread ?\n\n\nHow get support from all the managers ?\n\n\nHow get enough people work on this ?\n\n- People who are interested do their best\n\n\nHow do you convince services to adopt ?\n\n- People are heavily invested in their code\n\n- Do not brag and say we will through up previous work\n\n\nRequirements gathering is ESSENTIAL\n\n\n5 - Hard things learned\n\n\n\n\nRequirement gathering is hard\n\n\nRollout automation, everybody wants it, nobody has time to work on it\n\n\nRollout isn\nt stand alone, lots of integration work\n\n\nRollout are long, and we can not stop (in storage at least)", 
            "title": "20170831_srecon17_1.3"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#one-ring-to-rules-them-all", 
            "text": "", 
            "title": "One ring to rules them all"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#1-origin", 
            "text": "Why Rollout automation are reinvented all the time ?   Every SRE thinks that their service is  special  and no common things can work  Every SRE are lazy, impatient and  Haris  Every SRE thinks he can solve the problem, quickly, as a little fix   Break the circle of : \n-  short term project \n-  More long term tech debt, hacks, incidents   The situation: \n* SRE spends a non-trivial amount of time keeping rollout automation running  \n* Write  monolithc  and overly specific automation", 
            "title": "1 - Origin"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#2-how-to-converge-to-a-standard-solution", 
            "text": "Investment: \n-  Implement a standard product  We already have a  standard  framework to write automation !  There is a difference between: \n- standard framework \n- Common product  1st principle \n* Decompose monolithic automation in microservices  2nd \n* Tests  3rd \n* Unprivileged  -  Design for all services \n-  validate with MVP", 
            "title": "2 - How to converge to a \"standard\" solution ?"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#3-tech-bits", 
            "text": "Hosted and Shared \n* Scheduler \n* lock        (fined grained) \n* Policies    (To prevent issues) \n* Store       (Metadata) \n* UI  Custom to the automated system \n* Executor \n* Other actions", 
            "title": "3 - Tech bits"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#4-whats-next-how-to-spread", 
            "text": "How get support from all the managers ?  How get enough people work on this ? \n- People who are interested do their best  How do you convince services to adopt ? \n- People are heavily invested in their code \n- Do not brag and say we will through up previous work  Requirements gathering is ESSENTIAL", 
            "title": "4 - What's next / How to spread ?"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.3/#5-hard-things-learned", 
            "text": "Requirement gathering is hard  Rollout automation, everybody wants it, nobody has time to work on it  Rollout isn t stand alone, lots of integration work  Rollout are long, and we can not stop (in storage at least)", 
            "title": "5 - Hard things learned"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/", 
            "text": "20170831_srecon17_1.4\n\n\n\n\n\n\n\nBecomming a Tech Lead\n\n\nTeam interest\n\n\nCommunication\n\n\nActivity\n\n\nBeeing TL is hard\n\n\nTLs / Manager\n\n\nHow to use TLs\n\n\n\n\n\n\n\nBecomming a Tech Lead\n\n\nTeam interest\n\n\nIt\ns all about Team execution\n\n* Help the team to meet its goals, objectives,\n\n* Help the team to be on the same page\n\n\n-\n It\ns primarily a social role\n\n\nCommunication\n\n\nTLs: People and communication\n\n* alias for \ngoes to meeting\n :)\n\n* Influence, guidance, sponsorship, delegation\n\n* Share and spread the knowledge\n\n\nActivity\n\n\nTLs time\n\n* essential:\n\n  - Tech capability\n\n  - judgment\n\n* Less important\n\n  - accomplishment\n\n\n-\n 40-60% of time is communication\n\n\nBeeing TL is hard\n\n\n\n\nDelegate high impact / \nfun\n and \nintersting\n work\n\n\nConince people to to things in a certain way is hard\n\n\n\n\n-\n You are responible for your team impact, not only yourself any more\n\n\nTLs / Manager\n\n\nComplementary and NOT overlapping\n\n\nManager (PO): Focus on business needs\n\n\n-\n Where = Manager\n\n-\n How = TL\n\n\nHow to use TLs\n\n\n\n\nSource of information\n\n\nProvide informations\n\n\nOR redirect to the right people\n\n\nDecision making\n\n\nAdvocate \n Amplifier\n\n\n\n\n=\n Key result: The team move forward\n\n\n\n\nThe TL works for the team,\n\nnot the other way around,", 
            "title": "20170831_srecon17_1.4"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#becomming-a-tech-lead", 
            "text": "", 
            "title": "Becomming a Tech Lead"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#team-interest", 
            "text": "It s all about Team execution \n* Help the team to meet its goals, objectives, \n* Help the team to be on the same page  -  It s primarily a social role", 
            "title": "Team interest"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#communication", 
            "text": "TLs: People and communication \n* alias for  goes to meeting  :) \n* Influence, guidance, sponsorship, delegation \n* Share and spread the knowledge", 
            "title": "Communication"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#activity", 
            "text": "TLs time \n* essential: \n  - Tech capability \n  - judgment \n* Less important \n  - accomplishment  -  40-60% of time is communication", 
            "title": "Activity"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#beeing-tl-is-hard", 
            "text": "Delegate high impact /  fun  and  intersting  work  Conince people to to things in a certain way is hard   -  You are responible for your team impact, not only yourself any more", 
            "title": "Beeing TL is hard"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#tls-manager", 
            "text": "Complementary and NOT overlapping  Manager (PO): Focus on business needs  -  Where = Manager \n-  How = TL", 
            "title": "TLs / Manager"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.4/#how-to-use-tls", 
            "text": "Source of information  Provide informations  OR redirect to the right people  Decision making  Advocate   Amplifier   =  Key result: The team move forward   The TL works for the team, \nnot the other way around,", 
            "title": "How to use TLs"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/", 
            "text": "20170831_srecon17_1.5\n\n\n\n\n\n\n\nDebug after bad deployments\n\n\nIntro\n\n\nContext\n\n\nBasic resolution\n\n\nMain types of errors\n\n\n\n\n\n\nDebugging\n\n\n1 - Find the errors\n\n\n2 - Find the bad commit\n\n\n\n\n\n\n\n\n\n\n\nDebug after bad deployments\n\n\nIntro\n\n\nContext\n\n\nBad deploy happens\n\n* Many commits\n\n* Many deploys happens, which was the cause ?\n\n\nBasic resolution\n\n\n\n\nRollback\n\n\nDebug\n\n  + Find the errors\n\n\nFix\n\n\nRestart\n\n\n\n\nMain types of errors\n\n\n\n\nPerf regressions\n\n\nBroken pages\n\n\nEmpty results\n\n\nUnexpected status code\n\n\n\n\nDebugging\n\n\nFind the NEW stack trace\n\nFind the bad commit\n\n\n1 - Find the errors\n\n\n\n\nLogs aggregation\n\n\nIdentifying NEW stack traces\n\n  -\n Compare 2 ES Queries by example\n\n  -\n Deduplicat similar stack traces\n\n\n\n\n2 - Find the bad commit\n\n\nAka: Wich of the 71 commits did that ?\n\n\nhttps://github.com/pinterest/git-stacktrace\n\n\ngit stacktrace COMMIT_ID..COMMIT_ID \n STACKTRACE", 
            "title": "20170831_srecon17_1.5"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#debug-after-bad-deployments", 
            "text": "", 
            "title": "Debug after bad deployments"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#intro", 
            "text": "", 
            "title": "Intro"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#context", 
            "text": "Bad deploy happens \n* Many commits \n* Many deploys happens, which was the cause ?", 
            "title": "Context"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#basic-resolution", 
            "text": "Rollback  Debug \n  + Find the errors  Fix  Restart", 
            "title": "Basic resolution"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#main-types-of-errors", 
            "text": "Perf regressions  Broken pages  Empty results  Unexpected status code", 
            "title": "Main types of errors"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#debugging", 
            "text": "Find the NEW stack trace \nFind the bad commit", 
            "title": "Debugging"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#1-find-the-errors", 
            "text": "Logs aggregation  Identifying NEW stack traces \n  -  Compare 2 ES Queries by example \n  -  Deduplicat similar stack traces", 
            "title": "1 - Find the errors"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.5/#2-find-the-bad-commit", 
            "text": "Aka: Wich of the 71 commits did that ?  https://github.com/pinterest/git-stacktrace  git stacktrace COMMIT_ID..COMMIT_ID   STACKTRACE", 
            "title": "2 - Find the bad commit"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.6/", 
            "text": "20170831_srecon17_1.6\n\n\n\n\n\n\n\nDebug at scale\n\n\n\n\n\n\n\nDebug at scale", 
            "title": "20170831_srecon17_1.6"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.6/#debug-at-scale", 
            "text": "", 
            "title": "Debug at scale"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.7/", 
            "text": "20170831_srecon17_1.7\n\n\n\n\n\n\n\nLinux Metrics - Workshop\n\n\n\n\n\n\n\nLinux Metrics - Workshop\n\n\nhttps://github.com/natict/linux-metrics", 
            "title": "20170831_srecon17_1.7"
        }, 
        {
            "location": "/2017/srecon/day2/20170831_srecon17_1.7/#linux-metrics-workshop", 
            "text": "https://github.com/natict/linux-metrics", 
            "title": "Linux Metrics - Workshop"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/", 
            "text": "20170901_srecon17_1.1\n\n\n\n\n\n\n\nOn premise K8s Cluster\n\n\n0 - Agenda\n\n\n1 - What is k8s ?\n\n\n2 - Why run k8s ?\n\n\n3 - Own cluster ?\n\n\n4 - Build what cloud offers\n\n\nHA for K8s masters\n\n\nIngress\n\n\nStorage\n\n\n\n\n\n\n\n\n\n\n\nOn premise K8s Cluster\n\n\n0 - Agenda\n\n\n\n\nWhat is k8s ?\n\n\nWhy run / Not run k8s\n\n\nWhy own cluster\n\n\nBuilding what public cloud provide\n\n\n\n\n1 - What is k8s ?\n\n\n\n\nNode\n\n\nPod\n\n\nService\n\n\nIngress (Bridge to access the service)\n\n\n\n\n2 - Why run k8s ?\n\n\nShopify:\n\n- Already using containers\n\n- OWN containers mgmt, but lot of glue \n\n\n-\n OK\n\n\nBUT\n\n- Long running Jobs\n\n- \nFixed\n scheduling assomption workflow\n\n- Stateful services\n\n\n-\n Maybe NOT\n\n\n3 - Own cluster ?\n\n\nShopify:\n\n- 2 DCs\n\n- Cloud might NOT be competitive at scale\n\n- Allow is to migrate stuff, 1 at a time, in a \ncloud\n way, without latency\n\n- Lack of full automation and security\n\n\n4 - Build what cloud offers\n\n\nComponents\n\n- HA (masters)\n\n- Ingress\n\n- Storage\n\n\nHA for K8s masters\n\n\n\n\nECMP\n Anycast for requests to API server\n\n\netcd\n is already clustered and provide the consistency\n\n\n\n\nIngress\n\n\nClusterIP / NodePort\n\n-\n Not really efficient\n\n\nImplemented IngressController for Haproxy Outside of the Cluster\n\n\nStorage\n\n\nComplicated", 
            "title": "20170901_srecon17_1.1"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#on-premise-k8s-cluster", 
            "text": "", 
            "title": "On premise K8s Cluster"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#0-agenda", 
            "text": "What is k8s ?  Why run / Not run k8s  Why own cluster  Building what public cloud provide", 
            "title": "0 - Agenda"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#1-what-is-k8s", 
            "text": "Node  Pod  Service  Ingress (Bridge to access the service)", 
            "title": "1 - What is k8s ?"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#2-why-run-k8s", 
            "text": "Shopify: \n- Already using containers \n- OWN containers mgmt, but lot of glue   -  OK  BUT \n- Long running Jobs \n-  Fixed  scheduling assomption workflow \n- Stateful services  -  Maybe NOT", 
            "title": "2 - Why run k8s ?"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#3-own-cluster", 
            "text": "Shopify: \n- 2 DCs \n- Cloud might NOT be competitive at scale \n- Allow is to migrate stuff, 1 at a time, in a  cloud  way, without latency \n- Lack of full automation and security", 
            "title": "3 - Own cluster ?"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#4-build-what-cloud-offers", 
            "text": "Components \n- HA (masters) \n- Ingress \n- Storage", 
            "title": "4 - Build what cloud offers"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#ha-for-k8s-masters", 
            "text": "ECMP  Anycast for requests to API server  etcd  is already clustered and provide the consistency", 
            "title": "HA for K8s masters"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#ingress", 
            "text": "ClusterIP / NodePort \n-  Not really efficient  Implemented IngressController for Haproxy Outside of the Cluster", 
            "title": "Ingress"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.1/#storage", 
            "text": "Complicated", 
            "title": "Storage"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.2/", 
            "text": "20170901_srecon17_1.2\n\n\n\n\n\n\n\nDistributed systems\n\n\n\n\n\n\n\nDistributed systems\n\n\nhttp://l42.org/JAE", 
            "title": "20170901_srecon17_1.2"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.2/#distributed-systems", 
            "text": "http://l42.org/JAE", 
            "title": "Distributed systems"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.3/", 
            "text": "20170901_srecon17_1.3\n\n\n\n\n\n\n\nAvoiding and breaking out of Capacity prison\n\n\n\n\n\n\n\nAvoiding and breaking out of Capacity prison", 
            "title": "20170901_srecon17_1.3"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.3/#avoiding-and-breaking-out-of-capacity-prison", 
            "text": "", 
            "title": "Avoiding and breaking out of Capacity prison"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.4/", 
            "text": "20170901_srecon17_1.4\n\n\n\n\n\n\n\nRun less software\n\n\nIntroduction\n\n\n1 - Save time - Choose \nStandard\n Tech\n\n\n\n\n\n\n\nRun less software\n\n\nIntroduction\n\n\nStartup\n\nIf 1 of GAFA goes on your business\n\n-\n DEAD\n\n\nNo the same:\n\n- engineering workload\n\n- money\n\n- resources\n\n\nTo compete\n\n\n1 - Save time - Choose \nStandard\n Tech\n\n\n\n\nConstrain yoursef\n\n\nSet a set of standard technologies\n\n*", 
            "title": "20170901_srecon17_1.4"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.4/#run-less-software", 
            "text": "", 
            "title": "Run less software"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.4/#introduction", 
            "text": "Startup \nIf 1 of GAFA goes on your business \n-  DEAD  No the same: \n- engineering workload \n- money \n- resources  To compete", 
            "title": "Introduction"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.4/#1-save-time-choose-standard-tech", 
            "text": "Constrain yoursef  Set a set of standard technologies \n*", 
            "title": "1 - Save time - Choose \"Standard\" Tech"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/", 
            "text": "20170901_srecon17_1.5\n\n\n\n\n\n\n\nCloudflare planet scale edge network monitoring with Prometheus\n\n\n0 - Outline\n\n\n1 - What is Prometheus ?\n\n\n2 - Why ?\n\n\n3 - Architecture\n\n\nContext\n\n\nArchi\n\n\n\n\n\n\n4 - Alerting\n\n\n5 - Monitoring the Monitoring\n\n\n6 - Tools\n\n\n\n\n\n\n\nCloudflare planet scale edge network monitoring with Prometheus\n\n\n0 - Outline\n\n\n\n\nWhat\n\n\nWhy\n\n\nArchitecture\n\n\nReducing alert fatigue\n\n\n\n\nTake Away:\n\n* Prometheus is the new GOLD standard\n\n* Good monitoring doesn\nt happen for free\n\n* Monitoring is interaction with Human !\n\n\n1 - What is Prometheus ?\n\n\n2 - Why ?\n\n\n\n\nSimple to operate ad deploy\n\n\nDynamic config\n\n\na Query languqge\n\n\n\n\nMetrics usage is powerfull\n\n\n\n\n\n\nIntegration\n\n\n\n\nKube\n\n\n\n\n\n\n3 - Architecture\n\n\nContext\n\n\nUsage\n\n- Monitoring\n\n- NOT long terme metrics storage\n\n\nOverview\n\n- 188 Prom servers\n\n- 4 Top levels\n\n- 250Gb of data / servers\n\n\nEdge architecture\n\n- Routing via Anycast\n\n- POPs configured identicly\n\n- POPs are independent\n\n\nCoreDC\n\n- Apps fore Business\n\n\nPromQL\n\n\nArchi\n\n\nPop\n\n- Node_exporter running on nodes\n\n- 1 Prom / POP\n\n- Prom pol queries on every nodes of a POP\n\n\nCore\n\n- Pol POPs Prom\n\n\nHA\n\n- x Prom in CoreUS\n\n- x Prom in CoreEU\n\n- \n\n\nRetention\n\n- 15 Days\n\n- Scrapped every 60s\n\n- Federation every 30s\n\n- No down sampling\n\n\nExporters used\n\n- Node Exporter\n\n- Blackbox exporter\n\n- Mtail\n\n- Cadvisor\n\n\nDeploying Exporters\n\n- Deploy in the same \ndomain of Failure\n\n\n4 - Alerting\n\n\nAlterManager\n\n- In CORE dc\n\n- Regions reporting to AlertManager\n\n\nWritin alters rules\n\n- Test query on past data\n\n- Use descriptive names\n\n- Alert reference\n\n- Must have action\n\n\nDashboards\n\n- Create DRILL-DOWN dashboards\n\n\n5 - Monitoring the Monitoring\n\n\nEach Prom monotir the other Prom: Mesh\n\n\n6 - Tools\n\n\nhttps://github.com/cloudflare/unsee", 
            "title": "20170901_srecon17_1.5"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#cloudflare-planet-scale-edge-network-monitoring-with-prometheus", 
            "text": "", 
            "title": "Cloudflare planet scale edge network monitoring with Prometheus"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#0-outline", 
            "text": "What  Why  Architecture  Reducing alert fatigue   Take Away: \n* Prometheus is the new GOLD standard \n* Good monitoring doesn t happen for free \n* Monitoring is interaction with Human !", 
            "title": "0 - Outline"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#1-what-is-prometheus", 
            "text": "", 
            "title": "1 - What is Prometheus ?"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#2-why", 
            "text": "Simple to operate ad deploy  Dynamic config  a Query languqge   Metrics usage is powerfull    Integration   Kube", 
            "title": "2 - Why ?"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#3-architecture", 
            "text": "", 
            "title": "3 - Architecture"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#context", 
            "text": "Usage \n- Monitoring \n- NOT long terme metrics storage  Overview \n- 188 Prom servers \n- 4 Top levels \n- 250Gb of data / servers  Edge architecture \n- Routing via Anycast \n- POPs configured identicly \n- POPs are independent  CoreDC \n- Apps fore Business  PromQL", 
            "title": "Context"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#archi", 
            "text": "Pop \n- Node_exporter running on nodes \n- 1 Prom / POP \n- Prom pol queries on every nodes of a POP  Core \n- Pol POPs Prom  HA \n- x Prom in CoreUS \n- x Prom in CoreEU \n-   Retention \n- 15 Days \n- Scrapped every 60s \n- Federation every 30s \n- No down sampling  Exporters used \n- Node Exporter \n- Blackbox exporter \n- Mtail \n- Cadvisor  Deploying Exporters \n- Deploy in the same  domain of Failure", 
            "title": "Archi"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#4-alerting", 
            "text": "AlterManager \n- In CORE dc \n- Regions reporting to AlertManager  Writin alters rules \n- Test query on past data \n- Use descriptive names \n- Alert reference \n- Must have action  Dashboards \n- Create DRILL-DOWN dashboards", 
            "title": "4 - Alerting"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#5-monitoring-the-monitoring", 
            "text": "Each Prom monotir the other Prom: Mesh", 
            "title": "5 - Monitoring the Monitoring"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.5/#6-tools", 
            "text": "https://github.com/cloudflare/unsee", 
            "title": "6 - Tools"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/", 
            "text": "20170901_srecon17_1.6\n\n\n\n\n\n\n\nMonitoring design principles\n\n\n0 - Agenda\n\n\n1 - History\n\n\n2 - SLO\n\n\n3 - Best practices\n\n\n\n\n\n\n\nMonitoring design principles\n\n\n0 - Agenda\n\n\n\n\nHistory\n\n\nService Level Objectif\n\n\nSimple designs\n\n\n\n\n1 - History\n\n\nMonitoring:\n\n- Action of Observing, checking, and compute a status\n\n\nEvolution of Web monitoring\n\n- 1995: Load webpage every 15 minutes\n\n- 2000: Wathcing every webrequests\n\n- 2015: Deep analysis of every transaction from a user perspective\n\n\nEvolution of DB monitoring\n\n- 1995: Synthetic queries to test perf\n\n- 2000: Wathcing every queries\n\n- 2015: Deep analysis\n\n\nMonitoring is sophisticated\n\n* Increased telemetry volume\n\n* More valuable operational questions\n\n* Increased organizational velocity\n\n\n2 - SLO\n\n\nUsually based on percentiles\n\n\n3 - Best practices\n\n\n\n\nMonitoring outside of the Tech stack / blast radius\n\n\nDo not silo your data (For correlation)\n\n\nAlerts require documentation\n\n  + Requires discipline\n\n  + Human readable explanation\n\n  + Business impact description\n\n  + Remediation procedure\n\n  + Escalation documentation", 
            "title": "20170901_srecon17_1.6"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/#monitoring-design-principles", 
            "text": "", 
            "title": "Monitoring design principles"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/#0-agenda", 
            "text": "History  Service Level Objectif  Simple designs", 
            "title": "0 - Agenda"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/#1-history", 
            "text": "Monitoring: \n- Action of Observing, checking, and compute a status  Evolution of Web monitoring \n- 1995: Load webpage every 15 minutes \n- 2000: Wathcing every webrequests \n- 2015: Deep analysis of every transaction from a user perspective  Evolution of DB monitoring \n- 1995: Synthetic queries to test perf \n- 2000: Wathcing every queries \n- 2015: Deep analysis  Monitoring is sophisticated \n* Increased telemetry volume \n* More valuable operational questions \n* Increased organizational velocity", 
            "title": "1 - History"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/#2-slo", 
            "text": "Usually based on percentiles", 
            "title": "2 - SLO"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.6/#3-best-practices", 
            "text": "Monitoring outside of the Tech stack / blast radius  Do not silo your data (For correlation)  Alerts require documentation \n  + Requires discipline \n  + Human readable explanation \n  + Business impact description \n  + Remediation procedure \n  + Escalation documentation", 
            "title": "3 - Best practices"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.7/", 
            "text": "20170901_srecon17_1.7\n\n\n\n\n\n\n\nHave you tryied to turn it off and on again ?\n\n\n\n\n\n\n\nHave you tryied to turn it off and on again ?", 
            "title": "20170901_srecon17_1.7"
        }, 
        {
            "location": "/2017/srecon/day3/20170901_srecon17_1.7/#have-you-tryied-to-turn-it-off-and-on-again", 
            "text": "", 
            "title": "Have you tryied to turn it off and on again ?"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/1-ONAP/", 
            "text": "1-ONAP\n\n\n\n\n\n\n\n1 - Overview\n\n\n2 - Usecases\n\n\n\n\n\n\n\n1 - Overview\n\n\nPlatform = Collection of software\n\n\nArchitecture\n\n* OOM: Onap Operations Manager\n\n  - Design: Portal\n\n  - Runtime:\n\n\nBuilt for OpenSource and Enterprise usage\n\n\n2 - Usecases", 
            "title": "1-ONAP"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/1-ONAP/#1-overview", 
            "text": "Platform = Collection of software  Architecture \n* OOM: Onap Operations Manager \n  - Design: Portal \n  - Runtime:  Built for OpenSource and Enterprise usage", 
            "title": "1 - Overview"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/1-ONAP/#2-usecases", 
            "text": "", 
            "title": "2 - Usecases"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/", 
            "text": "2 - Mozilla Molotov\n\n\n\n\n\n\n\n1 - Webservices / WebApps\n\n\n2 - App under Stress\n\n\n3 - Test ?\n\n\n4 - Distributed systems ?\n\n\n5 - Molotov\n\n\nDesc\n\n\nFeatures\n\n\n\n\n\n\n\n1 - Webservices / WebApps\n\n\nWebapp\n\n* Client-side\n\n* User Auth\n\n* Lots of caching\n\n\nWebservices\n\n* As dumb as possinble\n\n* App 2 App transaction\n\n\n2 - App under Stress\n\n\nGoals\n\n* What do we exhaust first under high load ?\n\n* When (Conditions)\n\n* Behavior\n\n* Recover ?\n\n\n3 - Test ?\n\n\nMetrics\n\n* Myth of client-side feedback\n\n\nExisting Tools\n\n* AB\n\n* Boom (AB in Go)\n\n* JMeter\n\n* Grinder\n\n* Bees (Aws based)\n\n* Locust (Pythonm Zmq, Gevent)\n\n\n4 - Distributed systems ?\n\n\n\n\nHard\n\n\n\n\n-\n Molotov\n\n\n5 - Molotov\n\n\nDesc\n\n\n\n\nPyhton3\n\n\nKISS\n\n\nFramework\n\n\nBased on AIO\n\n\n\n\n-\n 30k RQPS on a laptop\n\n\nhttp://molotov.readthedocs.io/en/stable/\n\n\nFeatures\n\n\n\n\nMulti-scenario support\n\n\nRemove scenario support (Github + Json)\n\n\nCSV export", 
            "title": "2 - Mozilla Molotov"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#1-webservices-webapps", 
            "text": "Webapp \n* Client-side \n* User Auth \n* Lots of caching  Webservices \n* As dumb as possinble \n* App 2 App transaction", 
            "title": "1 - Webservices / WebApps"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#2-app-under-stress", 
            "text": "Goals \n* What do we exhaust first under high load ? \n* When (Conditions) \n* Behavior \n* Recover ?", 
            "title": "2 - App under Stress"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#3-test", 
            "text": "Metrics \n* Myth of client-side feedback  Existing Tools \n* AB \n* Boom (AB in Go) \n* JMeter \n* Grinder \n* Bees (Aws based) \n* Locust (Pythonm Zmq, Gevent)", 
            "title": "3 - Test ?"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#4-distributed-systems", 
            "text": "Hard   -  Molotov", 
            "title": "4 - Distributed systems ?"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#5-molotov", 
            "text": "", 
            "title": "5 - Molotov"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#desc", 
            "text": "Pyhton3  KISS  Framework  Based on AIO   -  30k RQPS on a laptop  http://molotov.readthedocs.io/en/stable/", 
            "title": "Desc"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/2-Mozilla Molotov/#features", 
            "text": "Multi-scenario support  Remove scenario support (Github + Json)  CSV export", 
            "title": "Features"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/3-DevOpsTools_go/", 
            "text": "3 - Distributing DevOps Tools for fun and Profit\n\n\n\n\n\n\n\n1 - Past\n\n\n2 - Docker\n\n\n3 - Better distribution\n\n\n4 - OpenSource: Cali\n\n\n\n\n\n\n\n1 - Past\n\n\nVagrant with\n\n* Chefdk\n\n* Docker\n\n* Git\n\n* Terraform\n\n* Vault\n\n* AWS Cli\n\n* \n\n\nIssues:\n\n- Bigs\n\n- Slow\n\n- Take time to build\n\n- Hard to maintain\n\n\n2 - Docker\n\n\nDocker:\n\n* For Mac\n\n* For Windows\n\n* Linux native\n\n\nThe past -\n bash aliases\n\n\n3 - Better distribution\n\n\nPSCLI\n\n* Docker + Cobra/Viper\n\n\nFeatures:\n\n* Self updating\n\n* Bind $PWD\n\n* Run with Local + Remote docker\n\n* Anonymous usage statistics \n----- WOW !\n\n* Run arbitrary code before launching container\n\n  - Authen against vault\n\n  - Generate STS creds\n\n  - \n\n\n4 - OpenSource: Cali\n\n\nhttps://github.com/skybet/cali", 
            "title": "3 - Distributing DevOps Tools for fun and Profit"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/3-DevOpsTools_go/#1-past", 
            "text": "Vagrant with \n* Chefdk \n* Docker \n* Git \n* Terraform \n* Vault \n* AWS Cli \n*   Issues: \n- Bigs \n- Slow \n- Take time to build \n- Hard to maintain", 
            "title": "1 - Past"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/3-DevOpsTools_go/#2-docker", 
            "text": "Docker: \n* For Mac \n* For Windows \n* Linux native  The past -  bash aliases", 
            "title": "2 - Docker"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/3-DevOpsTools_go/#3-better-distribution", 
            "text": "PSCLI \n* Docker + Cobra/Viper  Features: \n* Self updating \n* Bind $PWD \n* Run with Local + Remote docker \n* Anonymous usage statistics  ----- WOW ! \n* Run arbitrary code before launching container \n  - Authen against vault \n  - Generate STS creds \n  -", 
            "title": "3 - Better distribution"
        }, 
        {
            "location": "/2018/01-FOSDEM/day1/3-DevOpsTools_go/#4-opensource-cali", 
            "text": "https://github.com/skybet/cali", 
            "title": "4 - OpenSource: Cali"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/", 
            "text": "1 - Diskimage-builder\n\n\n\n\n\n\n\n1 - Before\n\n\n2 - Disk image builder\n\n\nExample\n\n\nSupport Matrix\n\n\n3 - Details\n\n\nElements\n\n\nBlock Device Layer\n\n\nBlock Device Later MBR module\n\n\nTips\n\n\n\n\n\n\n\n1 - Before\n\n\nBuilding OS the \ntrad\n way\n\n- Iso (+ kind of automation KS / Preseed)\n\n- OS Treee (DebootStrap)\n\n\nIssue:\n\n- For each OS, you have a different tool\n\n- Expodential combinaison\n\n\nGoal:\n\n- 1 to rule them all\n\n\n2 - Disk image builder\n\n\nExample\n\n\ndisk-image-builder debian-minimal vm\ndisk-image-builder fedora-minimal vm\ndisk-image-builder centos-minimal vm\n\ndisk-image-builder -o docker debian-minimal\n\n\n\n\nSupport Matrix\n\n\nCombinaison:\n\n* OS\n\n* Arch\n\n* Env (VMWare, OpenStack, KVM, AWS, DOcker, Bare-metal)\n\n\nQemu is used for Arch and Image convertion\n\n\n3 - Details\n\n\nElements\n\n\n\n\nLike puppet-modules, \n\n\n\n\n\n\nAn element is:\n\n- Collection of config files\n\n- Collection of dirs\n\n- Collection of scripts\n\n\nREADME.rst\nelement-deps\npackage-installs.yaml   (By Arch)\n\nenvironment.d/          (Export ENV variables)\n\nroot.d/                 (Scripts)\n\n\n\n\nBlock Device Layer\n\n\nHierarchie:\n\n0 - Disk space, loop device, \n\n1 - Partitioning / LVM\n\n2 - FS generation\n\n3 - Mount\n\n4 - FSTab handling\n\n\n-\n Yaml file config with actions for each layer\n\n\nBlock Device Later MBR module\n\n\n-\n OK\n\n\nTips\n\n\nBuild lean OS image and pass it to your config Management system", 
            "title": "1 - Diskimage-builder"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#1-before", 
            "text": "Building OS the  trad  way \n- Iso (+ kind of automation KS / Preseed) \n- OS Treee (DebootStrap)  Issue: \n- For each OS, you have a different tool \n- Expodential combinaison  Goal: \n- 1 to rule them all", 
            "title": "1 - Before"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#2-disk-image-builder", 
            "text": "", 
            "title": "2 - Disk image builder"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#example", 
            "text": "disk-image-builder debian-minimal vm\ndisk-image-builder fedora-minimal vm\ndisk-image-builder centos-minimal vm\n\ndisk-image-builder -o docker debian-minimal", 
            "title": "Example"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#support-matrix", 
            "text": "Combinaison: \n* OS \n* Arch \n* Env (VMWare, OpenStack, KVM, AWS, DOcker, Bare-metal)  Qemu is used for Arch and Image convertion", 
            "title": "Support Matrix"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#3-details", 
            "text": "", 
            "title": "3 - Details"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#elements", 
            "text": "Like puppet-modules,     An element is: \n- Collection of config files \n- Collection of dirs \n- Collection of scripts  README.rst\nelement-deps\npackage-installs.yaml   (By Arch)\n\nenvironment.d/          (Export ENV variables)\n\nroot.d/                 (Scripts)", 
            "title": "Elements"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#block-device-layer", 
            "text": "Hierarchie: \n0 - Disk space, loop device,  \n1 - Partitioning / LVM \n2 - FS generation \n3 - Mount \n4 - FSTab handling  -  Yaml file config with actions for each layer", 
            "title": "Block Device Layer"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#block-device-later-mbr-module", 
            "text": "-  OK", 
            "title": "Block Device Later MBR module"
        }, 
        {
            "location": "/2018/01-FOSDEM/day2/1-Diskimage-builder/#tips", 
            "text": "Build lean OS image and pass it to your config Management system", 
            "title": "Tips"
        }, 
        {
            "location": "/2018/kubecon/README/", 
            "text": "README\n\n\n\n\nNotes taken during the CNCF / KubeCon EU17\n\n\nhttps://kccnceu18.sched.com/", 
            "title": "README"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_1/", 
            "text": "20180502_keynote_1\n\n\n\n\n\n\n\n1 - Intro\n\n\n2 - How good is our code ?\n\n\n3 - CNCF tools\n\n\n\n\n\n\n\n1 - Intro\n\n\nKubeCon growth\n\n* 2015: ~8000\n\n* 2018: ~+4000 (x3 2017)\n\n\nCNCF\n\n* 23000 contributors\n\n* Certification Program\n\n  + NEW: AppDevelopper cert (CKAD)\n\n* \n\n\n2 - How good is our code ?\n\n\nSoftware stack (Examples):\n\n* Kernel 17M LOCS\n\n* Kube: 35M\n\n* NodeJS: 12,3M\n\n* Libs: 2,5M\n\n* App: 40K\n\n\nOpenSouce helps to fix things sooner\n\n-\n BUT a pacth does not help till it is deployed\n\n\nCI Tests ?\n\n* Unit\n\n* Integration\n\n* Smoke\n\n* \n\n\nHow good is our code ?\n\n-\n NOT enought\n\n\nTesting ?\n\n* Like Science\n\n  - Has to be testable and falsiable\n\n\n3 - CNCF tools\n\n\nInteractive landscape map\n\n* \nhttps://l.cncf.io", 
            "title": "20180502_keynote_1"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_1/#1-intro", 
            "text": "KubeCon growth \n* 2015: ~8000 \n* 2018: ~+4000 (x3 2017)  CNCF \n* 23000 contributors \n* Certification Program \n  + NEW: AppDevelopper cert (CKAD) \n*", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_1/#2-how-good-is-our-code", 
            "text": "Software stack (Examples): \n* Kernel 17M LOCS \n* Kube: 35M \n* NodeJS: 12,3M \n* Libs: 2,5M \n* App: 40K  OpenSouce helps to fix things sooner \n-  BUT a pacth does not help till it is deployed  CI Tests ? \n* Unit \n* Integration \n* Smoke \n*   How good is our code ? \n-  NOT enought  Testing ? \n* Like Science \n  - Has to be testable and falsiable", 
            "title": "2 - How good is our code ?"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_1/#3-cncf-tools", 
            "text": "Interactive landscape map \n*  https://l.cncf.io", 
            "title": "3 - CNCF tools"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_2/", 
            "text": "20180502_keynote_2\n\n\n\n\n\n\n\n1 - Intro\n\n\nSandbox\n\n\nIncubation\n\n\n\n\n\n\n\n1 - Intro\n\n\nProjects lifecycle\n\n* Sandbox\n\n* Incubator\n\n* Stable\n\n\nSandbox\n\n\nRook:\n\n* CN storage orchestrator\n\n* Abstraction\n\n* Intergrated with K8s\n\n\nOpenPolicyAgent\n\n* Authorization\n\n* Control\n\n* \n\n\nSpiffe \n Spire\n\n* SPIFFE: Spec and API for security\n\n* SPIRE: Spiffe Runtime Environment\n\n* Notes\n\n  + Supported by Vault\n\n  + Spire runs on different platform\n\n  + github.com/spiffe\n\n\nIncubation\n\n\nCoreDNS\n\n* Beta in k8s since 1.10\n\n\nLinkedD\n\n* Service Mesh\n\n* 10+ users from Fortune500\n\n\nEnvoy\n\n* Service Mesh + Proxy\n\n* LB \n Routing, TCP Proxy, Tracing\n\n\nPrometheus\n\n* v2.0\n\n* Metrics\n\n* Time series db + Query Language\n\n\nOpenTracing\n\n* New bindings (Python, C++, \n)\n\n\nJaeger:\n\n* Tracinf\n\n* UIs\n\n\nFluentD \n FluentBit\n\n* k8s Daemonset integration\n\n* New API for consolidation\n\n* Better buffer\n\n\nNATS\n\n* Messaging service", 
            "title": "20180502_keynote_2"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_2/#1-intro", 
            "text": "Projects lifecycle \n* Sandbox \n* Incubator \n* Stable", 
            "title": "1 - Intro"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_2/#sandbox", 
            "text": "Rook: \n* CN storage orchestrator \n* Abstraction \n* Intergrated with K8s  OpenPolicyAgent \n* Authorization \n* Control \n*   Spiffe   Spire \n* SPIFFE: Spec and API for security \n* SPIRE: Spiffe Runtime Environment \n* Notes \n  + Supported by Vault \n  + Spire runs on different platform \n  + github.com/spiffe", 
            "title": "Sandbox"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_2/#incubation", 
            "text": "CoreDNS \n* Beta in k8s since 1.10  LinkedD \n* Service Mesh \n* 10+ users from Fortune500  Envoy \n* Service Mesh + Proxy \n* LB   Routing, TCP Proxy, Tracing  Prometheus \n* v2.0 \n* Metrics \n* Time series db + Query Language  OpenTracing \n* New bindings (Python, C++,  )  Jaeger: \n* Tracinf \n* UIs  FluentD   FluentBit \n* k8s Daemonset integration \n* New API for consolidation \n* Better buffer  NATS \n* Messaging service", 
            "title": "Incubation"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_3/", 
            "text": "20180502_keynote_3\n\n\n\n\n\n\n\nk8s @CERN\n\n\nUse case1\n\n\nUse case2\n\n\n\n\n\n\n\nk8s @CERN\n\n\nMotication for Federation\n\n* Periodic load spike\n\n* Campaign of data reconstruction before conferences \n\n\nUse case1\n\n\nTools used\n\n* HTCondor\n\n* Sched, collector, negotiator, StartD (Running workload in node)\n\n\nConfig Example:\n\n\nCERNEnvironment=\nDatacenter=\nTotalCPUs=\nTotalMemoru=\n\n\n\nK8sHost with HTCondor control plane\n\n* Sched\n\n* Collector\n\n* Negotiaor\n\n\nkubefed init fed --host-cluster-context=\n\nkubefed join --context= \\\n             --host-cluster= \\\n             --cluster-context=\n\n\n\n\nStartD is deployed as a Daemonset\n\n\nUse case2\n\n\nOpenSouce tools for Reusable Analysis Platform\n\n* github.com/recast-hep:       Live output\n\n* github.com/diana-hep/yadage: workflow orchestrator\n\n* github.com/reanahub:         Registry to share results", 
            "title": "20180502_keynote_3"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_3/#k8s-cern", 
            "text": "Motication for Federation \n* Periodic load spike \n* Campaign of data reconstruction before conferences", 
            "title": "k8s @CERN"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_3/#use-case1", 
            "text": "Tools used \n* HTCondor \n* Sched, collector, negotiator, StartD (Running workload in node)  Config Example:  CERNEnvironment=\nDatacenter=\nTotalCPUs=\nTotalMemoru=  K8sHost with HTCondor control plane \n* Sched \n* Collector \n* Negotiaor  kubefed init fed --host-cluster-context=\n\nkubefed join --context= \\\n             --host-cluster= \\\n             --cluster-context=  StartD is deployed as a Daemonset", 
            "title": "Use case1"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_3/#use-case2", 
            "text": "OpenSouce tools for Reusable Analysis Platform \n* github.com/recast-hep:       Live output \n* github.com/diana-hep/yadage: workflow orchestrator \n* github.com/reanahub:         Registry to share results", 
            "title": "Use case2"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_4/", 
            "text": "20180502_keynote_4\n\n\n\n\n\n\n\nFuture ?\n\n\n\n\n\n\n\nFuture ?\n\n\nWe are moving to pushing code, not containers\n\n\nEthics:\n\n* Great power comes \nYOUR responsability\n\n* Diversity is table stakes\n\n* We must low barrier to morality", 
            "title": "20180502_keynote_4"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_keynote_4/#future", 
            "text": "We are moving to pushing code, not containers  Ethics: \n* Great power comes  YOUR responsability \n* Diversity is table stakes \n* We must low barrier to morality", 
            "title": "Future ?"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/", 
            "text": "20180502_talk_1\n\n\n\n\n\n\n\nAdidas journey to k8s\n\n\n1 - Context\n\n\n1.1 - Origin\n\n\n1.2 - Docker\n\n\n1.3 - K8s\n\n\n\n\n\n\n2 - GiantSwarm collaboration\n\n\n2.1 - Introduction\n\n\n2.2 - Enterprise grade\n\n\n\n\n\n\n3 - The road to production\n\n\n3.1 - 1st steps\n\n\n3.2 - 2nd steps\n\n\n\n\n\n\n4 - Cluster strategy\n\n\n4.1 - Feedbacks\n\n\n\n\n\n\n5 - Questions\n\n\nStorage\n\n\nMonitoring\n\n\n\n\n\n\n\n\n\n\n\nAdidas journey to k8s\n\n\n1 - Context\n\n\n1.1 - Origin\n\n\nAdidas:\n\n* A the time, IT mainly controlled by managers\n\n\nTimeline\n\n* Late 2013\n\n* Asked to estimate cost\n\n* Costed a lot\n\n  + Because a LOT of MANUAL steps\n\n\n1.2 - Docker\n\n\nDocker cames\n\n* Lets use it !\n\n* Dev: Ubuntu: OK\n\n* Prod: RHEL: KO\n\n\nCurrent State:\n\n* everything running through Puppet (Orchestrated)\n\n* Worked but \n weird usage\n\n\n1.3 - K8s\n\n\nJuly 2015: k8s v1.0\n\n* Seems cool\n\n* Need it for IT productivity\n\n* How to embarq the company ?\n\n\nThen Organization change (New CTO)\n\n* New era\n\n* Was time to go fast\n\n\n2 - GiantSwarm collaboration\n\n\n2.1 - Introduction\n\n\nFirst, re-explain containers vs Docker marketing\n\n* Containers are small VMs ! (no)\n\n* You can run it anywhere (Hum \n not magically)\n\n* Want HA and everything\n\n\nHow we started (Adidas):\n\n* A new strategy for new project (Based on OpenSouce, collaboration)\n\n-\n Change comes with culture first\n\n\nGianSwarm\n\n* K8s distribution\n\n\nAdidas + GiantSwarm\n\n* Sandbox cluster\n\n\n2.2 - Enterprise grade\n\n\nMore than sandbox ?\n\n* Security\n\n* Backups\n\n* Automation\n\n* \n\n\nAdidas managed to create a group of users:\n\n* who wanted changes\n\n* who also understand this is a new platform, expect things to not run withou any issues\n\n\n3 - The road to production\n\n\n3.1 - 1st steps\n\n\nStep:\n\n* Took a store with little traffic (Finn)\n\n* Deployed om k8s\n\n* Shift some users\n\n* + LoadTesting\n\n-\n FAILED\n\n\nInvstigation\n\n* Kube-proxy\n\n* AWS network interface configuration was saturated\n\n* LoadTesting team did too huge loadtest (Know your tools)\n\n\nFixed everythin, go prod\n\n\n3.2 - 2nd steps\n\n\nGo production:\n\n* Flash sales\n\n-\n FAILED\n\n\nInvestigation\n\n-\n Not enought resources allocated to handle peak traffic\n\n\n4 - Cluster strategy\n\n\nNamespaces, Tenant, \n\n\n4.1 - Feedbacks\n\n\nYou have different teams, different speads, different workload\n\n-\n You can not put all together on the same cluster \n\n\nk8s Updates\n\n* Hard to be inplace\n\n* Impacts users\n\n-\n Give a cluster to each team\n\n-\n Give new cluster with upgrade\n\n\n5 - Questions\n\n\nStorage\n\n\nIt all depands your usage\n\n-\n Rook seems good for block storage\n\n-\n S3 API is good from app pov\n\n\nMonitoring\n\n\nPrometheus\n\n* How many containers (Spinning rate)\n\n* Diff between pods specs\n\n*", 
            "title": "20180502_talk_1"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#adidas-journey-to-k8s", 
            "text": "", 
            "title": "Adidas journey to k8s"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#1-context", 
            "text": "", 
            "title": "1 - Context"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#11-origin", 
            "text": "Adidas: \n* A the time, IT mainly controlled by managers  Timeline \n* Late 2013 \n* Asked to estimate cost \n* Costed a lot \n  + Because a LOT of MANUAL steps", 
            "title": "1.1 - Origin"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#12-docker", 
            "text": "Docker cames \n* Lets use it ! \n* Dev: Ubuntu: OK \n* Prod: RHEL: KO  Current State: \n* everything running through Puppet (Orchestrated) \n* Worked but   weird usage", 
            "title": "1.2 - Docker"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#13-k8s", 
            "text": "July 2015: k8s v1.0 \n* Seems cool \n* Need it for IT productivity \n* How to embarq the company ?  Then Organization change (New CTO) \n* New era \n* Was time to go fast", 
            "title": "1.3 - K8s"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#2-giantswarm-collaboration", 
            "text": "", 
            "title": "2 - GiantSwarm collaboration"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#21-introduction", 
            "text": "First, re-explain containers vs Docker marketing \n* Containers are small VMs ! (no) \n* You can run it anywhere (Hum   not magically) \n* Want HA and everything  How we started (Adidas): \n* A new strategy for new project (Based on OpenSouce, collaboration) \n-  Change comes with culture first  GianSwarm \n* K8s distribution  Adidas + GiantSwarm \n* Sandbox cluster", 
            "title": "2.1 - Introduction"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#22-enterprise-grade", 
            "text": "More than sandbox ? \n* Security \n* Backups \n* Automation \n*   Adidas managed to create a group of users: \n* who wanted changes \n* who also understand this is a new platform, expect things to not run withou any issues", 
            "title": "2.2 - Enterprise grade"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#3-the-road-to-production", 
            "text": "", 
            "title": "3 - The road to production"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#31-1st-steps", 
            "text": "Step: \n* Took a store with little traffic (Finn) \n* Deployed om k8s \n* Shift some users \n* + LoadTesting \n-  FAILED  Invstigation \n* Kube-proxy \n* AWS network interface configuration was saturated \n* LoadTesting team did too huge loadtest (Know your tools)  Fixed everythin, go prod", 
            "title": "3.1 - 1st steps"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#32-2nd-steps", 
            "text": "Go production: \n* Flash sales \n-  FAILED  Investigation \n-  Not enought resources allocated to handle peak traffic", 
            "title": "3.2 - 2nd steps"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#4-cluster-strategy", 
            "text": "Namespaces, Tenant,", 
            "title": "4 - Cluster strategy"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#41-feedbacks", 
            "text": "You have different teams, different speads, different workload \n-  You can not put all together on the same cluster   k8s Updates \n* Hard to be inplace \n* Impacts users \n-  Give a cluster to each team \n-  Give new cluster with upgrade", 
            "title": "4.1 - Feedbacks"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#5-questions", 
            "text": "", 
            "title": "5 - Questions"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#storage", 
            "text": "It all depands your usage \n-  Rook seems good for block storage \n-  S3 API is good from app pov", 
            "title": "Storage"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_1/#monitoring", 
            "text": "Prometheus \n* How many containers (Spinning rate) \n* Diff between pods specs \n*", 
            "title": "Monitoring"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/", 
            "text": "20180502_talk_2\n\n\n\n\n\n\n\nK8s SIG: Cluster Lifecycle\n\n\nIntro\n\n\nCluster API\n\n\nKubeADM\n\n\nTakes away\n\n\nRecent accomplishements\n\n\nOne more thing: Kops\n\n\n\n\n\n\n\nK8s SIG: Cluster Lifecycle\n\n\nMission\n\n\n\n\nChange k8s to make it easier to Create and Operate\n\n\n\n\n\n\nIntro\n\n\nWhat do we Do:\n\n1. Control Plane Installation Management\n\n  + Building \nkubeadm\n\n  + Maintaining the doc\n\n\n2.Control plane Config Management\n\n  + Building a CP API\n\n  + Componenent config Best practices\n\n\nNEW:\n\n\n\n\n\n\nSimplifying Infra Mgmt\n\n  + Machine API + Cluster API\n\n\n\n\n\n\nAdd Management\n\n  + WIP\n\n\n\n\n\n\nETCD Management\n\n  + WIP\n\n\n\n\n\n\nCluster API\n\n\nAlpha state\n\n+ API specs have been written\n\n\nAPI objects:\n\n* Cluster: General cluster level config (Ex: Network)\n\n* Machine: Host (VM or Physical), not yet a Node\n\n* MachineSet / MachineDeployment\n\n\n\n\nDeclarative way to create, configure and manage a cluster\n\n\n\n\n\n\n-\n Generic config interface for any tool (Kops, Aws, GKE, Terraform, \n)\n\n\nKubeADM\n\n\n| Layer3 | Addons: Paas services (Monitoring, Logging, LBs / Ingress, ....)\n\n|        | + ClusterAPI on top of k8s api server to manage the cluster     |\n\n| Layer2 | Kubeadm: bootstrap a cluster (Machines are now qualified nodes) |\n\n| Layer1 | Infra -\n Machines |\n\n\nTakes away\n\n\nKuebadm\n\n* Set of tasks as Best-practice cluster bootstrap\n\n* Focus on user experience\n\n* Linited scope: Building block\n\n  + Deals with LocalFS + k8s API\n\n  + Agnostic to \nkubectl\n\n  + NOT favoring any CNI\n\n* Composable: divided into Phases\n\n  + you can use only 1 step of kubeadm if you don\nt want it to spawn a whole CP by its own\n\n\nRecent accomplishements\n\n\n\n\nKubeadm 1.10 (Sync with k8s releases)\n\n\nAdvanced AUDITING\n\n\netcd 3.2\n\n\netcd TLS encryption\n\n\nCoreDNS beta support\n\n\nAlpha ClusterAPI\n\n\nIntegrated with Kops 1.9 and Kubespray\n\n\n\n\nOne more thing: Kops\n\n\nOpinonated way to build on AWS + GCE\n\n* \nkops create cluster CLUSTERNAME --options\n\n* \nkops Update cluster CLUSTERNAME --options\n\n\nKops roadmap\n\n* Extract components for ClusterAPI and MachineAPI\n\n* Extract addons-manager\n\n* ETCD manager", 
            "title": "20180502_talk_2"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#k8s-sig-cluster-lifecycle", 
            "text": "Mission   Change k8s to make it easier to Create and Operate", 
            "title": "K8s SIG: Cluster Lifecycle"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#intro", 
            "text": "What do we Do: \n1. Control Plane Installation Management \n  + Building  kubeadm \n  + Maintaining the doc  2.Control plane Config Management \n  + Building a CP API \n  + Componenent config Best practices  NEW:    Simplifying Infra Mgmt \n  + Machine API + Cluster API    Add Management \n  + WIP    ETCD Management \n  + WIP", 
            "title": "Intro"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#cluster-api", 
            "text": "Alpha state \n+ API specs have been written  API objects: \n* Cluster: General cluster level config (Ex: Network) \n* Machine: Host (VM or Physical), not yet a Node \n* MachineSet / MachineDeployment   Declarative way to create, configure and manage a cluster    -  Generic config interface for any tool (Kops, Aws, GKE, Terraform,  )", 
            "title": "Cluster API"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#kubeadm", 
            "text": "| Layer3 | Addons: Paas services (Monitoring, Logging, LBs / Ingress, ....) \n|        | + ClusterAPI on top of k8s api server to manage the cluster     | \n| Layer2 | Kubeadm: bootstrap a cluster (Machines are now qualified nodes) | \n| Layer1 | Infra -  Machines |", 
            "title": "KubeADM"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#takes-away", 
            "text": "Kuebadm \n* Set of tasks as Best-practice cluster bootstrap \n* Focus on user experience \n* Linited scope: Building block \n  + Deals with LocalFS + k8s API \n  + Agnostic to  kubectl \n  + NOT favoring any CNI \n* Composable: divided into Phases \n  + you can use only 1 step of kubeadm if you don t want it to spawn a whole CP by its own", 
            "title": "Takes away"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#recent-accomplishements", 
            "text": "Kubeadm 1.10 (Sync with k8s releases)  Advanced AUDITING  etcd 3.2  etcd TLS encryption  CoreDNS beta support  Alpha ClusterAPI  Integrated with Kops 1.9 and Kubespray", 
            "title": "Recent accomplishements"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_2/#one-more-thing-kops", 
            "text": "Opinonated way to build on AWS + GCE \n*  kops create cluster CLUSTERNAME --options \n*  kops Update cluster CLUSTERNAME --options  Kops roadmap \n* Extract components for ClusterAPI and MachineAPI \n* Extract addons-manager \n* ETCD manager", 
            "title": "One more thing: Kops"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/", 
            "text": "20180502_talk_3\n\n\n\n\n\n\n\nBuilding images without Docker\n\n\n1 - Why\n\n\n2 - Dockerfile-less \n Daemon-less\n\n\n2.1 - Why\n\n\n2.2 - Tools\n\n\n\n\n\n\n4 - Runtime-less ?\n\n\n4.1 - Why\n\n\n4.2 - Tools\n\n\n\n\n\n\n5 - Libs\n\n\n6 - Next ?\n\n\n\n\n\n\n\nBuilding images without Docker\n\n\n1 - Why\n\n\nSepare these phases\n\n* Build\n\n* Push/pull\n\n* Run\n\n\nPush/pull \n Run: \nCRI-O\n or \nContainerd\n\n\nPush/pull \n Run \n Build: \nDocker\n (Initially)\n\n\nCurrently:\n\n- Implicit dependency between build \n run\n\n\n2 - Dockerfile-less \n Daemon-less\n\n\n2.1 - Why\n\n\nDockerfiles: Really like shell scripts but with some \nDockerfile\n syntax \n\nDaeomon:  \n Missed notes\n\n\n2.2 - Tools\n\n\nBuildah\n\n\n\n\nNo docker daemin involved\n\n\nCan build from Dockerfile (compatibility)\n\n\nCan build imperatif via CLI steps\n\n\n\n\ngenuinetools/img\n\n\n\n\nSame UX as docker commands\n\n\nStandalone\n\n\nDockefile and OCI compatible images\n\n\nUse \nRunC\n rootless\n\n\n\n\n4 - Runtime-less ?\n\n\n4.1 - Why\n\n\n\n\nTo be more portable\n\n\nLess complexity ?\n\n\nSwap container tech on the fly ?\n\n\n\n\n4.2 - Tools\n\n\nGoogleContainerTools/distroless\n\n\n\n\nDeclarative and reproducible\n\n\nNo timestamps\n\n\nall dependencies at build time\n\n\nCan NOT interpret Dockefile\n\n\nRebase-able\n\n\nMinimal images\n\n\n\n\nGoogleContainerTools/kaniko\n\n\n\n\nInterpret Dockerfile\n\n\nSnapshots layers without UnionFS\n\n\nMeant exclusively for running inside containerized env\n\n\nNo runtime or containers\n\n\ngVisor + RunSc + Kube\n\n\n\n\n5 - Libs\n\n\n\n\ncontainers/image\n\n\ngoogle/go-containerregistry\n\n\ngoogle/container-registry\n\n\n\n\n6 - Next ?\n\n\nCNI: Network\n\nCRI: Runtime\n\nCSI: Storage\n\nCBI: ???", 
            "title": "20180502_talk_3"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#building-images-without-docker", 
            "text": "", 
            "title": "Building images without Docker"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#1-why", 
            "text": "Separe these phases \n* Build \n* Push/pull \n* Run  Push/pull   Run:  CRI-O  or  Containerd  Push/pull   Run   Build:  Docker  (Initially)  Currently: \n- Implicit dependency between build   run", 
            "title": "1 - Why"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#2-dockerfile-less-daemon-less", 
            "text": "", 
            "title": "2 - Dockerfile-less &amp; Daemon-less"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#21-why", 
            "text": "Dockerfiles: Really like shell scripts but with some  Dockerfile  syntax  \nDaeomon:    Missed notes", 
            "title": "2.1 - Why"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#22-tools", 
            "text": "", 
            "title": "2.2 - Tools"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#buildah", 
            "text": "No docker daemin involved  Can build from Dockerfile (compatibility)  Can build imperatif via CLI steps", 
            "title": "Buildah"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#genuinetoolsimg", 
            "text": "Same UX as docker commands  Standalone  Dockefile and OCI compatible images  Use  RunC  rootless", 
            "title": "genuinetools/img"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#4-runtime-less", 
            "text": "", 
            "title": "4 - Runtime-less ?"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#41-why", 
            "text": "To be more portable  Less complexity ?  Swap container tech on the fly ?", 
            "title": "4.1 - Why"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#42-tools", 
            "text": "", 
            "title": "4.2 - Tools"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#googlecontainertoolsdistroless", 
            "text": "Declarative and reproducible  No timestamps  all dependencies at build time  Can NOT interpret Dockefile  Rebase-able  Minimal images", 
            "title": "GoogleContainerTools/distroless"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#googlecontainertoolskaniko", 
            "text": "Interpret Dockerfile  Snapshots layers without UnionFS  Meant exclusively for running inside containerized env  No runtime or containers  gVisor + RunSc + Kube", 
            "title": "GoogleContainerTools/kaniko"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#5-libs", 
            "text": "containers/image  google/go-containerregistry  google/container-registry", 
            "title": "5 - Libs"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_3/#6-next", 
            "text": "CNI: Network \nCRI: Runtime \nCSI: Storage \nCBI: ???", 
            "title": "6 - Next ?"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/", 
            "text": "20180502_talk_4\n\n\n\n\n\n\n\nCI upgrading K8s\n\n\n1 - Infra overview\n\n\n2 - Phylosophy\n\n\n3 - Cluster Setup\n\n\n3.1 - Overview\n\n\n3.2 - CLM\n\n\n3.3 - Workflow\n\n\n\n\n\n\n4 - E2E tests\n\n\n5 - Live node upgrade\n\n\n5.1 - Naive aproach\n\n\n5.2 - PodDisruptionBudget\n\n\n5.3 - Postgres Operator\n\n\n5.4 - Other Issues notes\n\n\n\n\n\n\n\n\n\n\n\nCI upgrading K8s\n\n\n1 - Infra overview\n\n\nOverview\n\n* 84 Clusters\n\n* 366 Aws accounts\n\n\nBefore\n\n* Accounts per team\n\n* All instances are the same\n\n* PowerUser access to Production\n\n* You built it, you run EVERYHING\n\n\n-\n\n\nK8s\n\n* 1 cluster per product (Multiple Teams)\n\n* Intances are not managed by team\n\n* Hands Off approach\n\n* A lot of stuff out of the box\n\n\n2 - Phylosophy\n\n\n\n\nNo pet clusters: No tweaking for 80 clusters\n\n\nAlways update with latest k8s release\n\n\nContinuous NOT disuptive\n\n\n\n\n3 - Cluster Setup\n\n\n3.1 - Overview\n\n\n\n\nAWS Provision resources\n\n\nETCD Stack outside of k8s resources\n\n\nCoreOS based image\n\n\nMulti-AZ workers nodes\n\n\nHA ControlPlane with ELB\n\n\nCluster config in Git\n\n\ne2e tests with Jenkins\n\n\nChanges rolled out with \nCluster Lifecycle Manager\n (OpenSource since Friday 20180427)\n\n\n\n\n+ ClusterMetada (Cluster Registry)\n\n\ncluster/\n|\n- cluster.yaml\n- etcd-cluster.yaml\n- Manifests/\n  - ... services running in k8s\n-\n\n\n\n\n3.2 - CLM\n\n\nManager lookup\n\n- API server\n\n- ClusterRegistry\n\n- GitRepo\n\n\n\n\nUser: Creates a cluster via the Clusterregistry\n\n\nCLM Creates the cluster resources via AWS API\n\n\nCLM Pushes to GitRepo\n\n\n\n\n3.3 - Workflow\n\n\nWe use Git repo with 3 branches as \nrelease channels\n\n- Dev: 1 clusters\n\n- Alpha: 3\n\n- Beta: 80+\n\n- Stable\n\n\n4 - E2E tests\n\n\n\n\nUpstream Conformance Tests\n\n\nStatefulsets Tests\n\n\nZalendo tests (Custom to our integration)\n\n\n\n\ngithub.com/mikkeloscar/kuberntes-e2e\n\n\ndocker run ...\n  mikkeloscar/kuberntes-e2e:latest \\\n  -focus \n\\[Conformance\\]\n \\\n  -skip \n\\[...\\]\n\n\n\n\n\n5 - Live node upgrade\n\n\n5.1 - Naive aproach\n\n\nConcept:\n\n* Use Autoscaling capabilities\n\n* Add 1 new node (With new version)\n\n* Drain an old node, ASG delete and recreate a new one\n\n\nIssue:\n\n* Volume accross AZs\n\n* No differences between Master / Workers\n\n* No clear definition of \nOK, ready, do next\n\n\nSolution:\n\n* Kubelet\nNodeReady\n meta\n\n* ASG \nInService\n meta\n\n* ELB \nInService\n meta\n\n\n5.2 - PodDisruptionBudget\n\n\nResource in k8s\n\n\n5.3 - Postgres Operator\n\n\n5.4 - Other Issues notes\n\n\n\n\nFlannel store config in ETCD\n\n\nTook down internal docker registry, while updating too many nodes that did not have every images locally ....", 
            "title": "20180502_talk_4"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#ci-upgrading-k8s", 
            "text": "", 
            "title": "CI upgrading K8s"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#1-infra-overview", 
            "text": "Overview \n* 84 Clusters \n* 366 Aws accounts  Before \n* Accounts per team \n* All instances are the same \n* PowerUser access to Production \n* You built it, you run EVERYHING  -  K8s \n* 1 cluster per product (Multiple Teams) \n* Intances are not managed by team \n* Hands Off approach \n* A lot of stuff out of the box", 
            "title": "1 - Infra overview"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#2-phylosophy", 
            "text": "No pet clusters: No tweaking for 80 clusters  Always update with latest k8s release  Continuous NOT disuptive", 
            "title": "2 - Phylosophy"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#3-cluster-setup", 
            "text": "", 
            "title": "3 - Cluster Setup"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#31-overview", 
            "text": "AWS Provision resources  ETCD Stack outside of k8s resources  CoreOS based image  Multi-AZ workers nodes  HA ControlPlane with ELB  Cluster config in Git  e2e tests with Jenkins  Changes rolled out with  Cluster Lifecycle Manager  (OpenSource since Friday 20180427)   + ClusterMetada (Cluster Registry)  cluster/\n|\n- cluster.yaml\n- etcd-cluster.yaml\n- Manifests/\n  - ... services running in k8s\n-", 
            "title": "3.1 - Overview"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#32-clm", 
            "text": "Manager lookup \n- API server \n- ClusterRegistry \n- GitRepo   User: Creates a cluster via the Clusterregistry  CLM Creates the cluster resources via AWS API  CLM Pushes to GitRepo", 
            "title": "3.2 - CLM"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#33-workflow", 
            "text": "We use Git repo with 3 branches as  release channels \n- Dev: 1 clusters \n- Alpha: 3 \n- Beta: 80+ \n- Stable", 
            "title": "3.3 - Workflow"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#4-e2e-tests", 
            "text": "Upstream Conformance Tests  Statefulsets Tests  Zalendo tests (Custom to our integration)   github.com/mikkeloscar/kuberntes-e2e  docker run ...\n  mikkeloscar/kuberntes-e2e:latest \\\n  -focus  \\[Conformance\\]  \\\n  -skip  \\[...\\]", 
            "title": "4 - E2E tests"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#5-live-node-upgrade", 
            "text": "", 
            "title": "5 - Live node upgrade"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#51-naive-aproach", 
            "text": "Concept: \n* Use Autoscaling capabilities \n* Add 1 new node (With new version) \n* Drain an old node, ASG delete and recreate a new one  Issue: \n* Volume accross AZs \n* No differences between Master / Workers \n* No clear definition of  OK, ready, do next  Solution: \n* Kubelet NodeReady  meta \n* ASG  InService  meta \n* ELB  InService  meta", 
            "title": "5.1 - Naive aproach"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#52-poddisruptionbudget", 
            "text": "Resource in k8s", 
            "title": "5.2 - PodDisruptionBudget"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#53-postgres-operator", 
            "text": "", 
            "title": "5.3 - Postgres Operator"
        }, 
        {
            "location": "/2018/kubecon/day1/20180502_talk_4/#54-other-issues-notes", 
            "text": "Flannel store config in ETCD  Took down internal docker registry, while updating too many nodes that did not have every images locally ....", 
            "title": "5.4 - Other Issues notes"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_1/", 
            "text": "20180503_keynote_1\n\n\n\n\n\n\n\n1 - Sandboxed Containers\n\n\n2 - Applications\n\n\n3 - Monitoring\n\n\n4 - Dev UX\n\n\n\n\n\n\n\nGoogle Cloud updates\n\n\n1 - Sandboxed Containers\n\n\ngVisor\n: \nhttps://cloudplatform.googleblog.com/2018/05/Open-sourcing-gVisor-a-sandboxed-container-runtime.html\n\n\nCompatible with Docker and k8s\n\n\n2 - Applications\n\n\n\n\n Batch workload \n CronJob \n Spark (beta) in \n1.8\n\n\n Workload Controllers + Local Storage\n\n\n GPUs support in \n1.9\n\n\n StatefullSet\n\n\n\n\nNow : Statefull applications\n\n\n-\n \noperators\n\n\nOperatots:\n\n* leverage k8s API\n\n\nDemo: Spark Operator on GKE\n\n* Operator introduce new \nKind\n in k8s Manifests\n\n* Creates a \nspark-driver\n + \nspark-executors\n Pods\n\n\n3 - Monitoring\n\n\n4 - Dev UX\n\n\n\n\nSource Code\n\n\nBuild\n\n\nDeploy\n\n\nRun\n\n\n\n\nSo many steps between Devs and actual run in the process, Devs do not really interact with k8s \n\n\nCountinuous Developpement ?\n\n\n-\n New OpenSource project: \nSkaffold", 
            "title": "20180503_keynote_1"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_1/#1-sandboxed-containers", 
            "text": "gVisor :  https://cloudplatform.googleblog.com/2018/05/Open-sourcing-gVisor-a-sandboxed-container-runtime.html  Compatible with Docker and k8s", 
            "title": "1 - Sandboxed Containers"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_1/#2-applications", 
            "text": "Batch workload   CronJob   Spark (beta) in  1.8   Workload Controllers + Local Storage   GPUs support in  1.9   StatefullSet   Now : Statefull applications  -   operators  Operatots: \n* leverage k8s API  Demo: Spark Operator on GKE \n* Operator introduce new  Kind  in k8s Manifests \n* Creates a  spark-driver  +  spark-executors  Pods", 
            "title": "2 - Applications"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_1/#3-monitoring", 
            "text": "", 
            "title": "3 - Monitoring"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_1/#4-dev-ux", 
            "text": "Source Code  Build  Deploy  Run   So many steps between Devs and actual run in the process, Devs do not really interact with k8s   Countinuous Developpement ?  -  New OpenSource project:  Skaffold", 
            "title": "4 - Dev UX"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_2/", 
            "text": "20180503_keynote_2\n\n\n\n\n\n\n\n1 - Operators\n\n\n2 - Operators Framework\n\n\n\n\n\n\n\nK8s Applications\n\n\n1 - Operators\n\n\nLast year: Introduction of Operators principles\n\n\nFeedbacks:\n\n\n\n\nPrometheus Operator allows DevTeams and SRE TEams to provision end-to-end monitoring for Apps\n\n\n\n\n\n\n2 - Operators Framework\n\n\nIntroduced this week:\n\n- \nhttps://coreos.com/blog/introducing-operator-framework\n\n- \nhttps://github.com/operator-framework\n\n- SDK, Lifecycle Managers, and Metering\n\n\nLifecycle Managers:\n\n* Maintains catalog of Operatos", 
            "title": "20180503_keynote_2"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_2/#1-operators", 
            "text": "Last year: Introduction of Operators principles  Feedbacks:   Prometheus Operator allows DevTeams and SRE TEams to provision end-to-end monitoring for Apps", 
            "title": "1 - Operators"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_2/#2-operators-framework", 
            "text": "Introduced this week: \n-  https://coreos.com/blog/introducing-operator-framework \n-  https://github.com/operator-framework \n- SDK, Lifecycle Managers, and Metering  Lifecycle Managers: \n* Maintains catalog of Operatos", 
            "title": "2 - Operators Framework"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_3/", 
            "text": "20180503_keynote_3\n\n\n\n\n\n\n\n1 - Journey\n\n\n2 - Decisions\n\n\n3 - Migrating\n\n\n4 - CCL\n\n\n\n\n\n\n\nMigrating to k8s from the Finanicial Times\n\n\n1 - Journey\n\n\nChallenge is when you start with Existing services, and not starting from scratch\n\n\nWhy change ?\n\n* Hand rolled Docker in Prod mid 2015\n\n* Spend your innovation tokens wisely\n\n* Estimated cost reduction: 80% of EC2\n\n\nBUT\n\n* Many fewer steps to start running a new service in Prod \n\n* Doc challenge to know current running app\n\n* Choose boting tech (Etsy blog post)\n\n\nWARNING\n\n* The FT is not a cluster operations teams\n\n* Metrics for success:\n\n  - Amount of time spent keeping cluster healthy\n\n\n2 - Decisions\n\n\nBe confortable with change, don\nt scare makine bad descision.\n\nNowadays we now how to measure fast failure to change the plan\n\n\n3 - Migrating\n\n\nRunning train:\n\n* Do things in parallel \n (2 stacks)\n\n\nNotes:\n\n* Branch vs if/else\n\n  - Branch is very difficult to maintain\n\n  - If/else: you deploy on your living stack stuff for the new stack \n\n* Separate deployment mechanism / same tool\n\n  - Same than branches \n\n\nWarning:\n\n* Doing things in parallel takes time and money\n\n\n4 - CCL\n\n\n\n\nWe did not start from the easier starting point\n\n\nWe did not end where we thought\n\n\n\n\n-\n At some point ask your self if you are happy with the system and settle :)", 
            "title": "20180503_keynote_3"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_3/#1-journey", 
            "text": "Challenge is when you start with Existing services, and not starting from scratch  Why change ? \n* Hand rolled Docker in Prod mid 2015 \n* Spend your innovation tokens wisely \n* Estimated cost reduction: 80% of EC2  BUT \n* Many fewer steps to start running a new service in Prod  \n* Doc challenge to know current running app \n* Choose boting tech (Etsy blog post)  WARNING \n* The FT is not a cluster operations teams \n* Metrics for success: \n  - Amount of time spent keeping cluster healthy", 
            "title": "1 - Journey"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_3/#2-decisions", 
            "text": "Be confortable with change, don t scare makine bad descision. \nNowadays we now how to measure fast failure to change the plan", 
            "title": "2 - Decisions"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_3/#3-migrating", 
            "text": "Running train: \n* Do things in parallel   (2 stacks)  Notes: \n* Branch vs if/else \n  - Branch is very difficult to maintain \n  - If/else: you deploy on your living stack stuff for the new stack  \n* Separate deployment mechanism / same tool \n  - Same than branches   Warning: \n* Doing things in parallel takes time and money", 
            "title": "3 - Migrating"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_3/#4-ccl", 
            "text": "We did not start from the easier starting point  We did not end where we thought   -  At some point ask your self if you are happy with the system and settle :)", 
            "title": "4 - CCL"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/", 
            "text": "20180503_keynote_5\n\n\n\n\n\n\n\n1 - About Spotify\n\n\n2 - How to get Started\n\n\n3 - Support\n\n\n4 - Guidance\n\n\n5 - Let\ns do better\n\n\nCCL\n\n\n\n\n\n\n\nSoftware\ns community\n\n\n\n\nby Spotify\n\n\n\n\n1 - About Spotify\n\n\n\n\nWokring with community\n\n\nWe are not OpenSource shop like RedHat\n\n\nMoving to the cloud\n\n\n\n\n2 - How to get Started\n\n\nFear:\n\n* I don\nt do amazing stuff\n\n* Who wants to know about the stuff we do\n\n\nJust start to\n\n* share stories,\n\n* build relationship with other users\n\n\nWhy:\n\n* Getting support\n\n* Getting iead about products\n\n\n3 - Support\n\n\n\n\nGithub is an amazing platform to share, looks at issues, pace, contributors\n\n\nNow, look at Slack\n\n\n\n\n4 - Guidance\n\n\nCommunity is a tremendous source of guidance.\n\n* What exists today\n\n* What works best with the other software\n\n* What are other people doing with these tools\n\n* \n\n\n5 - Let\ns do better\n\n\nDoing something is hard, doing it alone is even harder.\n\n\nCCL\n\n\nThe \nfraud syndrome", 
            "title": "20180503_keynote_5"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#1-about-spotify", 
            "text": "Wokring with community  We are not OpenSource shop like RedHat  Moving to the cloud", 
            "title": "1 - About Spotify"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#2-how-to-get-started", 
            "text": "Fear: \n* I don t do amazing stuff \n* Who wants to know about the stuff we do  Just start to \n* share stories, \n* build relationship with other users  Why: \n* Getting support \n* Getting iead about products", 
            "title": "2 - How to get Started"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#3-support", 
            "text": "Github is an amazing platform to share, looks at issues, pace, contributors  Now, look at Slack", 
            "title": "3 - Support"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#4-guidance", 
            "text": "Community is a tremendous source of guidance. \n* What exists today \n* What works best with the other software \n* What are other people doing with these tools \n*", 
            "title": "4 - Guidance"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#5-lets-do-better", 
            "text": "Doing something is hard, doing it alone is even harder.", 
            "title": "5 - Let's do better"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_keynote_5/#ccl", 
            "text": "The  fraud syndrome", 
            "title": "CCL"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_other-resources/", 
            "text": "Other resources\n\n\nCNI: \nhttps://schd.ws/hosted_files/kccnceu18/64/Kubernetes-and-the-CNI-Kubecon-218.pdf\n\nKube day2: \nhttps://schd.ws/hosted_files/kccnceu18/fe/Managing%20Kubernetes-Day%202.pdf", 
            "title": "Other resources"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/", 
            "text": "20180503_talk_1\n\n\n\n\n\n\n\nAutoscaling with Prometheus\n\n\nType of scales\n\n\nHistory in K8s\n\n\nk8s 1.1\n\n\n\n\n\n\nNew Resource \n Custom Metrics APis\n\n\nPrometheus intro\n\n\nHorizontalPodAutoscaler (v2Bbta1)\n\n\n\n\n\n\nFuture\n\n\n\n\n\n\n\nAutoscaling with Prometheus\n\n\nType of scales\n\n\nVertical\n\n* Increase the resource liit when necessary\n\n\nHorizontal\n\n* Increase the number of instances\n\n\nHistory in K8s\n\n\nk8s 1.1\n\n\nUsed to rely on \nHeapster\n\n* \nHeapster\n collects metrics and write to TSDB\n\n* \nHeapster\n was compiled to the \nkubelet\n\n* Added manifests \nKind: HorizontalPodAutoscaler\n\n\n....\nspec:\n  scaleRef:\n    ...\n  minReplicas:\n  maxReplicas:\n\n\n\n\nBUT\n\n* Only based on CPU % resource\n\n* Loosely specified APIs\n\n* Unmaintained vendor implem\n\n* Push only TSDB\n\n\nNew Resource \n Custom Metrics APis\n\n\nPrometheus intro\n\n\nHorizontalPodAutoscaler (v2Bbta1)\n\n\nResource Metrics APis\n\n\n...\nspec:\n  scaleRef:\n    ...\n  minReplicas:\n  maxReplicas:\n\n  metrics:\n  - type: Pods\n    pods:\n      metricName: http_requets\n      targetAverageValue: 200\n\n\n\nProm query\n\n\nsum(rate(http_requests_total{namespace=\n...\n}[5min]))\n\n\n\nCustum Metrics APis\n\n* Need a \nk8s-prometheus-adapter\n + \nPrometheus\n\n\n=\n k8s \n1.10\n\n\nFuture\n\n\nHeapster deprecation\n\n* Planned for \n1.13\n\n\nVerticalPod\n\n* AutoSizing in \nalpha\n for Prometheus\n\n\nCRD AutoScale\n\n* Planned for \n1.11\n\n\nStable metrics !\n\n\nCluster AutoScaler\n\n* Try to not be a separate thing\n\n\nStandardization of Monitoring\n\n* With OpenMetrics project", 
            "title": "20180503_talk_1"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#autoscaling-with-prometheus", 
            "text": "", 
            "title": "Autoscaling with Prometheus"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#type-of-scales", 
            "text": "Vertical \n* Increase the resource liit when necessary  Horizontal \n* Increase the number of instances", 
            "title": "Type of scales"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#history-in-k8s", 
            "text": "", 
            "title": "History in K8s"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#k8s-11", 
            "text": "Used to rely on  Heapster \n*  Heapster  collects metrics and write to TSDB \n*  Heapster  was compiled to the  kubelet \n* Added manifests  Kind: HorizontalPodAutoscaler  ....\nspec:\n  scaleRef:\n    ...\n  minReplicas:\n  maxReplicas:  BUT \n* Only based on CPU % resource \n* Loosely specified APIs \n* Unmaintained vendor implem \n* Push only TSDB", 
            "title": "k8s 1.1"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#new-resource-custom-metrics-apis", 
            "text": "", 
            "title": "New Resource &amp; Custom Metrics APis"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#prometheus-intro", 
            "text": "", 
            "title": "Prometheus intro"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#horizontalpodautoscaler-v2bbta1", 
            "text": "Resource Metrics APis  ...\nspec:\n  scaleRef:\n    ...\n  minReplicas:\n  maxReplicas:\n\n  metrics:\n  - type: Pods\n    pods:\n      metricName: http_requets\n      targetAverageValue: 200  Prom query  sum(rate(http_requests_total{namespace= ... }[5min]))  Custum Metrics APis \n* Need a  k8s-prometheus-adapter  +  Prometheus  =  k8s  1.10", 
            "title": "HorizontalPodAutoscaler (v2Bbta1)"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_1/#future", 
            "text": "Heapster deprecation \n* Planned for  1.13  VerticalPod \n* AutoSizing in  alpha  for Prometheus  CRD AutoScale \n* Planned for  1.11  Stable metrics !  Cluster AutoScaler \n* Try to not be a separate thing  Standardization of Monitoring \n* With OpenMetrics project", 
            "title": "Future"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/", 
            "text": "20180503_talk_1\n\n\n\n\n\n\n\nKubeSpray Introduction\n\n\nk8s\n\n\nGoals\n\n\nRequirements\n\n\nDeployments\n\n\n\n\n\n\nKubespray\n\n\nOverview\n\n\nAnsible playbook\n\n\nApproach\n\n\nCommunity\n\n\nCi\n\n\nWorkflows\n\n\nHA\n\n\nScale tips\n\n\n\n\n\n\nNext steps\n\n\n\n\n\n\n\nKubeSpray Introduction\n\n\nk8s\n\n\nGoals\n\n\n\n\nRun app\n\n\nKeep these running\n\n\nGive resources\n\n\nConnecting them together\n\n\nPublishing services\n\n\n\n\nRequirements\n\n\n\n\nEtcd\n\n\nMasters\n\n\nWorkers\n\n\nContainer Runtime\n\n\nContainer Network\n\n\nPKI\n\n\nStorage\n\n\n\n\nDeployments\n\n\nDivivsive\n\n* Golang vs CfgMgnt\n\n* Orchestration approcaches (do your own)\n\n* CfgMgmt divisions\n\n* Not invented here syndrome\n\n* Reliability and maintenance\n\n\nEcosystem\n\n* Kops (cloud 1st, BareMetal now)\n\n* KubeAdm\n\n* Comercials\n\n\nKubespray\n\n\nOverview\n\n\n\n\nLifecycle manager\n\n\nFlexible and composable\n\n\nProd ready\n\n\nCommunity driven since 2015\n\n\n\n\nAnsible playbook\n\n\n\n\nComprehensive approach with playbooks concepts\n\n\nCovers all componenents\n\n\nReadable\n\n\nFlexible\n\n\nActively maintained\n\n\n\n\nApproach\n\n\n\n\nContainerize everything\n\n\nContainer engine option (Dokcer or rkt)\n\n\nDepoyment options (container-static-pods, Kubeadm, SelfHosted)\n\n\n\n\nCommunity\n\n\nAcvite \n Helpful: 250+ contributors\n\n\nCi\n\n\n\n\n20+ cluster topolgies tested\n\n\nDeploys real clusters\n\n\nRuns n GCE and DO\n\n\n\n\nWorkflows\n\n\nDeployment\n\n* Bootstrap OS\n\n* Preinstall (?)\n\n* Container Runtime\n\n* ETCD\n\n* PKI\n\n* Kube Masters\n\n* Kube workers\n\n* Network\n\n* DNS\n\n\nUpgrade\n\n* Change ansible value\n\n* Apply changes\n\n\n\n\nNotes\n\n\nUpgrade concerns: Drain nodes before maintenance\n\n\n\n\n\n\nHA\n\n\n\n\nETCD\n\n\nKubeAPIServer\n\n\nRuns Nnginx on every master servers (Or Cloud LBs of available)\n\n\n\n\nScale tips\n\n\n\n\nIOs for ETCD\n\n\n\n\nNext steps\n\n\n\n\nMore cloud providers support\n\n\nK8s e2e tests\n\n\nAuto-scaling\n\n\nMore add-on providers besides Helm", 
            "title": "20180503_talk_1"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#kubespray-introduction", 
            "text": "", 
            "title": "KubeSpray Introduction"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#k8s", 
            "text": "", 
            "title": "k8s"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#goals", 
            "text": "Run app  Keep these running  Give resources  Connecting them together  Publishing services", 
            "title": "Goals"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#requirements", 
            "text": "Etcd  Masters  Workers  Container Runtime  Container Network  PKI  Storage", 
            "title": "Requirements"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#deployments", 
            "text": "Divivsive \n* Golang vs CfgMgnt \n* Orchestration approcaches (do your own) \n* CfgMgmt divisions \n* Not invented here syndrome \n* Reliability and maintenance  Ecosystem \n* Kops (cloud 1st, BareMetal now) \n* KubeAdm \n* Comercials", 
            "title": "Deployments"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#kubespray", 
            "text": "", 
            "title": "Kubespray"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#overview", 
            "text": "Lifecycle manager  Flexible and composable  Prod ready  Community driven since 2015", 
            "title": "Overview"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#ansible-playbook", 
            "text": "Comprehensive approach with playbooks concepts  Covers all componenents  Readable  Flexible  Actively maintained", 
            "title": "Ansible playbook"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#approach", 
            "text": "Containerize everything  Container engine option (Dokcer or rkt)  Depoyment options (container-static-pods, Kubeadm, SelfHosted)", 
            "title": "Approach"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#community", 
            "text": "Acvite   Helpful: 250+ contributors", 
            "title": "Community"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#ci", 
            "text": "20+ cluster topolgies tested  Deploys real clusters  Runs n GCE and DO", 
            "title": "Ci"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#workflows", 
            "text": "Deployment \n* Bootstrap OS \n* Preinstall (?) \n* Container Runtime \n* ETCD \n* PKI \n* Kube Masters \n* Kube workers \n* Network \n* DNS  Upgrade \n* Change ansible value \n* Apply changes", 
            "title": "Workflows"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#notes", 
            "text": "Upgrade concerns: Drain nodes before maintenance", 
            "title": "Notes"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#ha", 
            "text": "ETCD  KubeAPIServer  Runs Nnginx on every master servers (Or Cloud LBs of available)", 
            "title": "HA"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#scale-tips", 
            "text": "IOs for ETCD", 
            "title": "Scale tips"
        }, 
        {
            "location": "/2018/kubecon/day2/20180503_talk_2/#next-steps", 
            "text": "More cloud providers support  K8s e2e tests  Auto-scaling  More add-on providers besides Helm", 
            "title": "Next steps"
        }
    ]
}