# 20170329_kubeCon_1.2.3

<!-- MarkdownTOC -->

- [2 - Presentations](#2---presentations)
  - [2.3 - Hosting and managing K8s clusters @Google](#23---hosting-and-managing-k8s-clusters-google)
    - [GKE](#gke)
    - [Challenges](#challenges)
    - [How: Creating a cluster](#how-creating-a-cluster)
    - [How: Upgrading](#how-upgrading)
    - [How: Repairs](#how-repairs)
    - [Running you Apps](#running-you-apps)
    - [Auto-scaling](#auto-scaling)
    - [Global network: AKA federation](#global-network-aka-federation)

<!-- /MarkdownTOC -->




## 2 - Presentations

### 2.3 - Hosting and managing K8s clusters @Google

#### GKE

Hosted and Managed vanilla K8s clusters


#### Challenges

Create / Destroy k8s cluster
Easily, quickly


#### How: Creating a cluster

1. Build an Koos image
  + k8s Optimized OS
  + Based on Chromium OS
2. Create 1 Master VM
  + Configure using Metadata server
3. Create Nodes VM
  + In an MIG (Auto-scaling groups)
  + Create a NodePool (that contains the MIG)
4. Networking Magic
  + Overlay network for the Pods
  + Routes to reach the master

=> 3 min


#### How: Upgrading

Monitoring:
* Watch the master VM
* Watch the Kube-system
* Watch the nodes VM
* Watch the MIG

Notes:
* ECTD is store on persistent disk
* Master has STATIC IP

Process:
* Master:
  + Shutdown the master
  + Restart the master with new Image base
* Nodes:
  + Update MIG template
  + Pick a node and drain it
  + MIG recreate a new Node according to the MIG template

OR: Use the NodePool feature
  + New Pool with same / new MIG template
  + Blue/green style


#### How: Repairs

?


#### Running you Apps

Leverage GKE Metadata as labels

#### Auto-scaling

Pods: OK

Nodes: SIG
* Cluster autoscaling is a signal that k8s will emit
* Cloud provider will have to support it


#### Global network: AKA federation

Federated Ingress !
Soon
